{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/prattisr/Duty_4/miniconda/~miniconda/envs/preprocess/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer, BertTokenizerFast\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# importing other required libraries\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import dill\n",
    "import os\n",
    "import options\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import data\n",
    "import utils\n",
    "from meters import AverageMeter\n",
    "from PGLoss import PGLoss\n",
    "from tqdm import tqdm\n",
    "from dictionary import Dictionary\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "# Importing Generator and Discriminator class methods\n",
    "from generator_tf_bert import TransformerModel_bert\n",
    "from discriminator_cnn_bert import Discriminator_cnn_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user's home directory\n",
    "home = os.path.expanduser(\"~\")\n",
    "\n",
    "# Define the path of the cache directory\n",
    "cache_dir = os.path.join(home, \".cache\", \"huggingface\", \"datasets\")\n",
    "\n",
    "# Define the name and configuration of the dataset\n",
    "dataset_name = \"wmt14\"\n",
    "config_name = \"fr-en\"\n",
    "\n",
    "# Build the path for the specific dataset configuration\n",
    "dataset_config_path = os.path.join(cache_dir, dataset_name, config_name)\n",
    "\n",
    "print(f\"Checking cache at: {dataset_config_path}\")\n",
    "\n",
    "# Check if the dataset configuration is already cached\n",
    "if os.path.exists(dataset_config_path) and len(os.listdir(dataset_config_path)) > 0:\n",
    "    print(\"Dataset already downloaded, loading from cache.\")\n",
    "    # If the dataset is already downloaded, load it from the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "else:\n",
    "    print(\"Downloading the dataset.\")\n",
    "    # Download the dataset and specify the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "\n",
    "# Here, you should adjust the loading of subsets to avoid redundant downloads or loading.\n",
    "# Load 50k rows of the train dataset\n",
    "train_dataset = dataset['train'].select(range(10))\n",
    "\n",
    "# Keep the full valid and test datasets\n",
    "valid_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "\n",
    "# Loading Bert Model\n",
    "bert_model = \"bert-base-multilingual-cased\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
