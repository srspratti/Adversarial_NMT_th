{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache at: /root/.cache/huggingface/datasets/wmt14/fr-en\n",
      "Dataset already downloaded, loading from cache.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cebba38d6342bb82fa679365f824a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ec8486f13e407984e078922013f503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get user's home directory\n",
    "import os\n",
    "home = os.path.expanduser(\"~\")\n",
    "\n",
    "# Define the path of the cache directory\n",
    "cache_dir = os.path.join(home, \".cache\", \"huggingface\", \"datasets\")\n",
    "\n",
    "# Define the name and configuration of the dataset\n",
    "dataset_name = \"wmt14\"\n",
    "config_name = \"fr-en\"\n",
    "\n",
    "# Build the path for the specific dataset configuration\n",
    "dataset_config_path = os.path.join(cache_dir, dataset_name, config_name)\n",
    "\n",
    "print(f\"Checking cache at: {dataset_config_path}\")\n",
    "\n",
    "# Check if the dataset configuration is already cached\n",
    "if os.path.exists(dataset_config_path) and len(os.listdir(dataset_config_path)) > 0:\n",
    "    print(\"Dataset already downloaded, loading from cache.\")\n",
    "    # If the dataset is already downloaded, load it from the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "else:\n",
    "    print(\"Downloading the dataset.\")\n",
    "    # Download the dataset and specify the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "\n",
    "# Here, you should adjust the loading of subsets to avoid redundant downloads or loading.\n",
    "# Load 50k rows of the train dataset\n",
    "train_dataset = dataset[\"train\"].select(range(100020))\n",
    "# train_dataset = dataset[\"train\"].select(range(600))\n",
    "\n",
    "# Keep the full valid and test datasets\n",
    "valid_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 3003\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts =[]\n",
    "labels = []\n",
    "for element in test_dataset[\"translation\"]:\n",
    "        # print(\"element: \", element)\n",
    "        texts.append(element[\"en\"])\n",
    "        labels.append(element[\"fr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "import os\n",
    "getpwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path\n",
    "\n",
    "file_path_en = os.path.join(getpwd, \"original_english.txt\")\n",
    "# file_path = \"/path/to/translations.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_en, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for text in texts:\n",
    "        file.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path\n",
    "# import os\n",
    "# getpwd = os.getcwd()\n",
    "file_path_fr = os.path.join(getpwd, \"original_french.txt\")\n",
    "# file_path = \"/path/to/translations.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_fr, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for label in labels:\n",
    "        file.write(label + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# google-t5/t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 3003/3003 [23:23<00:00,  2.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Generate multiple translations in batched format\n",
    "\n",
    "\n",
    "# # Initialize the tokenizer and model\n",
    "tokenizer_t5_small_pretrained = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model_t5_small_pretrained = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "translations_t5_small_pretrained = []\n",
    "# for text in tqdm(texts):\n",
    "for idx, text in tqdm(enumerate(texts), desc=\"Translating\", total=len(texts)):\n",
    "    # print(\"Original English Sentence: \", text)\n",
    "    # print(\"Original French Sentence: \", labels[idx])\n",
    "    inputs_t5_small_pretrained = tokenizer_t5_small_pretrained(text, return_tensors=\"pt\").input_ids\n",
    "    outputs_t5_small_pretrained = model_t5_small_pretrained.generate(inputs_t5_small_pretrained, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "    translation_t5_small_pretrained = tokenizer_t5_small_pretrained.decode(outputs_t5_small_pretrained[0], skip_special_tokens=True)\n",
    "    # print(\"translated french sentences:\",translation_t5_small_pretrained)\n",
    "    translations_t5_small_pretrained.append(translation_t5_small_pretrained)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "# Specify the file path\n",
    "file_path_t5_small_pretrained = os.path.join(getpwd, \"translated_french_by_t5_small_pretrained.txt\")\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_t5_small_pretrained, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for translation in translations_t5_small_pretrained:\n",
    "        file.write(translation + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 2.041124318007492}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_t5_small_pretrained = metric.compute(predictions=translations_t5_small_pretrained, references=labels)\n",
    "result_t5_small_pretrained = {\"bleu\": result_t5_small_pretrained[\"score\"]}\n",
    "result_t5_small_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# google-t5/t5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57d2289f2b64de8ba94717d7d653919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4ec86c01904be2a0d0942505ab7595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90556e832f4f41b29925c9c1d1efe985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4be55a02474190916629acf0d7748f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41292aec251f4b46a27af6c8911291dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 3003/3003 [50:13<00:00,  1.00s/it] \n"
     ]
    }
   ],
   "source": [
    "# # Initialize the tokenizer and model\n",
    "tokenizer_t5_base_pretrained = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n",
    "model_t5_base_pretrained = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n",
    "\n",
    "translations_t5_base_pretrained = []\n",
    "# for text in tqdm(texts):\n",
    "for idx, text in tqdm(enumerate(texts), desc=\"Translating\", total=len(texts)):\n",
    "    # print(\"Original English Sentence: \", text)\n",
    "    # print(\"Original French Sentence: \", labels[idx])\n",
    "    inputs_t5_base_pretrained = tokenizer_t5_base_pretrained(text, return_tensors=\"pt\").input_ids\n",
    "    outputs_t5_base_pretrained = model_t5_base_pretrained.generate(inputs_t5_base_pretrained, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "    translation_t5_base_pretrained = tokenizer_t5_base_pretrained.decode(outputs_t5_base_pretrained[0], skip_special_tokens=True)\n",
    "    # print(\"translated french sentences:\",translation_t5_small_pretrained)\n",
    "    translations_t5_base_pretrained.append(translation_t5_base_pretrained)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "# Specify the file path\n",
    "file_path_t5_base_pretrained = os.path.join(getpwd, \"translated_french_by_t5_base_pretrained.txt\")\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_t5_base_pretrained, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for translation in translations_t5_base_pretrained:\n",
    "        file.write(translation + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 1.196346176518329}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_t5_base_pretrained = metric.compute(predictions=translations_t5_base_pretrained, references=labels)\n",
    "result_t5_base_pretrained = {\"bleu\": result_t5_base_pretrained[\"score\"]}\n",
    "result_t5_base_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helsinki-NLP/opus-mt-en-fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 3003/3003 [55:23<00:00,  1.11s/it]  \n"
     ]
    }
   ],
   "source": [
    "# # Initialize the tokenizer and model\n",
    "tokenizer_opus_mt_en_fr_pretrained = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "model_opus_mt_en_fr_pretrained = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "translations_opus_mt_en_fr_pretrained = []\n",
    "# for text in tqdm(texts):\n",
    "for idx, text in tqdm(enumerate(texts), desc=\"Translating\", total=len(texts)):\n",
    "    # print(\"Original English Sentence: \", text)\n",
    "    # print(\"Original French Sentence: \", labels[idx])\n",
    "    inputs_opus_mt_en_fr_pretrained= tokenizer_opus_mt_en_fr_pretrained(text, return_tensors=\"pt\").input_ids\n",
    "    outputs_opus_mt_en_fr_pretrained = model_opus_mt_en_fr_pretrained.generate(inputs_opus_mt_en_fr_pretrained, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "    translation_opus_mt_en_fr_pretrained = tokenizer_opus_mt_en_fr_pretrained.decode(outputs_opus_mt_en_fr_pretrained[0], skip_special_tokens=True)\n",
    "    # print(\"translated french sentences:\",translation_t5_small_pretrained)\n",
    "    translations_opus_mt_en_fr_pretrained.append(translation_opus_mt_en_fr_pretrained)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "# Specify the file path\n",
    "file_path_opus_mt_en_fr_pretrained = os.path.join(getpwd, \"translated_french_by_opus_mt_en_fr_pretrained.txt\")\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_opus_mt_en_fr_pretrained, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for translation in translations_opus_mt_en_fr_pretrained:\n",
    "        file.write(translation + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 33.13527604394126}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_opus_mt_en_fr_pretrained = metric.compute(predictions=translations_opus_mt_en_fr_pretrained, references=labels)\n",
    "result_opus_mt_en_fr_pretrained = {\"bleu\": result_opus_mt_en_fr_pretrained[\"score\"]}\n",
    "result_opus_mt_en_fr_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sriram-sanjeev9s/T5_wmt14_En_Fr_1million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 3003/3003 [38:09<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_t5_finetuned_wmt14_1mil = AutoTokenizer.from_pretrained(\"sriram-sanjeev9s/T5_wmt14_En_Fr_1million\")\n",
    "model_t5_finetuned_wmt14_1mil = AutoModelForSeq2SeqLM.from_pretrained(\"sriram-sanjeev9s/T5_wmt14_En_Fr_1million\")\n",
    "\n",
    "translations_t5_finetuned_wmt14_1mil = []\n",
    "# for text in tqdm(texts):\n",
    "for idx, text in tqdm(enumerate(texts), desc=\"Translating\", total=len(texts)):\n",
    "    # print(\"english sentence: \", text)\n",
    "    # print(\"Original French Sentence: \", labels[texts.index(text)])\n",
    "    inputs_t5_finetuned_wmt14_1mil = tokenizer_t5_finetuned_wmt14_1mil(text, return_tensors=\"pt\").input_ids\n",
    "    # outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "    outputs_t5_finetuned_wmt14_1mil = model_t5_finetuned_wmt14_1mil.generate(inputs_t5_finetuned_wmt14_1mil, max_length=60, num_beams=5, early_stopping=True)\n",
    "    translation_t5_finetuned_wmt14_1mil = tokenizer_t5_finetuned_wmt14_1mil.decode(outputs_t5_finetuned_wmt14_1mil[0], skip_special_tokens=True)\n",
    "    # print(\"translated french sentences:\",translation_t5_finetuned_wmt14_1mil)\n",
    "    translations_t5_finetuned_wmt14_1mil.append(translation_t5_finetuned_wmt14_1mil)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "# Specify the file path\n",
    "file_path_t5_finetuned_wmt14_1mil = os.path.join(getpwd, \"translated_french_by_t5_finetuned_wmt14_1mil.txt\")\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_t5_finetuned_wmt14_1mil, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for translation in translations_t5_finetuned_wmt14_1mil:\n",
    "        file.write(translation + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 21.71904404108645}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_t5_finetuned_wmt14_1mil = metric.compute(predictions=translations_t5_finetuned_wmt14_1mil, references=labels)\n",
    "result_t5_finetuned_wmt14_1mil = {\"bleu\": result_t5_finetuned_wmt14_1mil[\"score\"]}\n",
    "result_t5_finetuned_wmt14_1mil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sriram-sanjeev9s/opus-mt-en-fr_wmt14_En_Fr_1million_20epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2970f2d608ea452b88c1488c3ff44635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/299M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 3003/3003 [04:54<00:00, 10.19it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs = AutoTokenizer.from_pretrained(\"sriram-sanjeev9s/opus-mt-en-fr_wmt14_En_Fr_1million_20epochs_v2\")\n",
    "model_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs = AutoModelForSeq2SeqLM.from_pretrained(\"sriram-sanjeev9s/opus-mt-en-fr_wmt14_En_Fr_1million_20epochs_v2\")\n",
    "\n",
    "translations_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs = []\n",
    "# for text in tqdm(texts):\n",
    "for idx, text in tqdm(enumerate(texts), desc=\"Translating\", total=len(texts)):\n",
    "    # print(\"english sentence: \", text)\n",
    "    # print(\"Original French Sentence: \", labels[texts.index(text)])\n",
    "    inputs_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs = tokenizer_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs(text, return_tensors=\"pt\").input_ids\n",
    "    # outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "    outputs_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs = model_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs.generate(inputs_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs, max_length=60, num_beams=5, early_stopping=True)\n",
    "    translation_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs = tokenizer_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs.decode(outputs_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs[0], skip_special_tokens=True)\n",
    "    # print(\"translated french sentences:\",translation_t5_finetuned_wmt14_1mil)\n",
    "    translations_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs.append(translation_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "# Specify the file path\n",
    "file_path_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs = os.path.join(getpwd, \"translated_french_by_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs.txt\")\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for translation in translations_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs:\n",
    "        file.write(translation + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs = metric.compute(predictions=translations_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs, references=labels)\n",
    "result_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs = {\"bleu\": result_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs[\"score\"]}\n",
    "result_opus_mt_en_fr_wmt14_En_Fr_1million_20epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sriram-sanjeev9s/T5_base_wmt14_En_Fr_1million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 3003/3003 [1:29:14<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_t5_base_finetuned_wmt14_1mil = AutoTokenizer.from_pretrained(\"sriram-sanjeev9s/T5_base_wmt14_En_Fr_1million\")\n",
    "model_t5_base_finetuned_wmt14_1mil = AutoModelForSeq2SeqLM.from_pretrained(\"sriram-sanjeev9s/T5_base_wmt14_En_Fr_1million\")\n",
    "\n",
    "translations_t5_base_finetuned_wmt14_1mil = []\n",
    "# for text in tqdm(texts):\n",
    "for idx, text in tqdm(enumerate(texts), desc=\"Translating\", total=len(texts)):\n",
    "    # print(\"english sentence: \", text)\n",
    "    # print(\"Original French Sentence: \", labels[texts.index(text)])\n",
    "    inputs_t5_base_finetuned_wmt14_1mil = tokenizer_t5_base_finetuned_wmt14_1mil(text, return_tensors=\"pt\").input_ids\n",
    "    # outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "    outputs_t5_base_finetuned_wmt14_1mil = model_t5_base_finetuned_wmt14_1mil.generate(inputs_t5_base_finetuned_wmt14_1mil, max_length=60, num_beams=5, early_stopping=True)\n",
    "    translation_t5_base_finetuned_wmt14_1mil = tokenizer_t5_base_finetuned_wmt14_1mil.decode(outputs_t5_base_finetuned_wmt14_1mil[0], skip_special_tokens=True)\n",
    "    # print(\"translated french sentences:\",translation_t5_finetuned_wmt14_1mil)\n",
    "    translations_t5_base_finetuned_wmt14_1mil.append(translation_t5_base_finetuned_wmt14_1mil)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "# Specify the file path\n",
    "file_path_t5_base_finetuned_wmt14_1mil = os.path.join(getpwd, \"translated_french_by_t5_base_finetuned_wmt14_1mil.txt\")\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_t5_base_finetuned_wmt14_1mil, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for translation in translations_t5_base_finetuned_wmt14_1mil:\n",
    "        file.write(translation + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 23.78058757171012}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_t5_base_finetuned_wmt14_1mil = metric.compute(predictions=translations_t5_base_finetuned_wmt14_1mil, references=labels)\n",
    "result_t5_base_finetuned_wmt14_1mil = {\"bleu\": result_t5_base_finetuned_wmt14_1mil[\"score\"]}\n",
    "result_t5_base_finetuned_wmt14_1mil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saved generator from joint_train_Bert_dualG_v2_logTrnsDB_pg_kd.py wmt14_en_fr_800sent_pg_kd_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_t5_base_finetuned_wmt14_1mil = AutoTokenizer.from_pretrained(\"/home/paperspace/google_drive_v4/Research_Thesis/2024/Adversarial_NMT_th/checkpoints/bert_dualG/wmt14_en_fr_800sent_pg_kd_loss/best_generator.pt\")\n",
    "model_t5_base_finetuned_wmt14_1mil = AutoModelForSeq2SeqLM.from_pretrained(\"/home/paperspace/google_drive_v4/Research_Thesis/2024/Adversarial_NMT_th/checkpoints/bert_dualG/wmt14_en_fr_800sent_pg_kd_loss/best_generator.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_t5_base_finetuned_wmt14_1mil = AutoTokenizer.from_pretrained(\"/home/paperspace/google_drive_v4/Research_Thesis/2024/Adversarial_NMT_th/checkpoints/bert_dualG/wmt14_en_fr_800sent_pg_kd_loss/best_generator.pt\")\n",
    "model_t5_base_finetuned_wmt14_1mil = AutoModelForSeq2SeqLM.from_pretrained(\"/home/paperspace/google_drive_v4/Research_Thesis/2024/Adversarial_NMT_th/checkpoints/bert_dualG/wmt14_en_fr_800sent_pg_kd_loss/best_generator.pt\")\n",
    "\n",
    "translations_t5_base_finetuned_wmt14_1mil = []\n",
    "# for text in tqdm(texts):\n",
    "for idx, text in tqdm(enumerate(texts), desc=\"Translating\", total=len(texts)):\n",
    "    # print(\"english sentence: \", text)\n",
    "    # print(\"Original French Sentence: \", labels[texts.index(text)])\n",
    "    inputs_t5_base_finetuned_wmt14_1mil = tokenizer_t5_base_finetuned_wmt14_1mil(text, return_tensors=\"pt\").input_ids\n",
    "    # outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "    outputs_t5_base_finetuned_wmt14_1mil = model_t5_base_finetuned_wmt14_1mil.generate(inputs_t5_base_finetuned_wmt14_1mil, max_length=60, num_beams=5, early_stopping=True)\n",
    "    translation_t5_base_finetuned_wmt14_1mil = tokenizer_t5_base_finetuned_wmt14_1mil.decode(outputs_t5_base_finetuned_wmt14_1mil[0], skip_special_tokens=True)\n",
    "    # print(\"translated french sentences:\",translation_t5_finetuned_wmt14_1mil)\n",
    "    translations_t5_base_finetuned_wmt14_1mil.append(translation_t5_base_finetuned_wmt14_1mil)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "# Specify the file path\n",
    "file_path_t5_base_finetuned_wmt14_1mil = os.path.join(getpwd, \"translated_french_by_t5_base_finetuned_wmt14_1mil.txt\")\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_t5_base_finetuned_wmt14_1mil, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for translation in translations_t5_base_finetuned_wmt14_1mil:\n",
    "        file.write(translation + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocess_bert_udem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
