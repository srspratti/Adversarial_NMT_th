{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer, BertTokenizerFast\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# importing other required libraries\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import dill\n",
    "import os\n",
    "import options\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import data\n",
    "import utils\n",
    "from meters import AverageMeter\n",
    "from PGLoss import PGLoss\n",
    "from tqdm import tqdm\n",
    "from dictionary import Dictionary\n",
    "import re\n",
    "import subprocess\n",
    "#Define the path of the cache directory\n",
    "cache_dir = \"~/.cache/huggingface/datasets\"\n",
    "\n",
    "#Define the name of the dataset\n",
    "dataset_name = \"wmt14\"\n",
    "\n",
    "# check if the dataset is already downloaded\n",
    "if os.path.exists(os.path.join(cache_dir, dataset_name)):\n",
    "    print(\"Dataset already downloaded\") # if the dataset is already downloaded\n",
    "else:\n",
    "    print(\"Downloading the dataset\")\n",
    "    # Download the dataset\n",
    "    dataset = load_dataset(\"wmt14\", \"fr-en\")\n",
    "    # dataset = datasets.load_dataset('wmt14', 'fr-en', cache_dir='path_to_cache_dir', download_mode='force_redownload') # if any error occurs, use this line of code\n",
    "    # dataset = load_dataset(\"wmt14\", \"en-fr\", split='train[:1%]', streaming=True) # Only loading 1% of train dataset for proto-typing purposes\n",
    "\n",
    "# Load 50k rows of the train dataset\n",
    "train_dataset = load_dataset(\"wmt14\", \"fr-en\", split=\"train[:50000]\")\n",
    "\n",
    "# Keep the full valid and test datasets\n",
    "valid_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# Loading Bert Model\n",
    "\n",
    "bert_model = \"bert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_data(json_data):    \n",
    "    en =list()\n",
    "    fr = list()\n",
    "    for row_idx in json_data:\n",
    "        en.append(row_idx['translation']['en'])\n",
    "        fr.append(row_idx['translation']['fr'])\n",
    "    \n",
    "    return en, fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "def preprocess(data):\n",
    "    # Initialize the BERT tokenizer\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "    # print(\"data: \", data)\n",
    "    en=list()\n",
    "    fr=list()\n",
    "    for element in data['translation']:\n",
    "        # print(\"element: \", element)\n",
    "        en.append(element['en'])\n",
    "        fr.append(element['fr'] )\n",
    "    # en, fr = get_json_data(data)\n",
    "    \n",
    "    # Tokenize the data\n",
    "    inputs = tokenizer(\n",
    "        en, truncation=True, padding=\"max_length\", max_length=128\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        fr, truncation=True, padding=\"max_length\", max_length=128\n",
    "    )\n",
    "\n",
    "    # Convert tokens to their corresponding IDs\n",
    "    input_ids = inputs.input_ids\n",
    "    target_ids = targets.input_ids\n",
    "\n",
    "    # Create attention masks\n",
    "    input_attention_mask = inputs.attention_mask\n",
    "    target_attention_mask = targets.attention_mask\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": input_attention_mask,\n",
    "        \"target_ids\": target_ids,\n",
    "        \"target_attention_mask\": target_attention_mask,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "en, fr = get_json_data(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50000/50000 [02:40<00:00, 311.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_datasets = train_dataset.map(\n",
    "    preprocess, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation', 'input_ids', 'attention_mask', 'target_ids', 'target_attention_mask'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator_tf_bert import TransformerModel_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/u/prattisr/phase-2/all_repos/Adversarial_NMT/neural-machine-translation-using-gan-master/joint_train+Bert.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Boctal28/u/prattisr/phase-2/all_repos/Adversarial_NMT/neural-machine-translation-using-gan-master/joint_train%2BBert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m generator2_train \u001b[39m=\u001b[39m TransformerModel_bert(args, use_cuda\u001b[39m=\u001b[39muse_cuda)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "generator2_train = TransformerModel_bert(args, use_cuda=use_cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
