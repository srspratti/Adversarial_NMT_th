{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 16:24:06 | INFO | fairseq.file_utils | loading archive file /home/paperspace/google_drive_v1/Research_Thesis/2024/Adversarial_NMT_th/pretrained_models/wmt14.en-fr.joined-dict.transformer\n",
      "2024-04-16 16:24:06 | INFO | fairseq.file_utils | loading archive file /home/paperspace/google_drive_v1/Research_Thesis/2024/Adversarial_NMT_th/pretrained_models/wmt14.en-fr.joined-dict.transformer\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/fairseq/checkpoint_utils.py:425: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/fairseq/models/fairseq_model.py:267: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "2024-04-16 16:43:01 | INFO | fairseq.tasks.translation | [en] dictionary: 44512 types\n",
      "2024-04-16 16:43:01 | INFO | fairseq.tasks.translation | [fr] dictionary: 44512 types\n",
      "2024-04-16 16:43:05 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair0253:58342', 'distributed_port': 58342, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 5120, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5120, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0007], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=10, log_format='json', seed=2, data='/home/paperspace/google_drive_v1/Research_Thesis/2024/Adversarial_NMT_th/pretrained_models/wmt14.en-fr.joined-dict.transformer', source_lang='en', target_lang='fr', max_source_positions=1024, max_target_positions=1024, skip_invalid_size_inputs_valid_test=False, max_tokens=5120, max_sentences=None, train_subset='train', valid_subset='valid', max_sentences_valid=None, sample_without_replacement=0, distributed_world_size=128, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://learnfair0253:58342', distributed_port=58342, device_id=0, arch='transformer_vaswani_wmt_en_de_big', criterion='label_smoothed_cross_entropy', max_epoch=0, max_update=80000, clip_norm=0.0, sentence_avg=False, update_freq=[1.0], fp16=True, optimizer='adam', lr=[0.0007], momentum=0.99, weight_decay=0.0, lr_scheduler='inverse_sqrt', lr_shrink=0.1, save_dir='/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', restore_file='checkpoint_last.pt', save_interval=1, no_save=False, no_epoch_checkpoints=False, validate_interval=1, dropout=0.1, attention_dropout=0.0, relu_dropout=0.0, encoder_normalize_before=False, encoder_learned_pos=False, decoder_learned_pos=False, decoder_normalize_before=False, share_decoder_input_output_embed=True, share_all_embeddings=True, label_smoothing=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, warmup_updates=4000, warmup_init_lr=1e-07, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_attention_heads=16, bpe='subword_nmt', bpe_codes='/home/paperspace/google_drive_v1/Research_Thesis/2024/Adversarial_NMT_th/pretrained_models/wmt14.en-fr.joined-dict.transformer/bpecodes', task='translation', stop_min_lr=1e-09, _name='transformer_vaswani_wmt_en_de_big', encoder_embed_path=None, decoder_embed_path=None, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0), 'task': {'_name': 'translation', 'data': '/home/paperspace/google_drive_v1/Research_Thesis/2024/Adversarial_NMT_th/pretrained_models/wmt14.en-fr.joined-dict.transformer', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0007]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0007]}, 'scoring': None, 'bpe': {'_name': 'subword_nmt', 'bpe_codes': '/home/paperspace/google_drive_v1/Research_Thesis/2024/Adversarial_NMT_th/pretrained_models/wmt14.en-fr.joined-dict.transformer/bpecodes', 'bpe_separator': '@@'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Generator loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "path_to_your_pretrained_model = '/home/paperspace/google_drive_v1/Research_Thesis/2024/Adversarial_NMT_th/pretrained_models/wmt14.en-fr.joined-dict.transformer'\n",
    "from fairseq.models.transformer import TransformerModel\n",
    "generator_pt = TransformerModel.from_pretrained(\n",
    "model_name_or_path=path_to_your_pretrained_model,\n",
    "checkpoint_file='model.pt',\n",
    "bpe='subword_nmt',\n",
    "# data_name_or_path='/u/prattisr/phase-2/all_repos/Adversarial_NMT/neural-machine-translation-using-gan-master/data-bin/wmt14_en_fr_raw_sm/50kLines',\n",
    "data_name_or_path='/home/paperspace/google_drive_v1/Research_Thesis/2024/Adversarial_NMT_th/pretrained_models/wmt14.en-fr.joined-dict.transformer',\n",
    "bpe_codes = '/home/paperspace/google_drive_v1/Research_Thesis/2024/Adversarial_NMT_th/pretrained_models/wmt14.en-fr.joined-dict.transformer/bpecodes'\n",
    ")\n",
    "print(\"Pretrained Generator loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fairseq.hub_utils.GeneratorHubInterface"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generator_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_pt.eval()\n",
    "sentence_to_translate = [\"Good Morning, I am going to a picnic tomorrow\", \"This is in English\"]\n",
    "# translated_sentences = generator_pt.translate([\"Good Morning, I am going to a picnic tomorrow\",\"Good Afternoon\"])\n",
    "translated_sentences = generator_pt.translate(sentence_to_translate)\n",
    "# translated_sentences = generator_pt.translate(['Je suis sriram', 'Bon après @-@ midi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bon matin , je vais faire un pique @-@ nique demain .',\n",
       " 'Ceci est en anglais']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_to_translate_toks = generator_pt.tokenize(sentence_to_translate)\n",
    "sentence_to_translate_bpe = [generator_pt.apply_bpe(sentence_to_translate_tok) for sentence_to_translate_tok in sentence_to_translate_toks]\n",
    "sentence_to_translate_bnrz = [generator_pt.binarize(sentence_to_translate_bpe_i) for sentence_to_translate_bpe_i in sentence_to_translate_bpe]\n",
    "\n",
    "translated_sentences_using_generate = generator_pt.generate(sentence_to_translate_bnrz, beam=5, sampling=True, sampling_topk=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good Morning, I am going to a picnic tomorrow', 'This is in English']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_translate_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good Mor@@ n@@ ing@@ , I am going to a pic@@ nic tomorrow',\n",
       " 'This is in English']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_translate_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 9938,  5384,   668,  3440,     4,    99,  1157,  3415,    13,    18,\n",
       "         11238, 36300, 13653,     2]),\n",
       " tensor([ 159,   37,   21, 1891,    2])]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_translate_bnrz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(translated_sentences_using_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translated_sentences_using_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "tensor([  255,  7243, 33597,   342,     4,   471, 13772,   184,    33, 24715,\n",
      "           17, 35429, 11548,     7,     2])\n",
      "<class 'list'>\n",
      "tensor([ 159,   37,   21, 1891,    2])\n"
     ]
    }
   ],
   "source": [
    "for ii in translated_sentences_using_generate:\n",
    "    print(type(ii))\n",
    "    print(ii[0]['tokens'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tokens': tensor([  255,  7243, 33597,   342,     4,   471, 13772,   184,    33, 24715,\n",
       "             17, 35429, 11548,     7,     2]),\n",
       "  'score': tensor(-0.5233),\n",
       "  'attention': tensor([[4.9452e-01, 1.3585e-01, 4.4123e-02, 1.2497e-02, 2.9693e-02, 1.6742e-02,\n",
       "           6.3386e-03, 3.8226e-03, 3.2419e-03, 6.9201e-03, 6.9377e-04, 5.3205e-04,\n",
       "           1.0833e-02, 1.4092e-02, 3.2125e-02],\n",
       "          [2.1300e-01, 1.0203e-01, 3.4556e-01, 1.8764e-02, 3.6897e-02, 7.9515e-03,\n",
       "           6.2303e-03, 4.0561e-03, 3.3481e-03, 2.6904e-03, 6.3988e-03, 2.2656e-03,\n",
       "           9.0324e-03, 3.1794e-02, 2.2253e-02],\n",
       "          [6.7006e-03, 1.4069e-02, 4.7352e-02, 9.6709e-03, 3.3410e-03, 4.1474e-04,\n",
       "           9.2928e-04, 2.1604e-04, 2.7235e-04, 2.4749e-03, 8.3939e-03, 4.0745e-03,\n",
       "           4.5135e-04, 1.8908e-03, 3.0161e-03],\n",
       "          [1.1468e-02, 9.0962e-03, 4.7761e-02, 3.6612e-02, 7.4676e-03, 2.5475e-04,\n",
       "           7.1450e-04, 5.1992e-04, 4.8983e-04, 7.7422e-04, 3.2650e-03, 2.1249e-03,\n",
       "           4.8863e-04, 1.3862e-03, 1.7509e-03],\n",
       "          [1.5785e-02, 1.1480e-02, 1.1597e-02, 4.6172e-02, 3.1943e-02, 3.4167e-03,\n",
       "           4.8304e-03, 2.9877e-03, 1.2539e-02, 1.3628e-03, 4.0896e-03, 3.3269e-03,\n",
       "           3.0235e-03, 4.5646e-03, 1.2348e-02],\n",
       "          [1.7507e-02, 4.9424e-03, 3.4959e-03, 4.9837e-03, 9.4726e-02, 7.7688e-02,\n",
       "           2.1400e-02, 2.1758e-02, 1.6476e-02, 4.0152e-03, 3.0643e-03, 1.1300e-03,\n",
       "           8.4758e-03, 9.7967e-03, 1.3320e-02],\n",
       "          [7.1104e-03, 2.1022e-03, 1.2851e-03, 1.7082e-03, 1.3429e-02, 3.2266e-02,\n",
       "           3.9142e-02, 6.7775e-03, 4.8046e-03, 1.3892e-03, 1.0517e-03, 2.7473e-04,\n",
       "           4.5252e-03, 5.6981e-03, 3.9211e-03],\n",
       "          [6.8948e-03, 3.2747e-03, 2.2398e-03, 8.8357e-04, 9.9391e-03, 6.3334e-02,\n",
       "           1.5022e-01, 2.3670e-02, 4.8639e-03, 1.8768e-03, 1.1476e-03, 3.6736e-04,\n",
       "           6.1119e-03, 4.3485e-03, 3.6103e-03],\n",
       "          [4.3452e-03, 8.7167e-04, 2.1475e-03, 1.0176e-03, 8.2548e-03, 3.1301e-02,\n",
       "           9.8300e-02, 4.7556e-02, 8.4417e-03, 4.4231e-03, 2.0442e-03, 9.5991e-04,\n",
       "           7.7634e-03, 6.8991e-03, 2.2943e-03],\n",
       "          [2.5610e-03, 1.9285e-03, 5.5399e-04, 1.1661e-03, 2.9941e-03, 1.6388e-02,\n",
       "           7.7654e-03, 2.0114e-02, 2.5979e-02, 7.7210e-03, 3.1078e-03, 1.2152e-03,\n",
       "           6.3696e-03, 6.2574e-03, 2.2921e-03],\n",
       "          [1.2490e-02, 1.9890e-03, 6.9380e-03, 1.1110e-03, 8.3135e-03, 1.5091e-01,\n",
       "           2.5965e-01, 3.7147e-01, 1.5584e-01, 4.9253e-01, 4.1873e-02, 1.5391e-02,\n",
       "           3.4477e-02, 1.8810e-02, 7.5861e-03],\n",
       "          [6.2965e-03, 1.3842e-03, 1.6471e-02, 3.5582e-03, 4.5438e-03, 3.0249e-02,\n",
       "           9.5195e-02, 1.6576e-01, 6.4823e-02, 1.8889e-01, 2.2182e-01, 1.3026e-01,\n",
       "           1.6077e-02, 1.0226e-02, 3.5268e-03],\n",
       "          [5.2391e-02, 3.5404e-03, 1.4162e-02, 5.5330e-03, 1.1304e-01, 3.5947e-01,\n",
       "           4.8792e-02, 1.6119e-01, 2.0529e-01, 3.1840e-02, 3.7801e-02, 4.3381e-03,\n",
       "           5.1791e-01, 6.8728e-02, 1.8481e-02],\n",
       "          [1.4893e-01, 7.0745e-01, 4.5632e-01, 8.5632e-01, 6.3542e-01, 2.0962e-01,\n",
       "           2.6049e-01, 1.7009e-01, 4.9359e-01, 2.5310e-01, 6.6525e-01, 8.3374e-01,\n",
       "           3.7446e-01, 8.1551e-01, 8.7348e-01]]),\n",
       "  'alignment': tensor([]),\n",
       "  'positional_scores': tensor([-2.5025, -0.0329, -1.3654, -0.0914, -0.1681, -0.3925, -0.3757, -1.0081,\n",
       "          -0.1596, -0.1244, -0.1265, -0.0547, -0.1203, -1.2203, -0.1066])},\n",
       " {'tokens': tensor([15671,  9255,     4,   471, 13772, 24715,    17,  2511,  6062, 11548,\n",
       "              2]),\n",
       "  'score': tensor(-0.5591),\n",
       "  'attention': tensor([[4.9452e-01, 5.6259e-02, 1.9477e-02, 1.9461e-02, 7.3392e-03, 4.0901e-03,\n",
       "           7.9882e-04, 7.5832e-04, 2.8638e-03, 9.7299e-03, 1.4250e-02],\n",
       "          [2.1300e-01, 3.5108e-01, 2.7485e-02, 1.1486e-02, 8.1608e-03, 5.0400e-03,\n",
       "           5.7423e-03, 2.6725e-03, 2.9799e-03, 7.0694e-03, 2.6872e-02],\n",
       "          [6.7006e-03, 5.5886e-02, 2.5912e-03, 5.1184e-04, 1.1061e-03, 2.4595e-04,\n",
       "           1.0101e-02, 4.1533e-03, 3.9485e-03, 3.6632e-04, 2.3408e-03],\n",
       "          [1.1468e-02, 4.1106e-02, 3.6995e-03, 2.9827e-04, 8.1164e-04, 5.6165e-04,\n",
       "           2.8828e-03, 2.1655e-03, 5.2067e-03, 4.3622e-04, 1.2036e-03],\n",
       "          [1.5785e-02, 1.1369e-02, 2.0424e-02, 4.1021e-03, 5.4424e-03, 3.0335e-03,\n",
       "           4.4972e-03, 3.4021e-03, 3.4744e-03, 2.9834e-03, 5.7456e-03],\n",
       "          [1.7507e-02, 4.2045e-03, 1.0837e-01, 7.4446e-02, 2.1161e-02, 2.0767e-02,\n",
       "           3.3666e-03, 1.9401e-03, 5.9650e-03, 8.7019e-03, 1.2154e-02],\n",
       "          [7.1104e-03, 2.0348e-03, 1.6628e-02, 3.2105e-02, 4.0238e-02, 6.6421e-03,\n",
       "           9.0173e-04, 3.3167e-04, 1.3949e-03, 4.9502e-03, 6.3030e-03],\n",
       "          [6.8948e-03, 3.6693e-03, 1.5626e-02, 6.5765e-02, 1.4696e-01, 2.3524e-02,\n",
       "           1.3408e-03, 5.3857e-04, 2.1123e-03, 6.9246e-03, 4.6862e-03],\n",
       "          [4.3452e-03, 2.9986e-03, 7.4859e-03, 2.4782e-02, 9.3963e-02, 4.6610e-02,\n",
       "           2.1011e-03, 1.3872e-03, 2.6704e-03, 9.1755e-03, 8.3706e-03],\n",
       "          [2.5610e-03, 9.6910e-04, 2.8692e-03, 1.3800e-02, 7.4511e-03, 1.9350e-02,\n",
       "           2.6011e-03, 1.2999e-03, 2.5553e-03, 9.6608e-03, 7.8001e-03],\n",
       "          [1.2490e-02, 1.3569e-02, 8.6237e-03, 7.9213e-02, 2.5827e-01, 3.7561e-01,\n",
       "           2.6684e-02, 1.0261e-02, 9.0631e-03, 3.7429e-02, 1.5246e-02],\n",
       "          [6.2965e-03, 1.7730e-02, 4.3749e-03, 1.6377e-02, 9.4457e-02, 1.6412e-01,\n",
       "           1.6046e-01, 1.0061e-01, 4.0935e-02, 1.8891e-02, 7.3594e-03],\n",
       "          [5.2391e-02, 2.0911e-02, 7.7788e-02, 4.0167e-01, 4.8674e-02, 1.6623e-01,\n",
       "           2.6721e-02, 6.5688e-03, 2.4276e-02, 5.1313e-01, 5.2340e-02],\n",
       "          [1.4893e-01, 4.1821e-01, 6.8455e-01, 2.5598e-01, 2.6597e-01, 1.6417e-01,\n",
       "           7.5180e-01, 8.6392e-01, 8.9255e-01, 3.7056e-01, 8.3533e-01]]),\n",
       "  'alignment': tensor([]),\n",
       "  'positional_scores': tensor([-1.6289, -0.5985, -0.1416, -0.3728, -0.3292, -1.7104, -0.1713, -0.1504,\n",
       "          -0.0858, -0.1177, -0.8442])},\n",
       " {'tokens': tensor([ 9938,  5384,   668,  3440,     4,   471, 13772, 24715,    17,  2511,\n",
       "           6062, 11548,     2]),\n",
       "  'score': tensor(-0.6054),\n",
       "  'attention': tensor([[4.9452e-01, 3.0220e-02, 2.8470e-03, 3.0358e-03, 4.7792e-02, 2.2591e-02,\n",
       "           6.1530e-03, 3.5249e-03, 6.8754e-04, 5.6994e-04, 2.5654e-03, 9.8538e-03,\n",
       "           2.6733e-02],\n",
       "          [2.1300e-01, 6.3899e-01, 2.0579e-02, 4.3916e-03, 4.6456e-02, 5.7601e-03,\n",
       "           5.7959e-03, 3.8887e-03, 6.0429e-03, 2.1682e-03, 2.7018e-03, 6.8090e-03,\n",
       "           4.7888e-02],\n",
       "          [6.7006e-03, 2.6052e-02, 6.4290e-01, 5.5067e-02, 1.9414e-03, 3.8694e-04,\n",
       "           8.8165e-04, 2.4659e-04, 1.0971e-02, 3.8729e-03, 4.2754e-03, 3.3618e-04,\n",
       "           3.1956e-03],\n",
       "          [1.1468e-02, 9.9867e-03, 5.9223e-02, 7.1334e-01, 6.8192e-03, 1.6378e-04,\n",
       "           5.7497e-04, 4.6332e-04, 3.4547e-03, 1.8338e-03, 5.5430e-03, 4.3924e-04,\n",
       "           2.1628e-03],\n",
       "          [1.5785e-02, 6.2012e-03, 4.9153e-03, 6.6192e-03, 8.5448e-02, 5.6054e-03,\n",
       "           5.3192e-03, 3.0720e-03, 3.9976e-03, 3.2887e-03, 3.4426e-03, 3.1605e-03,\n",
       "           6.4639e-03],\n",
       "          [1.7507e-02, 9.2210e-04, 1.4375e-03, 6.6218e-04, 1.7163e-01, 1.8895e-01,\n",
       "           2.4002e-02, 2.2553e-02, 4.2623e-03, 2.0959e-03, 6.8640e-03, 8.5075e-03,\n",
       "           1.1413e-02],\n",
       "          [7.1104e-03, 1.0005e-03, 1.0281e-03, 7.9046e-04, 4.8052e-02, 6.7149e-02,\n",
       "           5.0605e-02, 7.9251e-03, 1.1730e-03, 3.6002e-04, 1.5243e-03, 5.4377e-03,\n",
       "           5.4833e-03],\n",
       "          [6.8948e-03, 3.0446e-03, 1.0464e-03, 4.2279e-04, 2.4077e-02, 1.0295e-01,\n",
       "           1.6624e-01, 2.8483e-02, 1.7217e-03, 6.3883e-04, 2.2169e-03, 7.4607e-03,\n",
       "           4.5946e-03],\n",
       "          [4.3452e-03, 5.9659e-04, 2.8434e-04, 2.5929e-04, 1.5897e-02, 1.7467e-02,\n",
       "           9.0449e-02, 5.0728e-02, 2.6879e-03, 1.7274e-03, 2.5819e-03, 8.7010e-03,\n",
       "           5.1039e-03],\n",
       "          [2.5610e-03, 1.4647e-04, 1.1975e-04, 1.2689e-04, 4.7880e-03, 1.8735e-02,\n",
       "           8.3896e-03, 2.2759e-02, 3.5229e-03, 1.6691e-03, 2.5247e-03, 1.0256e-02,\n",
       "           5.6874e-03],\n",
       "          [1.2490e-02, 7.5635e-03, 7.9447e-04, 9.3286e-05, 1.5495e-02, 3.5902e-02,\n",
       "           2.6327e-01, 3.8917e-01, 3.0202e-02, 1.1518e-02, 8.2940e-03, 3.2593e-02,\n",
       "           1.4994e-02],\n",
       "          [6.2965e-03, 1.7323e-03, 2.5857e-03, 1.8227e-03, 8.0212e-03, 3.5124e-03,\n",
       "           8.6691e-02, 1.5295e-01, 2.1456e-01, 1.2624e-01, 4.4981e-02, 1.5279e-02,\n",
       "           7.1819e-03],\n",
       "          [5.2391e-02, 4.1073e-03, 5.5998e-04, 2.9453e-04, 2.5603e-01, 3.0189e-01,\n",
       "           5.5501e-02, 1.4717e-01, 5.2398e-02, 8.4476e-03, 2.7056e-02, 5.4315e-01,\n",
       "           4.2668e-02],\n",
       "          [1.4893e-01, 2.6944e-01, 2.6168e-01, 2.1308e-01, 2.6755e-01, 2.2894e-01,\n",
       "           2.3612e-01, 1.6707e-01, 6.6432e-01, 8.3557e-01, 8.8543e-01, 3.4801e-01,\n",
       "           8.1643e-01]]),\n",
       "  'alignment': tensor([]),\n",
       "  'positional_scores': tensor([-3.2100, -0.1353, -0.0742, -0.0348, -0.1764, -0.9509, -0.2821, -1.8296,\n",
       "          -0.2257, -0.2105, -0.0840, -0.1129, -0.5438])},\n",
       " {'tokens': tensor([15671,  5384,   668,  3440,     4, 11548,     4,   471, 13772,   184,\n",
       "             35,  3301,  2084,   639,  3266,     7,     2]),\n",
       "  'score': tensor(-1.0263),\n",
       "  'attention': tensor([[4.9452e-01, 5.6259e-02, 5.0792e-03, 7.2573e-03, 7.2653e-02, 2.8457e-02,\n",
       "           8.9261e-03, 1.7877e-02, 3.0866e-03, 2.3120e-03, 2.5795e-03, 8.5018e-03,\n",
       "           1.2169e-03, 1.2453e-03, 1.0264e-03, 1.6083e-02, 3.5002e-02],\n",
       "          [2.1300e-01, 3.5108e-01, 1.8532e-02, 1.0471e-02, 6.3548e-02, 1.5983e-02,\n",
       "           1.1943e-02, 1.3816e-02, 3.3316e-03, 2.8134e-03, 3.3772e-03, 6.0984e-03,\n",
       "           1.7449e-03, 2.5681e-03, 2.2499e-03, 2.5517e-02, 2.2169e-02],\n",
       "          [6.7006e-03, 5.5886e-02, 1.3613e-01, 3.1292e-02, 2.5247e-03, 6.2571e-04,\n",
       "           7.4194e-04, 8.0098e-04, 7.5323e-04, 3.2253e-04, 3.8574e-04, 3.7365e-03,\n",
       "           7.3801e-03, 1.5369e-02, 1.1072e-03, 2.1753e-03, 2.2663e-03],\n",
       "          [1.1468e-02, 4.1106e-02, 5.1616e-02, 4.9244e-01, 9.9451e-03, 3.0665e-04,\n",
       "           4.2081e-04, 2.8154e-04, 3.6544e-04, 4.9281e-04, 5.4904e-04, 2.5181e-03,\n",
       "           2.6467e-03, 1.1305e-02, 2.4089e-03, 1.5777e-03, 1.3571e-03],\n",
       "          [1.5785e-02, 1.1369e-02, 1.0890e-02, 1.1673e-02, 6.2000e-02, 4.2427e-03,\n",
       "           4.1814e-03, 3.8425e-03, 2.9699e-03, 2.1855e-03, 1.1585e-02, 5.0798e-03,\n",
       "           2.8194e-03, 5.4109e-03, 3.4951e-03, 5.5975e-03, 9.9467e-03],\n",
       "          [1.7507e-02, 4.2045e-03, 4.7663e-03, 1.4410e-03, 1.4279e-01, 7.4329e-02,\n",
       "           8.7462e-02, 7.9352e-02, 1.4157e-02, 1.6313e-02, 1.5698e-02, 7.6005e-03,\n",
       "           1.7898e-03, 1.2251e-03, 4.3353e-03, 1.3944e-02, 1.4204e-02],\n",
       "          [7.1104e-03, 2.0348e-03, 1.2226e-03, 1.3734e-03, 3.1772e-02, 3.1995e-02,\n",
       "           3.2207e-02, 3.6320e-02, 2.8106e-02, 5.1845e-03, 3.7206e-03, 4.1405e-03,\n",
       "           8.1632e-04, 9.0388e-04, 1.2440e-03, 4.4223e-03, 4.1742e-03],\n",
       "          [6.8948e-03, 3.6693e-03, 1.2734e-03, 7.5625e-04, 1.8147e-02, 6.0365e-02,\n",
       "           9.3595e-02, 1.0279e-01, 1.3290e-01, 1.9862e-02, 4.6978e-03, 5.4946e-03,\n",
       "           7.4065e-04, 1.0397e-03, 1.2715e-03, 4.7477e-03, 4.2231e-03],\n",
       "          [4.3452e-03, 2.9986e-03, 1.1190e-03, 6.2289e-04, 1.0303e-02, 1.8460e-02,\n",
       "           4.4368e-02, 5.1403e-02, 9.2261e-02, 4.4943e-02, 9.2566e-03, 1.5373e-02,\n",
       "           1.6599e-03, 3.9947e-03, 2.5944e-03, 8.4394e-03, 2.8002e-03],\n",
       "          [2.5610e-03, 9.6910e-04, 4.5867e-04, 3.1829e-04, 3.4138e-03, 1.1193e-02,\n",
       "           3.0875e-02, 2.5794e-02, 9.9536e-03, 2.5689e-02, 2.9581e-02, 1.8031e-02,\n",
       "           2.6694e-03, 7.8066e-03, 2.7605e-03, 7.9037e-03, 3.0313e-03],\n",
       "          [1.2490e-02, 1.3569e-02, 4.6426e-03, 8.4344e-04, 1.5048e-02, 7.5747e-02,\n",
       "           2.0653e-01, 1.9737e-01, 2.9827e-01, 4.1387e-01, 1.9058e-01, 1.9798e-01,\n",
       "           2.7478e-02, 1.1534e-02, 1.3718e-02, 3.3920e-02, 9.4866e-03],\n",
       "          [6.2965e-03, 1.7730e-02, 3.1866e-02, 1.1207e-02, 7.5041e-03, 1.2779e-02,\n",
       "           5.5805e-02, 4.3820e-02, 1.3076e-01, 2.0393e-01, 8.8197e-02, 2.4039e-01,\n",
       "           1.7969e-01, 1.7327e-01, 6.1815e-02, 1.9863e-02, 4.1371e-03],\n",
       "          [5.2391e-02, 2.0911e-02, 3.3746e-03, 1.9104e-03, 2.4900e-01, 4.1301e-01,\n",
       "           7.2985e-02, 8.3635e-02, 2.2374e-02, 3.8504e-02, 9.0096e-02, 1.4860e-02,\n",
       "           3.3364e-03, 2.5401e-03, 1.7453e-02, 7.9857e-02, 3.8463e-02],\n",
       "          [1.4893e-01, 4.1821e-01, 7.2903e-01, 4.2840e-01, 3.1135e-01, 2.5251e-01,\n",
       "           3.4996e-01, 3.4290e-01, 2.6071e-01, 2.2359e-01, 5.4969e-01, 4.7019e-01,\n",
       "           7.6601e-01, 7.6179e-01, 8.8452e-01, 7.7595e-01, 8.4874e-01]]),\n",
       "  'alignment': tensor([]),\n",
       "  'positional_scores': tensor([-1.6289, -2.1168, -0.1762, -0.0536, -0.1989, -1.6933, -1.0124, -0.1566,\n",
       "          -0.2146, -0.8729, -5.3129, -2.2317, -0.4475, -0.0730, -0.0110, -1.1407,\n",
       "          -0.1060])},\n",
       " {'tokens': tensor([ 9804,   391,  5384,   668,  3440,     4,   594,   167,  3473,    16,\n",
       "          24715,    17, 35429, 11548,     2]),\n",
       "  'score': tensor(-1.2876),\n",
       "  'attention': tensor([[4.9452e-01, 2.5984e-02, 1.1782e-01, 6.1621e-03, 5.5261e-03, 6.6141e-02,\n",
       "           2.7113e-02, 6.8943e-03, 7.7316e-03, 1.2492e-02, 5.2158e-03, 7.3074e-04,\n",
       "           2.5597e-04, 1.2825e-02, 1.9331e-02],\n",
       "          [2.1300e-01, 1.4918e-01, 4.2657e-01, 3.0362e-02, 9.4852e-03, 5.0475e-02,\n",
       "           1.2391e-02, 6.9667e-03, 4.3087e-03, 8.4781e-03, 3.5268e-03, 6.1755e-03,\n",
       "           1.3280e-03, 9.7029e-03, 3.6307e-02],\n",
       "          [6.7006e-03, 1.3594e-02, 7.3953e-02, 2.6499e-01, 3.8645e-02, 3.0864e-03,\n",
       "           5.4794e-04, 9.3570e-04, 5.4370e-04, 2.8462e-04, 4.4805e-04, 5.0421e-03,\n",
       "           2.0732e-03, 3.7574e-04, 1.9182e-03],\n",
       "          [1.1468e-02, 5.9789e-03, 6.1731e-02, 9.6338e-02, 5.5497e-01, 6.6066e-03,\n",
       "           3.3416e-04, 7.7323e-04, 5.2094e-04, 3.5182e-04, 5.9035e-04, 2.0453e-03,\n",
       "           1.2223e-03, 4.2413e-04, 1.8194e-03],\n",
       "          [1.5785e-02, 5.5890e-03, 1.4403e-02, 9.3204e-03, 9.8283e-03, 5.8738e-02,\n",
       "           4.4632e-03, 5.1573e-03, 1.3211e-02, 5.9626e-03, 2.4654e-03, 3.8413e-03,\n",
       "           2.6909e-03, 3.3071e-03, 5.5208e-03],\n",
       "          [1.7507e-02, 8.9580e-03, 6.0052e-03, 4.0602e-03, 1.3160e-03, 1.3814e-01,\n",
       "           6.7085e-02, 1.8452e-02, 2.3083e-02, 9.0190e-03, 2.0247e-02, 3.4662e-03,\n",
       "           1.1303e-03, 8.4390e-03, 9.7263e-03],\n",
       "          [7.1104e-03, 1.9473e-03, 1.2723e-03, 1.3387e-03, 1.6254e-03, 2.7236e-02,\n",
       "           3.1827e-02, 3.7953e-02, 2.3092e-02, 6.4464e-03, 2.8957e-03, 1.0723e-03,\n",
       "           2.0912e-04, 5.4330e-03, 7.1634e-03],\n",
       "          [6.8948e-03, 1.9310e-03, 1.0688e-03, 1.1657e-03, 8.3187e-04, 1.1590e-02,\n",
       "           5.2135e-02, 1.3885e-01, 5.0340e-02, 6.8642e-03, 8.1444e-03, 1.5821e-03,\n",
       "           3.8961e-04, 6.6485e-03, 4.8155e-03],\n",
       "          [4.3452e-03, 3.9618e-04, 6.1907e-04, 5.4372e-04, 5.3620e-04, 6.7891e-03,\n",
       "           1.9521e-02, 1.0526e-01, 7.8724e-02, 5.3257e-03, 2.9276e-02, 2.6557e-03,\n",
       "           1.1315e-03, 7.1821e-03, 7.5585e-03],\n",
       "          [2.5610e-03, 3.0437e-04, 4.9595e-04, 2.4245e-04, 2.4264e-04, 2.4350e-03,\n",
       "           1.1597e-02, 8.0449e-03, 1.7630e-02, 1.1901e-02, 3.5358e-02, 4.1998e-03,\n",
       "           1.1253e-03, 6.2983e-03, 7.6568e-03],\n",
       "          [1.2490e-02, 5.8952e-03, 2.5250e-03, 1.2157e-03, 4.6886e-04, 8.1377e-03,\n",
       "           6.6546e-02, 2.5764e-01, 1.4363e-01, 6.7368e-02, 3.7445e-01, 3.5131e-02,\n",
       "           1.0788e-02, 3.3011e-02, 1.7862e-02],\n",
       "          [6.2965e-03, 2.4159e-03, 3.1994e-03, 1.1539e-02, 7.2763e-03, 4.1899e-03,\n",
       "           1.2380e-02, 9.9837e-02, 5.5664e-02, 1.8793e-02, 2.2967e-01, 1.5239e-01,\n",
       "           1.2709e-01, 1.4881e-02, 1.0463e-02],\n",
       "          [5.2391e-02, 2.6608e-02, 5.6384e-03, 1.5878e-03, 9.5665e-04, 1.6779e-01,\n",
       "           4.6189e-01, 4.9559e-02, 2.6333e-02, 5.1201e-01, 5.4940e-02, 3.3024e-02,\n",
       "           4.6159e-03, 5.3300e-01, 1.0288e-01],\n",
       "          [1.4893e-01, 7.5122e-01, 2.8470e-01, 5.7113e-01, 3.6829e-01, 4.4865e-01,\n",
       "           2.3217e-01, 2.6367e-01, 5.5519e-01, 3.3470e-01, 2.3277e-01, 7.4864e-01,\n",
       "           8.4595e-01, 3.5848e-01, 7.6698e-01]]),\n",
       "  'alignment': tensor([]),\n",
       "  'positional_scores': tensor([-0.9569, -0.1617, -2.4856, -0.0395, -0.0231, -0.4435, -2.0817, -5.2372,\n",
       "          -1.0959, -0.5182, -2.8461, -0.1395, -1.8711, -0.1677, -1.2471])}]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_sentences_using_generate[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tokens': tensor([ 159,   37,   21, 1891,    2]),\n",
       "  'score': tensor(-0.6456),\n",
       "  'attention': tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3830, 0.1308, 0.0302, 0.0585, 0.0422],\n",
       "          [0.1891, 0.2644, 0.0409, 0.0297, 0.0499],\n",
       "          [0.0771, 0.1522, 0.2233, 0.0449, 0.0304],\n",
       "          [0.0461, 0.0111, 0.0307, 0.2820, 0.0096],\n",
       "          [0.3046, 0.4415, 0.6749, 0.5849, 0.8679]]),\n",
       "  'alignment': tensor([]),\n",
       "  'positional_scores': tensor([-2.1030, -0.0982, -0.0385, -0.4934, -0.4947])},\n",
       " {'tokens': tensor([  77,  734, 1453,   41,   25, 1966,    7,    2]),\n",
       "  'score': tensor(-1.0348),\n",
       "  'attention': tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3830, 0.2185, 0.1499, 0.0204, 0.0183, 0.0029, 0.0175, 0.0110],\n",
       "          [0.1891, 0.0987, 0.1055, 0.0289, 0.0272, 0.0051, 0.0095, 0.0078],\n",
       "          [0.0771, 0.0819, 0.0827, 0.0573, 0.0996, 0.0200, 0.0106, 0.0069],\n",
       "          [0.0461, 0.0258, 0.0272, 0.0215, 0.1076, 0.3307, 0.0162, 0.0140],\n",
       "          [0.3046, 0.5751, 0.6347, 0.8719, 0.7473, 0.6413, 0.9462, 0.9604]]),\n",
       "  'alignment': tensor([]),\n",
       "  'positional_scores': tensor([-3.6837, -0.5436, -0.7490, -0.1219, -1.4349, -0.1018, -1.5388, -0.1050])},\n",
       " {'tokens': tensor([  77,  734, 2606,   41,   25, 1966,    2]),\n",
       "  'score': tensor(-1.4426),\n",
       "  'attention': tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3830, 0.2185, 0.1499, 0.0201, 0.0226, 0.0029, 0.0176],\n",
       "          [0.1891, 0.0987, 0.1055, 0.0388, 0.0296, 0.0056, 0.0095],\n",
       "          [0.0771, 0.0819, 0.0827, 0.0766, 0.1208, 0.0204, 0.0102],\n",
       "          [0.0461, 0.0258, 0.0272, 0.0203, 0.1186, 0.3484, 0.0148],\n",
       "          [0.3046, 0.5751, 0.6347, 0.8443, 0.7084, 0.6226, 0.9478]]),\n",
       "  'alignment': tensor([]),\n",
       "  'positional_scores': tensor([-3.6837, -0.5436, -4.4341, -0.1055, -0.6833, -0.0969, -0.5511])},\n",
       " {'tokens': tensor([  98, 5150,   41, 3212,   25, 1966,    7,    2]),\n",
       "  'score': tensor(-1.4427),\n",
       "  'attention': tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3830, 0.2069, 0.0644, 0.0496, 0.0170, 0.0041, 0.0178, 0.0109],\n",
       "          [0.1891, 0.1000, 0.0763, 0.0462, 0.0105, 0.0055, 0.0101, 0.0079],\n",
       "          [0.0771, 0.0483, 0.1159, 0.1152, 0.0384, 0.0211, 0.0106, 0.0069],\n",
       "          [0.0461, 0.0252, 0.0230, 0.0879, 0.0641, 0.3275, 0.0176, 0.0146],\n",
       "          [0.3046, 0.6197, 0.7204, 0.7011, 0.8700, 0.6417, 0.9438, 0.9597]]),\n",
       "  'alignment': tensor([]),\n",
       "  'positional_scores': tensor([-4.1215, -2.0197, -0.2106, -3.8464, -0.1073, -0.0996, -1.0312, -0.1053])},\n",
       " {'tokens': tensor([3372,  862,   41,   25, 1966,    2]),\n",
       "  'score': tensor(-1.7388),\n",
       "  'attention': tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3830, 0.2265, 0.0361, 0.0501, 0.0054, 0.0252],\n",
       "          [0.1891, 0.1262, 0.0572, 0.0597, 0.0102, 0.0148],\n",
       "          [0.0771, 0.0473, 0.0778, 0.1025, 0.0311, 0.0135],\n",
       "          [0.0461, 0.0306, 0.0263, 0.0753, 0.3256, 0.0182],\n",
       "          [0.3046, 0.5694, 0.8026, 0.7124, 0.6277, 0.9282]]),\n",
       "  'alignment': tensor([]),\n",
       "  'positional_scores': tensor([-4.4828, -3.9344, -0.5096, -0.8261, -0.1515, -0.5282])}]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_sentences_using_generate[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(translated_sentences_using_generate[0])\n",
    "# translated_sentences_using_generate[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translated_sentences_using_generate_generated_toks = translated_sentences_using_generate[0]['tokens'] # single sentence\n",
    "translated_Sentences_using_generate_generated_toks = [translated_sentences_using_generate_i[0]['tokens'] for translated_sentences_using_generate_i in translated_sentences_using_generate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  255,  7243, 33597,   342,     4,   471, 13772,   184,    33, 24715,\n",
       "            17, 35429, 11548,     7,     2]),\n",
       " tensor([ 159,   37,   21, 1891,    2])]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_Sentences_using_generate_generated_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_sentences_using_generate_generated_toks_str_with_bpe = generator_pt.string(translated_sentences_using_generate_generated_toks)\n",
    "translated_sentences_using_generate_generated_toks_str_bpe_removed = generator_pt.remove_bpe(translated_sentences_using_generate_generated_toks_str_with_bpe)\n",
    "fr = generator_pt.detokenize(translated_sentences_using_generate_generated_toks_str_bpe_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour , demain , je vais faire un pique @-@ nique'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fr_decoded = generator_pt.decode(translated_sentences_using_generate_generated_toks)\n",
    "fr_decoded = [generator_pt.decode(translated_Sentences_using_generate_generated_toks_i) for translated_Sentences_using_generate_generated_toks_i in  translated_Sentences_using_generate_generated_toks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonne matinée , je vais faire un pique @-@ nique demain .',\n",
       " 'This is in English']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A little detour for KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "max_length = 128\n",
    "# bert_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "checkpoint = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "fr_decoded_bert = bert_tokenizer(fr_decoded, padding ='max_length' ,truncation=True ,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 3982, 10243,    49,     2,     5, 12207,    49,     2,   157,   740,\n",
       "           900,  3921,    51,    34,   357,  1754,  8941,    21,  1802,    49,\n",
       "         10669,     0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_decoded_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_decoded_bert.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_decoded_bert['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_train = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good Morning, I am going to a picnic tomorrow'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_sentence = \"Bonjour, je vais pique-niquer demain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_to_translate_bert = bert_tokenizer(sentence_to_translate, padding ='max_length' ,truncation=True ,return_tensors=\"pt\")\n",
    "tgt_sentence_bert = bert_tokenizer(tgt_sentence, padding ='max_length' ,truncation=True ,return_tensors=\"pt\")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_translate_bert.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3703, 20894,     2,    47,  1010,  1124,    12,    15, 42765,  7806,\n",
       "             0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_translate_bert.input_ids        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of generator2_train_out <class 'transformers.modeling_outputs.Seq2SeqLMOutput'>\n",
      " generator2_train_keys()  odict_keys(['logits', 'past_key_values', 'decoder_hidden_states', 'encoder_last_hidden_state', 'encoder_hidden_states'])\n",
      "generator2_train_out logits shape  torch.Size([1, 512, 59514])\n",
      "generator2_train_out decoder_hidden_states shape  (tensor([[[ 1.1242, -0.5911,  0.2952,  ...,  0.0113, -0.1090, -0.6437],\n",
      "         [ 2.0119,  0.8947, -0.5386,  ..., -2.0128, -1.2236, -0.6353],\n",
      "         [ 1.2007,  1.4186,  1.1736,  ...,  0.3132, -0.4540,  0.3406],\n",
      "         ...,\n",
      "         [ 0.0620,  0.7982,  0.6589,  ...,  0.9984,  0.9985,  0.9986],\n",
      "         [ 0.8733,  0.9498, -0.2097,  ...,  0.9984,  0.9985,  0.9986],\n",
      "         [ 0.8818,  0.2840, -0.9094,  ...,  0.9984,  0.9985,  0.9986]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[ 0.5952,  1.0051,  0.3736,  ..., -0.5990, -1.4175,  0.6018],\n",
      "         [ 1.3035,  0.9586,  0.3943,  ..., -0.8104, -1.3256,  0.5475],\n",
      "         [ 0.0805, -0.6850,  0.1649,  ..., -0.6276,  0.0400,  0.5172],\n",
      "         ...,\n",
      "         [-0.1155,  0.4436, -0.1103,  ...,  0.6022,  0.1969,  0.5453],\n",
      "         [-0.0111,  0.3434, -0.2560,  ...,  0.5161,  0.1499,  0.4987],\n",
      "         [ 0.0488,  0.1735, -0.3420,  ...,  0.4311,  0.1108,  0.4097]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0149,  0.4055, -0.1127,  ...,  0.4063, -1.1391,  0.9669],\n",
      "         [ 0.5175,  0.5776,  0.0865,  ...,  0.4188, -0.9275,  0.2169],\n",
      "         [-0.0515, -0.4479,  0.2475,  ..., -0.3035, -0.2607,  0.7030],\n",
      "         ...,\n",
      "         [-0.0127,  0.0896,  0.0033,  ...,  0.1065, -0.0301,  0.1935],\n",
      "         [ 0.0153,  0.0449, -0.0540,  ...,  0.0509, -0.0224,  0.1978],\n",
      "         [ 0.0218, -0.0298, -0.0978,  ..., -0.0063,  0.0096,  0.1736]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1143,  0.2862, -0.4488,  ..., -0.1723, -0.3606,  0.5666],\n",
      "         [ 0.1572,  0.3861, -0.2145,  ..., -0.2304, -0.3117, -0.0259],\n",
      "         [ 0.3970, -0.4837,  0.2402,  ..., -0.6037, -0.1490,  0.6054],\n",
      "         ...,\n",
      "         [ 0.0845,  0.0025, -0.0235,  ...,  0.0341,  0.0233,  0.0916],\n",
      "         [ 0.0832,  0.0007, -0.0326,  ...,  0.0237,  0.0193,  0.0884],\n",
      "         [ 0.0794, -0.0074, -0.0400,  ...,  0.0130,  0.0201,  0.0801]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2133,  0.0199,  1.1656,  ..., -0.0856, -0.1212,  0.8812],\n",
      "         [-0.0757, -0.0721,  0.7826,  ..., -0.1460,  0.0726,  0.0276],\n",
      "         [ 0.2343, -0.5181,  0.7385,  ..., -0.9107,  0.1309,  0.2441],\n",
      "         ...,\n",
      "         [ 0.0492, -0.0473,  0.0118,  ..., -0.0724,  0.0389,  0.0816],\n",
      "         [ 0.0486, -0.0486,  0.0091,  ..., -0.0737,  0.0379,  0.0801],\n",
      "         [ 0.0479, -0.0508,  0.0067,  ..., -0.0746,  0.0374,  0.0774]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.5617,  0.1504,  0.3020,  ..., -0.9557, -0.9756,  0.3733],\n",
      "         [ 0.5750,  0.1516, -0.0751,  ..., -1.2181, -0.4434,  0.0590],\n",
      "         [ 0.9689, -0.2803, -0.4263,  ..., -1.7241, -0.6920,  0.1370],\n",
      "         ...,\n",
      "         [ 0.2284,  0.0561, -0.1849,  ..., -0.3268, -0.2131, -0.0495],\n",
      "         [ 0.2275,  0.0564, -0.1855,  ..., -0.3269, -0.2128, -0.0500],\n",
      "         [ 0.2269,  0.0565, -0.1863,  ..., -0.3267, -0.2124, -0.0511]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.5622,  1.0025,  1.7159,  ..., -1.2343, -3.8219, -0.0163],\n",
      "         [-0.1743,  0.1469,  0.1438,  ..., -1.9979, -2.5289, -0.9437],\n",
      "         [ 0.8570, -1.1225, -0.7957,  ..., -5.6580, -3.1391,  0.3472],\n",
      "         ...,\n",
      "         [-1.2501,  0.4241,  0.4642,  ..., -3.1925, -1.6552, -1.2097],\n",
      "         [-1.2548,  0.4202,  0.4577,  ..., -3.2011, -1.6533, -1.2158],\n",
      "         [-1.2604,  0.4147,  0.4517,  ..., -3.2044, -1.6501, -1.2239]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>))\n",
      "generator2_train_out encoder_last_hidden_state shape  torch.Size([1, 512, 512])\n",
      "generator2_train_out encoder_hidden_states shape  (tensor([[[ 0.4847,  0.1435, -0.2029,  ..., -0.0678,  1.2087,  0.5896],\n",
      "         [ 0.6528,  0.6530,  0.8030,  ...,  0.8794,  0.3590,  1.8567],\n",
      "         [ 1.2007,  1.4186,  1.1736,  ...,  0.3132, -0.4540,  0.3406],\n",
      "         ...,\n",
      "         [ 0.0620,  0.7982,  0.6589,  ...,  0.9984,  0.9985,  0.9986],\n",
      "         [ 0.8733,  0.9498, -0.2097,  ...,  0.9984,  0.9985,  0.9986],\n",
      "         [ 0.8818,  0.2840, -0.9094,  ...,  0.9984,  0.9985,  0.9986]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[ 0.0477, -0.8025, -0.6532,  ..., -0.0822,  1.5818, -0.0937],\n",
      "         [ 0.7258,  2.0953,  0.5182,  ..., -0.2550,  0.3001,  0.8188],\n",
      "         [ 0.5411,  1.6435,  1.2141,  ..., -0.4317, -0.7530, -0.2505],\n",
      "         ...,\n",
      "         [-0.3132,  0.0530,  1.1846,  ..., -0.2124,  1.6865,  1.1028],\n",
      "         [ 0.2221,  0.2002,  0.3922,  ..., -0.1621,  1.6970,  1.0404],\n",
      "         [ 0.1882, -0.4026, -0.2980,  ..., -0.0973,  1.6623,  1.0519]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.5069, -0.7225, -0.5559,  ...,  0.2303,  1.4372, -0.3993],\n",
      "         [ 0.6354,  2.8916, -0.2403,  ..., -0.2628,  0.4632,  0.8550],\n",
      "         [ 0.3632,  1.6236,  0.7653,  ...,  0.1655, -0.5918, -0.7316],\n",
      "         ...,\n",
      "         [-0.2288,  0.1812,  1.0771,  ...,  0.1205,  1.4572,  1.0034],\n",
      "         [ 0.1753,  0.3414,  0.3823,  ...,  0.1426,  1.4484,  0.9891],\n",
      "         [ 0.1288, -0.3003, -0.3674,  ...,  0.1729,  1.3383,  0.9875]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0091, -0.3714, -0.0614,  ..., -0.2120,  1.4363, -0.4451],\n",
      "         [ 0.7253,  2.6209,  0.0182,  ..., -0.5105,  0.3704,  0.9678],\n",
      "         [ 0.1950,  1.2183,  0.8839,  ...,  0.0419, -0.4168, -0.6890],\n",
      "         ...,\n",
      "         [-0.0997, -0.2732,  0.8139,  ...,  0.5050,  1.0165,  1.4689],\n",
      "         [ 0.0977, -0.2341,  0.2993,  ...,  0.4602,  0.9333,  1.3624],\n",
      "         [ 0.0643, -0.7032, -0.2203,  ...,  0.4022,  0.7419,  1.3059]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3825,  0.0196,  0.6239,  ..., -0.4471,  1.5661, -0.3382],\n",
      "         [ 0.7789,  2.1414, -0.4322,  ..., -1.1397,  0.9091,  0.6294],\n",
      "         [ 0.1561,  0.9725,  0.6377,  ..., -0.3544, -0.4362,  0.0528],\n",
      "         ...,\n",
      "         [-0.1446, -0.7873,  0.6258,  ...,  0.0559,  1.0239,  1.6032],\n",
      "         [-0.0353, -0.6623,  0.3328,  ..., -0.0653,  0.7192,  1.3505],\n",
      "         [-0.0478, -1.0432, -0.1128,  ..., -0.0355,  0.5062,  1.1947]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0820,  0.0751,  0.4589,  ..., -0.2652,  1.5219, -0.7285],\n",
      "         [ 0.2029,  1.8386, -0.4050,  ..., -0.5408,  1.0688, -0.1780],\n",
      "         [ 0.7193,  0.7241,  0.2206,  ..., -0.5572,  0.4090,  0.2128],\n",
      "         ...,\n",
      "         [-0.3445, -0.8237,  0.2858,  ..., -0.0365,  1.3386,  0.8089],\n",
      "         [-0.1412, -0.5112,  0.3972,  ...,  0.1578,  1.1245,  0.7558],\n",
      "         [-0.0120, -0.6912,  0.2100,  ...,  0.2367,  0.7416,  0.6027]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.3028,  0.0135,  0.0763,  ..., -0.0694,  0.4502, -0.2770],\n",
      "         [-0.1250,  0.5794, -0.1913,  ..., -0.1061,  0.3896, -0.3465],\n",
      "         [ 0.4125,  0.0409, -0.0275,  ..., -0.2623,  0.3052,  0.2183],\n",
      "         ...,\n",
      "         [-0.3337, -0.2939,  0.1088,  ...,  0.0980,  0.4361,  0.2195],\n",
      "         [-0.2032, -0.1513,  0.1005,  ...,  0.0870,  0.3761,  0.2100],\n",
      "         [-0.1295, -0.1901,  0.0284,  ...,  0.0777,  0.2793,  0.2068]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>))\n",
      "generator2_train_out past_key_values shape  ((tensor([[[[-1.7255,  0.3406, -1.6776,  ..., -3.8218,  0.3778, -0.0964],\n",
      "          [-0.6952,  1.0930, -2.2104,  ..., -3.4020, -0.2680,  1.6821],\n",
      "          [ 0.5817,  0.0819, -0.6086,  ..., -1.9847,  0.2937,  0.0937],\n",
      "          ...,\n",
      "          [ 0.8068,  1.1424, -0.8260,  ...,  0.2701, -1.4216, -0.2174],\n",
      "          [ 0.7050,  0.9432, -0.6576,  ...,  0.1023, -1.4695, -0.3192],\n",
      "          [ 0.5204,  0.8176, -0.5507,  ..., -0.0776, -1.4240, -0.3731]],\n",
      "\n",
      "         [[ 0.3053, -0.2793,  1.0258,  ...,  1.9254,  2.4130,  1.0567],\n",
      "          [-1.5314, -0.7070,  0.8596,  ...,  1.5461, -0.1586,  1.7947],\n",
      "          [ 0.1036, -1.1786, -0.9772,  ...,  0.0482,  1.4753,  0.8291],\n",
      "          ...,\n",
      "          [-1.9351,  0.3586,  1.2884,  ...,  1.9964,  0.3813,  1.5236],\n",
      "          [-2.1723,  0.6529,  1.2708,  ...,  1.4824,  0.2927,  1.6268],\n",
      "          [-2.2849,  0.8453,  1.2688,  ...,  1.0193,  0.2570,  1.6058]],\n",
      "\n",
      "         [[ 0.2122,  1.4035,  1.1416,  ..., -0.9279,  0.5724, -0.9996],\n",
      "          [-0.7744,  1.3847,  0.2941,  ...,  0.1294, -0.0096, -1.2978],\n",
      "          [-0.0422,  0.4932,  0.8114,  ..., -0.1568,  0.8739, -0.8377],\n",
      "          ...,\n",
      "          [ 1.5583, -0.2866,  0.3669,  ..., -1.5830,  0.1092,  0.5002],\n",
      "          [ 1.6026, -0.2370,  0.4433,  ..., -1.4871,  0.2462,  0.7700],\n",
      "          [ 1.5485,  0.0848,  0.6146,  ..., -1.2605,  0.2009,  0.8967]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2495, -0.8897,  0.4774,  ..., -0.9737, -0.2304, -0.2174],\n",
      "          [ 1.7478, -0.8087, -0.3051,  ...,  0.4200,  0.1544, -1.7005],\n",
      "          [ 1.7864, -1.7723, -1.0901,  ..., -0.1789, -0.5594, -2.0870],\n",
      "          ...,\n",
      "          [-1.5376,  2.8198, -0.6384,  ..., -1.0113, -0.6272,  0.3075],\n",
      "          [-1.4610,  2.6993, -0.6337,  ..., -0.4107, -0.2337,  0.2679],\n",
      "          [-0.8970,  2.1968, -0.4417,  ...,  0.0225,  0.2352,  0.3300]],\n",
      "\n",
      "         [[-0.3141, -1.0047,  0.1105,  ..., -1.1976,  2.0913,  0.6218],\n",
      "          [-0.4355,  1.0565,  0.1911,  ...,  0.7007,  2.1683,  1.1392],\n",
      "          [-0.1426, -0.8794,  0.9654,  ...,  1.0532,  0.4626,  0.2242],\n",
      "          ...,\n",
      "          [ 0.1421, -0.5301,  0.2194,  ..., -0.6394, -0.3435, -0.6209],\n",
      "          [ 0.1326, -0.2139,  0.2848,  ..., -0.4995,  0.0430, -0.6391],\n",
      "          [ 0.0780,  0.1364,  0.1362,  ..., -0.2630,  0.4071, -0.7767]],\n",
      "\n",
      "         [[-1.0122, -0.1977,  0.2035,  ...,  1.4513, -0.1194, -0.6443],\n",
      "          [-0.6305,  0.6063, -0.7457,  ...,  1.6310, -0.7974,  0.3606],\n",
      "          [-0.3493, -0.3421, -0.3564,  ...,  0.8927, -0.4077,  0.1988],\n",
      "          ...,\n",
      "          [-0.0769, -0.5834, -0.1159,  ..., -0.9039,  0.9941,  1.8400],\n",
      "          [-0.0900, -0.3945,  0.0652,  ..., -0.8883,  1.0043,  1.7010],\n",
      "          [-0.2028, -0.2086,  0.1962,  ..., -0.8931,  0.9972,  1.5439]]]],\n",
      "       grad_fn=<CloneBackward0>), tensor([[[[-1.8780e+00,  9.2275e-01, -9.3211e-01,  ...,  2.8334e-01,\n",
      "            1.2483e-02,  1.2115e-01],\n",
      "          [-1.2543e+00,  1.2914e+00,  6.1543e-01,  ...,  8.6288e-01,\n",
      "            7.3216e-01, -1.6303e-01],\n",
      "          [-1.3008e-01, -4.8431e-01, -6.2338e-02,  ...,  1.0820e-01,\n",
      "            1.4511e-01,  2.3062e-01],\n",
      "          ...,\n",
      "          [ 2.8260e-01,  2.7474e-01, -8.2312e-01,  ..., -5.4725e-01,\n",
      "           -4.7971e-02,  1.3012e+00],\n",
      "          [ 3.4463e-01,  2.5485e-01, -7.2371e-01,  ..., -5.2433e-01,\n",
      "           -1.4598e-01,  1.4264e+00],\n",
      "          [ 4.4851e-01,  1.5697e-01, -6.0479e-01,  ..., -6.0917e-01,\n",
      "           -1.6129e-01,  1.4513e+00]],\n",
      "\n",
      "         [[-1.2384e-02, -1.2794e+00, -1.4764e+00,  ...,  1.3755e+00,\n",
      "           -2.9110e+00,  1.4848e+00],\n",
      "          [ 1.1558e+00, -1.5053e+00,  4.1685e-01,  ..., -8.1728e-01,\n",
      "            9.0595e-02,  7.8617e-02],\n",
      "          [ 2.9681e-01, -4.3468e-01,  3.7236e-01,  ..., -3.5975e-01,\n",
      "           -8.0765e-02, -1.5453e-01],\n",
      "          ...,\n",
      "          [ 1.9258e-01,  2.1123e-01,  1.4168e+00,  ...,  8.1058e-02,\n",
      "           -1.6577e-02, -2.5781e-01],\n",
      "          [ 1.3392e-01,  4.5197e-01,  1.3631e+00,  ...,  1.7215e-01,\n",
      "           -1.0581e-02, -2.6988e-01],\n",
      "          [ 1.1817e-01,  5.3822e-01,  1.3581e+00,  ...,  3.4102e-01,\n",
      "            6.3172e-02, -1.7731e-01]],\n",
      "\n",
      "         [[-3.3020e-01, -1.1338e-01,  6.4910e-01,  ..., -1.7404e+00,\n",
      "           -1.1863e+00, -6.5465e-02],\n",
      "          [ 8.4757e-01,  1.9758e+00,  4.0913e-01,  ...,  2.8774e-01,\n",
      "            4.9146e-03,  1.0113e+00],\n",
      "          [-3.9768e-01,  4.1316e-01, -6.0016e-01,  ...,  5.1325e-01,\n",
      "            3.1264e-02,  1.4021e-01],\n",
      "          ...,\n",
      "          [ 1.2012e+00, -2.3078e-01, -3.3807e-01,  ..., -4.0486e-01,\n",
      "           -1.1337e+00, -7.1086e-02],\n",
      "          [ 1.0326e+00, -7.9828e-02, -4.4037e-01,  ..., -3.8723e-01,\n",
      "           -1.1196e+00, -5.3212e-02],\n",
      "          [ 9.3180e-01, -2.4478e-02, -4.4499e-01,  ..., -4.0016e-01,\n",
      "           -1.0494e+00, -6.3922e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.9355e-01, -1.1249e+00,  9.0274e-02,  ...,  9.4232e-01,\n",
      "           -1.0720e+00,  1.0534e-01],\n",
      "          [ 2.3859e-01, -8.9198e-01,  5.0124e-01,  ...,  5.6920e-01,\n",
      "           -8.2358e-01, -3.9844e-01],\n",
      "          [ 3.6576e-01, -4.7835e-01, -1.9918e-01,  ..., -3.2820e-02,\n",
      "            3.7988e-01, -5.1827e-01],\n",
      "          ...,\n",
      "          [-1.7813e-01,  5.5072e-02, -4.4998e-01,  ..., -2.8930e-01,\n",
      "            6.7791e-02, -3.6779e-01],\n",
      "          [-2.8118e-01, -1.4938e-03, -6.0809e-01,  ..., -2.3967e-01,\n",
      "           -4.1146e-02, -4.2282e-01],\n",
      "          [-3.2816e-01,  6.8366e-02, -7.3593e-01,  ..., -2.1380e-01,\n",
      "           -6.2344e-02, -4.0178e-01]],\n",
      "\n",
      "         [[ 6.8520e-01, -7.4630e-01,  3.7052e-02,  ..., -7.4514e-01,\n",
      "            2.5517e-01,  8.2177e-01],\n",
      "          [ 7.5996e-01,  6.6844e-01,  6.7624e-01,  ...,  5.7048e-01,\n",
      "            5.7279e-01,  1.6681e-01],\n",
      "          [ 2.3873e-01,  1.5954e-01, -6.4840e-01,  ...,  1.0519e+00,\n",
      "            9.9368e-01,  6.5105e-02],\n",
      "          ...,\n",
      "          [-1.2793e-01, -1.3432e+00, -1.0921e+00,  ..., -1.3282e+00,\n",
      "            1.4997e-01,  3.3707e-01],\n",
      "          [-1.8137e-01, -1.4571e+00, -1.0513e+00,  ..., -1.1793e+00,\n",
      "            2.6298e-01, -1.0093e-02],\n",
      "          [-1.3861e-01, -1.5944e+00, -1.0337e+00,  ..., -1.0095e+00,\n",
      "            3.5723e-01, -3.0268e-01]],\n",
      "\n",
      "         [[-1.2848e+00, -2.3592e-01,  5.7130e-01,  ..., -1.2200e-01,\n",
      "            8.5478e-01,  5.8520e-02],\n",
      "          [ 1.4356e+00,  3.7221e-01,  1.7479e+00,  ...,  1.0024e+00,\n",
      "            5.2060e-01, -1.1307e+00],\n",
      "          [-1.2118e-01,  4.2468e-02,  8.7193e-01,  ..., -2.7745e-01,\n",
      "           -2.6440e-01, -2.3019e-01],\n",
      "          ...,\n",
      "          [-1.6754e+00, -5.3424e-01,  1.4576e-01,  ..., -1.2674e-01,\n",
      "           -8.1713e-01, -5.3382e-01],\n",
      "          [-1.4641e+00, -5.4443e-01,  1.9467e-01,  ..., -2.1300e-01,\n",
      "           -7.0186e-01, -6.2108e-01],\n",
      "          [-1.2678e+00, -4.8617e-01,  1.6939e-01,  ..., -3.0305e-01,\n",
      "           -5.6858e-01, -6.6349e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-0.6032, -0.4125, -0.0572,  ...,  0.4851,  0.9194,  0.3661],\n",
      "          [-0.8648, -0.4189, -0.2425,  ...,  0.3460,  1.0132,  0.5011],\n",
      "          [ 0.3703,  1.1617, -0.0611,  ..., -0.9465,  0.3933,  0.1223],\n",
      "          ...,\n",
      "          [ 0.0796, -0.4803,  0.1238,  ..., -0.7378,  0.9188, -0.1982],\n",
      "          [-0.0749, -0.1305,  0.2957,  ..., -0.4513,  0.6666, -0.0048],\n",
      "          [-0.2663,  0.2584,  0.2353,  ..., -0.3141,  0.6292, -0.0696]],\n",
      "\n",
      "         [[ 0.5692,  0.4977,  1.2640,  ...,  1.1279,  0.8367,  0.4994],\n",
      "          [ 0.8302,  0.0161,  1.4697,  ...,  1.4821,  0.9762,  1.2508],\n",
      "          [-0.2119,  0.2483,  0.1158,  ...,  0.4018,  0.6280,  1.4218],\n",
      "          ...,\n",
      "          [ 0.7305, -0.5349,  0.4269,  ..., -0.9784,  1.1204, -0.4258],\n",
      "          [ 0.8134, -0.7241,  0.4710,  ..., -0.7679,  0.8953, -0.3653],\n",
      "          [ 0.8272, -0.7974,  0.3518,  ..., -0.4767,  0.6618, -0.4442]],\n",
      "\n",
      "         [[ 0.3344, -0.6222, -0.1769,  ..., -0.2880,  0.4930,  0.5077],\n",
      "          [ 0.7750, -0.2903,  0.2990,  ...,  0.0260,  0.1756,  0.2818],\n",
      "          [ 0.3272,  0.3455,  1.3001,  ...,  0.2776,  0.5037,  0.0168],\n",
      "          ...,\n",
      "          [-0.2741, -0.3238,  0.4346,  ..., -0.0643,  0.1422,  0.8947],\n",
      "          [-0.2964, -0.0471,  0.8205,  ...,  0.0041,  0.2121,  0.9227],\n",
      "          [-0.3608,  0.0967,  1.0166,  ..., -0.0597,  0.2508,  0.8997]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9974,  0.3361,  0.8055,  ...,  1.1385, -0.2237, -0.2720],\n",
      "          [ 0.8139, -0.1295, -0.1303,  ...,  0.3547, -1.3506, -1.5959],\n",
      "          [ 0.4914, -0.1492,  0.0307,  ...,  0.9203,  0.1338, -1.0937],\n",
      "          ...,\n",
      "          [ 0.1897,  0.7404,  0.3434,  ...,  0.2703, -0.0330,  0.1289],\n",
      "          [ 0.5360,  0.5162,  0.2250,  ...,  0.3264,  0.0055,  0.0227],\n",
      "          [ 0.5020,  0.5580,  0.0731,  ...,  0.4759,  0.2539, -0.0225]],\n",
      "\n",
      "         [[-0.5977, -0.6514,  0.1119,  ...,  0.0694, -0.0735,  0.3119],\n",
      "          [ 0.1043, -0.5585, -0.5206,  ...,  1.1203,  0.1828,  0.0730],\n",
      "          [-0.6898, -0.5197,  0.2541,  ..., -0.7751, -0.3897,  1.0184],\n",
      "          ...,\n",
      "          [ 0.8285, -0.7914, -0.2877,  ...,  0.0128, -0.1612,  0.3662],\n",
      "          [ 0.8438, -0.8444, -0.3740,  ..., -0.2493, -0.3322,  0.7415],\n",
      "          [ 0.8827, -0.7823, -0.3341,  ..., -0.3971, -0.4634,  0.8198]],\n",
      "\n",
      "         [[-1.0982, -1.5781, -0.2865,  ...,  0.1722, -0.7890,  0.3920],\n",
      "          [-0.7233, -0.3057, -0.8618,  ...,  1.2168, -0.5976,  0.0581],\n",
      "          [-0.2747, -0.1529, -0.8877,  ..., -0.0956, -0.9084,  0.8434],\n",
      "          ...,\n",
      "          [ 0.4497, -1.2163,  0.4852,  ...,  0.1475, -1.3089,  0.2325],\n",
      "          [ 0.6077, -0.8716,  0.2181,  ...,  0.2972, -0.9252,  0.2683],\n",
      "          [ 0.5922, -0.6500,  0.0453,  ...,  0.2887, -0.7054,  0.3491]]]],\n",
      "       grad_fn=<CloneBackward0>), tensor([[[[ 0.6970, -0.2352,  0.6032,  ...,  0.8621,  0.3969,  0.8489],\n",
      "          [ 0.3044, -0.4286,  0.8709,  ...,  0.2015,  0.2839,  0.7747],\n",
      "          [ 0.0746, -0.4962, -0.1640,  ..., -0.1870, -0.5511,  0.4436],\n",
      "          ...,\n",
      "          [ 0.1095, -0.7524,  0.4937,  ..., -0.1850, -0.6304,  0.2137],\n",
      "          [ 0.0548, -0.7445,  0.2323,  ..., -0.2461, -0.5615,  0.1695],\n",
      "          [ 0.1420, -0.6666,  0.2054,  ..., -0.2970, -0.5410,  0.1422]],\n",
      "\n",
      "         [[ 0.2203, -0.3110, -0.6923,  ..., -0.5666, -0.4737,  0.0026],\n",
      "          [-0.3257, -0.0441, -1.0244,  ..., -0.3538, -0.5320, -0.2402],\n",
      "          [-0.3284,  0.1417,  0.4439,  ...,  0.3053, -0.0805,  0.6744],\n",
      "          ...,\n",
      "          [ 0.2068, -0.1660,  0.6139,  ..., -0.0753, -0.6076,  0.0292],\n",
      "          [ 0.1530, -0.3561,  0.7481,  ...,  0.0764, -0.4655,  0.0158],\n",
      "          [ 0.3048, -0.2637,  0.6181,  ...,  0.1262, -0.3357,  0.1213]],\n",
      "\n",
      "         [[-0.6283, -0.2242,  0.5013,  ...,  0.0513,  0.0940,  0.0058],\n",
      "          [-0.6023,  0.5115,  0.0712,  ...,  0.0388,  0.1990, -0.3339],\n",
      "          [-0.9815,  0.2360,  0.2187,  ...,  0.4967, -0.2608,  0.1472],\n",
      "          ...,\n",
      "          [ 0.6519,  0.1717, -0.6281,  ..., -0.3854,  0.3646, -0.3432],\n",
      "          [ 0.6175,  0.0487, -0.7449,  ..., -0.2263,  0.2037, -0.3788],\n",
      "          [ 0.5414,  0.0900, -0.8240,  ..., -0.1552,  0.0567, -0.3419]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4076, -0.1660,  0.8318,  ..., -0.7644,  0.1835,  0.1886],\n",
      "          [ 0.2732,  0.3526,  0.6837,  ..., -0.3997,  0.5574,  0.1928],\n",
      "          [-0.6061,  0.1277,  0.0542,  ..., -0.0955,  0.7104,  0.6072],\n",
      "          ...,\n",
      "          [-0.3219,  0.0378,  0.2071,  ..., -0.6108,  0.4524, -0.1151],\n",
      "          [-0.6837, -0.0835,  0.0559,  ..., -0.7749,  0.6080, -0.1182],\n",
      "          [-0.9399, -0.2166, -0.1709,  ..., -0.8336,  0.6766,  0.0249]],\n",
      "\n",
      "         [[ 0.2704,  0.8437, -0.4443,  ...,  0.5572, -0.1797, -1.0721],\n",
      "          [-0.4290,  1.0665, -0.2383,  ...,  0.2751,  0.2891, -1.2290],\n",
      "          [-0.1748,  0.0606, -0.8489,  ...,  0.6816,  0.0412, -0.1428],\n",
      "          ...,\n",
      "          [ 0.7381,  0.3202, -0.2142,  ..., -0.4847, -0.1578,  0.3784],\n",
      "          [ 0.8023,  0.1418, -0.4404,  ..., -0.5576, -0.1456,  0.2975],\n",
      "          [ 0.7814,  0.2140, -0.5305,  ..., -0.5638, -0.2253,  0.2574]],\n",
      "\n",
      "         [[ 0.0173,  0.2934, -0.4182,  ..., -0.3983, -0.1136, -0.7823],\n",
      "          [ 0.7476, -0.0070, -0.3610,  ..., -0.4508, -0.5426, -0.4661],\n",
      "          [ 0.0186,  0.2944,  0.0226,  ..., -1.2034,  0.0841, -0.4303],\n",
      "          ...,\n",
      "          [-0.4024,  0.0456, -0.2803,  ...,  0.4215, -0.2250, -0.0689],\n",
      "          [-0.2942, -0.0108, -0.0836,  ...,  0.4112, -0.1467, -0.0027],\n",
      "          [-0.3402, -0.0460, -0.0636,  ...,  0.3199, -0.0725,  0.1971]]]],\n",
      "       grad_fn=<CloneBackward0>)), (tensor([[[[-1.7906e+00,  2.2445e+00,  1.1413e+00,  ...,  2.4507e+00,\n",
      "           -3.2229e-01, -6.5086e-01],\n",
      "          [-5.8017e-01,  1.2498e+00, -4.5185e-01,  ...,  2.3222e-01,\n",
      "           -2.5374e-01, -8.7072e-01],\n",
      "          [-1.1654e-02,  1.4902e+00,  2.3627e+00,  ..., -3.1606e-01,\n",
      "           -1.0357e+00, -1.1006e+00],\n",
      "          ...,\n",
      "          [-2.1657e-01,  2.0833e+00, -8.0486e-01,  ...,  2.3305e+00,\n",
      "           -3.5091e-01, -3.2793e-01],\n",
      "          [-2.7017e-01,  1.9786e+00, -7.6476e-01,  ...,  2.2286e+00,\n",
      "           -4.1177e-01, -5.7969e-01],\n",
      "          [-3.0672e-01,  1.8002e+00, -6.8891e-01,  ...,  2.1401e+00,\n",
      "           -4.9890e-01, -7.9956e-01]],\n",
      "\n",
      "         [[-8.5005e-01,  2.1697e-02,  1.9565e+00,  ..., -1.4218e+00,\n",
      "            4.0860e-01,  4.3533e-02],\n",
      "          [-1.3870e+00,  1.1424e+00,  7.5397e-01,  ..., -6.3994e-01,\n",
      "            9.4070e-01,  3.1891e-01],\n",
      "          [-4.0657e-01, -6.2131e-01,  7.5696e-01,  ..., -8.7992e-01,\n",
      "           -1.0332e-02,  1.4022e-01],\n",
      "          ...,\n",
      "          [-5.3849e-01,  4.7757e-01,  1.3481e+00,  ..., -5.4143e-01,\n",
      "            1.0951e+00, -6.4920e-02],\n",
      "          [-5.3024e-01,  4.9295e-01,  1.2262e+00,  ..., -4.8394e-01,\n",
      "            1.0113e+00, -1.2268e-01],\n",
      "          [-5.4377e-01,  4.6549e-01,  1.0170e+00,  ..., -4.1675e-01,\n",
      "            9.1254e-01, -1.7974e-01]],\n",
      "\n",
      "         [[-5.4945e-01, -2.1801e-01, -7.8821e-01,  ..., -6.2327e-01,\n",
      "            1.7773e-01, -1.0890e-01],\n",
      "          [ 1.7830e-01, -1.8729e-01, -7.1021e-01,  ..., -2.9036e-01,\n",
      "            1.5852e-01,  3.6955e-05],\n",
      "          [-8.2841e-01, -7.7732e-02, -6.0589e-02,  ..., -1.8767e-01,\n",
      "            1.9124e-01, -4.0068e-01],\n",
      "          ...,\n",
      "          [-5.6045e-02,  2.0627e-01,  5.2881e-01,  ..., -2.6632e-01,\n",
      "            4.5699e-01, -1.5966e-01],\n",
      "          [ 4.2296e-02,  2.1736e-01,  4.7563e-01,  ..., -2.0179e-01,\n",
      "            4.8190e-01, -1.0340e-01],\n",
      "          [ 1.7486e-01,  2.0660e-01,  4.3735e-01,  ..., -1.0645e-01,\n",
      "            4.8984e-01, -5.0278e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.0068e-02, -3.9874e-01,  6.7430e-01,  ..., -5.6320e-02,\n",
      "            8.0668e-01, -7.2364e-01],\n",
      "          [ 5.3157e-01, -4.3491e-01, -3.0262e-01,  ...,  1.6384e+00,\n",
      "           -1.8798e-01, -2.0357e-01],\n",
      "          [ 4.2304e-01, -2.2636e-01, -6.1129e-02,  ...,  6.8514e-02,\n",
      "            1.3932e-01, -1.2346e+00],\n",
      "          ...,\n",
      "          [-7.8820e-02, -2.9397e-01,  4.8629e-01,  ..., -3.0134e-01,\n",
      "           -9.8136e-01,  1.3949e-01],\n",
      "          [-1.7663e-03, -1.8509e-01,  3.9783e-01,  ..., -3.3976e-01,\n",
      "           -9.4681e-01,  1.8999e-01],\n",
      "          [ 7.9078e-02, -9.9815e-02,  3.2784e-01,  ..., -3.6875e-01,\n",
      "           -8.5811e-01,  1.8627e-01]],\n",
      "\n",
      "         [[-6.6827e-01, -6.6292e-01, -1.0153e+00,  ..., -7.2893e-01,\n",
      "           -1.4856e-01,  7.5864e-01],\n",
      "          [-2.1193e+00,  3.0361e-01,  4.9582e-01,  ...,  3.1857e-01,\n",
      "            1.3577e+00, -6.4639e-01],\n",
      "          [-2.1999e+00,  3.1302e-01,  5.1797e-01,  ...,  1.4487e+00,\n",
      "           -1.1942e+00,  6.4476e-01],\n",
      "          ...,\n",
      "          [ 5.7409e-01, -2.0205e-01,  1.8721e-01,  ...,  1.9067e-01,\n",
      "           -8.2676e-02, -1.6830e+00],\n",
      "          [ 4.7873e-01, -2.6577e-01,  1.7240e-01,  ...,  2.0831e-01,\n",
      "           -1.3591e-01, -1.4075e+00],\n",
      "          [ 3.1965e-01, -2.7895e-01,  1.8630e-01,  ...,  1.9776e-01,\n",
      "           -1.8052e-01, -1.1733e+00]],\n",
      "\n",
      "         [[-8.9746e-01,  1.8930e-01,  2.5163e+00,  ...,  2.2509e+00,\n",
      "            1.2437e+00, -1.2793e+00],\n",
      "          [ 2.0951e+00,  4.9933e-03,  1.5268e+00,  ...,  1.5446e+00,\n",
      "            1.9545e+00, -4.5438e-01],\n",
      "          [ 5.1098e-01, -8.5946e-01, -5.4542e-02,  ...,  1.4297e-01,\n",
      "            2.6684e+00, -3.2215e-01],\n",
      "          ...,\n",
      "          [ 1.5507e-01,  1.2909e+00, -1.1128e-01,  ..., -8.9392e-01,\n",
      "           -1.2765e+00, -7.6043e-02],\n",
      "          [ 1.8452e-01,  1.1551e+00, -1.1580e-01,  ..., -9.0511e-01,\n",
      "           -1.3129e+00, -1.9960e-01],\n",
      "          [ 2.0780e-01,  1.0101e+00, -1.5200e-01,  ..., -8.9188e-01,\n",
      "           -1.3518e+00, -2.8983e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-0.4461,  1.6165, -0.5442,  ..., -0.0440, -0.6670,  0.2876],\n",
      "          [ 0.1301,  1.3993, -0.6428,  ..., -0.0858, -0.1109,  0.3197],\n",
      "          [ 0.0634, -0.1232, -0.3780,  ..., -0.4310,  0.3303,  0.1871],\n",
      "          ...,\n",
      "          [ 0.4436,  0.4972,  0.3166,  ...,  0.2810,  0.1137, -0.1803],\n",
      "          [ 0.3814,  0.3661,  0.3325,  ...,  0.2750,  0.0952, -0.1580],\n",
      "          [ 0.3133,  0.1977,  0.3349,  ...,  0.2795,  0.0988, -0.1306]],\n",
      "\n",
      "         [[-1.3902,  0.1855, -1.3091,  ...,  1.1556, -0.5015,  0.9530],\n",
      "          [-1.0659, -0.9894,  1.0203,  ...,  0.7754, -0.9575,  1.2162],\n",
      "          [-1.1207,  1.1810,  0.6646,  ...,  1.2342, -0.7039,  0.0657],\n",
      "          ...,\n",
      "          [-0.3994, -0.3831, -0.0491,  ...,  0.0174,  0.0325,  0.8301],\n",
      "          [-0.3427, -0.4457, -0.0576,  ...,  0.0081,  0.1122,  0.7750],\n",
      "          [-0.2797, -0.5006, -0.0732,  ..., -0.0155,  0.1537,  0.6881]],\n",
      "\n",
      "         [[ 0.3177, -0.1946,  0.0449,  ..., -2.2869, -0.3789,  0.1211],\n",
      "          [ 0.9845,  0.6596,  0.1391,  ..., -1.3314,  0.0586, -2.8177],\n",
      "          [-0.8500, -0.2597,  0.0066,  ..., -2.0825,  0.7280, -0.1797],\n",
      "          ...,\n",
      "          [-1.0419, -0.8024, -0.7583,  ...,  0.1965, -0.8219,  0.5607],\n",
      "          [-0.8471, -0.6523, -0.5850,  ...,  0.2156, -0.8356,  0.5864],\n",
      "          [-0.6440, -0.5390, -0.4521,  ...,  0.2214, -0.7936,  0.5725]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2335,  0.2224,  0.2726,  ..., -1.6653,  0.2321,  1.7124],\n",
      "          [-1.5938,  0.2641,  1.5122,  ..., -0.8225, -0.1960,  0.4328],\n",
      "          [-0.3784, -0.7022, -1.4493,  ...,  0.1688,  0.4171,  0.6416],\n",
      "          ...,\n",
      "          [-0.1561, -0.6884,  0.8191,  ..., -0.4921, -0.0763,  0.1706],\n",
      "          [-0.1724, -0.6268,  0.7073,  ..., -0.3975, -0.0642,  0.1286],\n",
      "          [-0.1457, -0.5024,  0.5744,  ..., -0.2852, -0.0987,  0.1390]],\n",
      "\n",
      "         [[ 0.2059,  0.7433,  0.5297,  ..., -0.8744,  0.2348, -0.2326],\n",
      "          [-1.4021, -0.3786, -1.1109,  ..., -1.1758,  0.2421, -1.8520],\n",
      "          [ 0.2108, -0.5196, -0.8761,  ..., -0.8735,  0.8651, -0.1790],\n",
      "          ...,\n",
      "          [-0.3204,  0.4655, -0.4532,  ...,  0.2550,  0.3128, -0.1180],\n",
      "          [-0.3836,  0.3872, -0.4848,  ...,  0.2781,  0.3675, -0.0558],\n",
      "          [-0.3871,  0.2951, -0.5297,  ...,  0.2822,  0.3843, -0.0470]],\n",
      "\n",
      "         [[-0.7726,  1.0073,  0.0576,  ..., -0.0757,  0.1624, -0.5878],\n",
      "          [-1.2633,  0.3357,  0.8099,  ..., -0.7366, -0.1232, -0.3805],\n",
      "          [-0.5178,  0.5016,  0.1235,  ..., -0.3739,  0.6050, -0.9333],\n",
      "          ...,\n",
      "          [-0.1851,  0.4948, -0.4224,  ...,  0.0587, -0.4630, -0.7300],\n",
      "          [-0.0371,  0.3406, -0.4720,  ...,  0.1644, -0.4708, -0.6609],\n",
      "          [ 0.0762,  0.2399, -0.5193,  ...,  0.2733, -0.4706, -0.5693]]]],\n",
      "       grad_fn=<CloneBackward0>), tensor([[[[-0.3752, -0.1988,  0.2794,  ...,  0.1016, -0.1931, -0.6344],\n",
      "          [-0.3673, -0.5946, -0.5067,  ...,  0.4811, -0.3804, -0.4613],\n",
      "          [ 0.4302, -0.3845, -0.3800,  ...,  0.0571, -0.4923, -0.6450],\n",
      "          ...,\n",
      "          [ 0.7861, -0.1982,  0.5002,  ..., -0.0948,  0.6418,  0.3632],\n",
      "          [ 0.7428, -0.0809,  0.5848,  ..., -0.0394,  0.5167,  0.1385],\n",
      "          [ 0.6345, -0.0475,  0.6279,  ...,  0.1031,  0.2871, -0.0700]],\n",
      "\n",
      "         [[ 0.1397, -0.5772,  0.3308,  ...,  0.1393, -0.4190, -1.0554],\n",
      "          [ 0.2812, -0.1460,  1.2666,  ..., -0.2873, -0.3935, -1.3676],\n",
      "          [-0.5072,  0.4732,  0.1760,  ..., -0.1700, -0.3945, -0.1065],\n",
      "          ...,\n",
      "          [ 0.1653, -0.3409,  0.6179,  ..., -0.0200,  0.7373, -0.4584],\n",
      "          [ 0.0110, -0.0255,  0.6823,  ..., -0.0470,  0.5132, -0.2897],\n",
      "          [-0.1169,  0.0414,  0.6116,  ..., -0.1285,  0.3758, -0.2981]],\n",
      "\n",
      "         [[-0.2219,  0.2008, -0.1475,  ..., -0.5906, -0.8747,  0.4340],\n",
      "          [ 0.0920,  0.9531,  0.4899,  ..., -1.1112, -0.5295,  0.5786],\n",
      "          [-0.4580, -0.3648, -0.1874,  ...,  0.4294, -0.1433, -0.1805],\n",
      "          ...,\n",
      "          [-0.7465,  0.4962,  0.1000,  ..., -0.3805,  0.2201,  0.3406],\n",
      "          [-0.8910,  0.3635, -0.1360,  ..., -0.6045,  0.2427,  0.4285],\n",
      "          [-1.0387,  0.4323, -0.2709,  ..., -0.4696,  0.2454,  0.3614]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6784, -0.2721,  0.5347,  ...,  0.2399, -0.8608, -0.1080],\n",
      "          [ 0.3309,  0.7503,  0.2715,  ...,  0.2283, -1.1942,  0.7392],\n",
      "          [ 0.7802,  0.3404,  0.5255,  ..., -0.2907, -0.3105, -0.4097],\n",
      "          ...,\n",
      "          [ 1.0898, -0.0717,  0.5425,  ..., -0.9871, -0.0553, -0.0099],\n",
      "          [ 1.1362, -0.0764,  0.3061,  ..., -0.5788, -0.0464, -0.2186],\n",
      "          [ 1.0431,  0.0146,  0.2748,  ..., -0.5286, -0.0653, -0.2565]],\n",
      "\n",
      "         [[-1.1590,  0.2353, -0.5230,  ...,  0.2711,  0.2272,  0.0577],\n",
      "          [-1.7667,  0.4616,  0.0500,  ...,  0.1990, -0.2934, -0.1777],\n",
      "          [-0.6694,  0.6807, -0.4470,  ..., -0.8003, -0.3076,  0.4225],\n",
      "          ...,\n",
      "          [-0.2874,  0.2265,  0.1890,  ...,  0.9111,  0.2108, -0.3712],\n",
      "          [-0.2201,  0.0339,  0.5398,  ...,  0.6187, -0.0639,  0.0766],\n",
      "          [-0.1833, -0.2416,  0.7361,  ...,  0.7258,  0.2530,  0.2173]],\n",
      "\n",
      "         [[-0.6933,  0.2750, -0.7915,  ..., -0.1279,  0.1537, -0.1187],\n",
      "          [-0.5757,  1.1363, -0.3052,  ..., -0.4291,  0.0044, -0.1525],\n",
      "          [ 0.4671,  0.7019,  0.2556,  ...,  0.1218, -0.3874,  0.2179],\n",
      "          ...,\n",
      "          [-0.2792, -0.6374,  0.1961,  ..., -0.2001,  0.3008,  1.0648],\n",
      "          [-0.0416, -0.5690,  0.3245,  ...,  0.0331,  0.2944,  0.9299],\n",
      "          [-0.1014, -0.5221,  0.2261,  ...,  0.0114,  0.2561,  0.6813]]]],\n",
      "       grad_fn=<CloneBackward0>), tensor([[[[ 0.9969, -0.6991,  0.4681,  ..., -0.2523, -0.5466,  0.1822],\n",
      "          [ 1.2860, -0.8233,  1.1001,  ...,  0.4517,  0.1417,  0.7144],\n",
      "          [-0.5941, -0.5200, -0.0771,  ..., -0.6341, -0.3476,  0.8733],\n",
      "          ...,\n",
      "          [-0.5015, -0.3788,  0.0783,  ..., -0.3412,  0.4038,  0.0911],\n",
      "          [-0.6896, -0.3517,  0.0941,  ..., -0.3896,  0.4499,  0.1673],\n",
      "          [-0.7967, -0.2978,  0.0819,  ..., -0.2637,  0.4729,  0.2817]],\n",
      "\n",
      "         [[-0.2766,  0.3820,  0.6688,  ...,  0.8283, -0.3018, -0.8413],\n",
      "          [-0.1339, -0.2457,  1.4422,  ...,  0.4164, -0.2532, -0.0081],\n",
      "          [-0.0302, -0.0459,  0.2927,  ...,  0.1467, -0.3062,  0.2443],\n",
      "          ...,\n",
      "          [-0.1808, -0.3477, -0.5624,  ..., -0.4678, -0.3536, -0.7608],\n",
      "          [-0.0973, -0.5032, -0.4777,  ..., -0.3174, -0.3388, -0.5993],\n",
      "          [-0.0404, -0.5488, -0.5021,  ..., -0.0962, -0.3047, -0.5549]],\n",
      "\n",
      "         [[ 0.0708, -0.0236, -0.7314,  ..., -0.1241,  0.2320,  0.3910],\n",
      "          [ 0.5263,  0.0696, -1.4117,  ...,  0.4022,  0.4056,  0.4342],\n",
      "          [ 0.3530, -0.2916, -0.2185,  ...,  0.9988, -0.4176,  0.0386],\n",
      "          ...,\n",
      "          [ 0.1476, -0.1952,  0.1954,  ...,  0.1426,  0.3216, -0.1537],\n",
      "          [ 0.0262, -0.0363, -0.0146,  ...,  0.3603,  0.4838, -0.1198],\n",
      "          [ 0.0676, -0.0330, -0.0975,  ...,  0.3922,  0.5467, -0.0856]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3862, -0.0771,  0.5960,  ...,  1.3277,  0.3911, -0.4258],\n",
      "          [ 0.3661,  0.3567,  0.0911,  ...,  1.5156,  0.4711, -0.0471],\n",
      "          [ 0.3119,  0.0456, -0.0561,  ..., -0.2098, -0.1050,  0.1516],\n",
      "          ...,\n",
      "          [-0.0737,  0.2775, -0.7643,  ..., -0.2754,  0.0346,  0.0427],\n",
      "          [-0.2650,  0.3220, -0.7424,  ..., -0.2411, -0.0023,  0.2767],\n",
      "          [-0.1444,  0.2125, -0.7325,  ..., -0.1715,  0.0875,  0.3687]],\n",
      "\n",
      "         [[ 0.3118,  0.4947, -0.1878,  ..., -0.6322, -0.3886, -0.0461],\n",
      "          [ 0.4348, -0.7303, -0.4166,  ..., -0.5033, -0.7489, -0.7382],\n",
      "          [-0.4798, -0.5756,  0.8813,  ..., -0.3671, -0.0471,  0.3453],\n",
      "          ...,\n",
      "          [-0.8866, -0.3063,  0.1693,  ..., -0.4892,  0.3736,  0.0301],\n",
      "          [-0.5853, -0.7068,  0.2398,  ..., -0.7254,  0.2451,  0.2466],\n",
      "          [-0.4554, -0.7745,  0.4689,  ..., -0.7104,  0.1830,  0.3683]],\n",
      "\n",
      "         [[ 0.1801, -0.1028,  0.3871,  ...,  0.1370,  0.2799, -0.0431],\n",
      "          [ 0.2543, -0.0980, -0.0100,  ..., -0.0881,  0.0587, -0.7116],\n",
      "          [ 0.0472, -0.1007, -0.7386,  ...,  0.2990, -0.0694,  0.3933],\n",
      "          ...,\n",
      "          [-0.2062,  0.4623, -0.6689,  ..., -0.7133,  0.5124,  0.5323],\n",
      "          [-0.4330,  0.2341, -0.6968,  ..., -0.5279,  0.5258,  0.7527],\n",
      "          [-0.6069,  0.1246, -0.6310,  ..., -0.5898,  0.6606,  0.8557]]]],\n",
      "       grad_fn=<CloneBackward0>)), (tensor([[[[-1.0444, -0.3939, -1.0176,  ..., -0.3598,  1.5252,  0.1436],\n",
      "          [-1.3957, -0.4846, -1.6136,  ...,  0.0165,  0.5554, -0.7341],\n",
      "          [-1.4684, -0.7847, -0.4468,  ..., -0.0570, -0.2890, -0.7855],\n",
      "          ...,\n",
      "          [-0.7787, -1.5903, -0.3093,  ..., -0.8082, -0.4321, -0.2384],\n",
      "          [-0.6943, -1.5186, -0.3017,  ..., -0.7531, -0.3487, -0.2054],\n",
      "          [-0.5577, -1.4642, -0.2995,  ..., -0.6824, -0.2603, -0.1677]],\n",
      "\n",
      "         [[ 1.0346, -0.7770,  1.0702,  ...,  1.0447,  0.8584,  1.3921],\n",
      "          [ 0.1273, -2.6620,  1.4934,  ...,  1.2481,  1.2553,  0.1267],\n",
      "          [ 0.8552, -0.5792,  0.4395,  ...,  2.0756, -0.0963, -0.0309],\n",
      "          ...,\n",
      "          [ 0.1503, -3.8794, -0.6793,  ..., -0.1797,  0.1724,  1.6553],\n",
      "          [ 0.2638, -3.6732, -0.7201,  ..., -0.2008,  0.1971,  1.6078],\n",
      "          [ 0.3829, -3.4167, -0.8049,  ..., -0.1988,  0.2508,  1.5509]],\n",
      "\n",
      "         [[ 0.7074,  0.5903,  1.4143,  ...,  1.4592,  1.5767,  1.4669],\n",
      "          [ 1.6036, -0.1687,  2.7651,  ...,  0.9871, -0.4933,  0.9786],\n",
      "          [ 0.8591, -1.4079,  2.0517,  ..., -0.0251, -1.3890,  0.7432],\n",
      "          ...,\n",
      "          [ 0.3316,  1.3136, -0.0713,  ..., -0.7114,  1.6920,  0.3730],\n",
      "          [ 0.3249,  1.3115, -0.1313,  ..., -0.6583,  1.5542,  0.3320],\n",
      "          [ 0.3520,  1.2854, -0.2123,  ..., -0.6034,  1.3860,  0.2841]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1669,  0.0718,  0.4452,  ..., -0.7430,  1.2418, -0.4018],\n",
      "          [ 1.1755,  0.7219,  0.6468,  ..., -1.9500,  1.3624, -0.8987],\n",
      "          [-0.0778,  1.2115, -0.3846,  ..., -0.9657, -0.8744, -0.1865],\n",
      "          ...,\n",
      "          [ 0.2837,  1.1656, -1.4872,  ...,  0.1729,  0.1119,  0.5525],\n",
      "          [ 0.2043,  1.0800, -1.5015,  ...,  0.1870,  0.1217,  0.5004],\n",
      "          [ 0.1326,  0.9997, -1.4813,  ...,  0.1927,  0.1124,  0.4121]],\n",
      "\n",
      "         [[-2.3707,  0.2009, -1.2207,  ...,  0.5896, -0.4777, -0.7295],\n",
      "          [-1.4389,  2.0244, -0.8674,  ..., -0.1679, -0.7115,  0.3143],\n",
      "          [-0.1519,  0.3896,  0.5820,  ...,  1.0270,  0.1522, -0.1652],\n",
      "          ...,\n",
      "          [ 1.6548, -0.2404,  0.6253,  ..., -0.4668, -4.0061, -0.2228],\n",
      "          [ 1.6747, -0.2172,  0.5305,  ..., -0.4458, -3.9702, -0.1930],\n",
      "          [ 1.6972, -0.2490,  0.4481,  ..., -0.4182, -3.8845, -0.1336]],\n",
      "\n",
      "         [[-0.1733,  0.4887,  0.6003,  ..., -1.3419, -1.3339,  0.8865],\n",
      "          [ 0.2484, -0.2628, -0.3263,  ...,  0.0958, -2.9337,  0.1719],\n",
      "          [ 1.0165, -0.1429,  0.0426,  ...,  1.0616, -1.0273,  0.1165],\n",
      "          ...,\n",
      "          [-0.9287,  1.9193,  3.6680,  ...,  1.0420, -0.6630, -0.6860],\n",
      "          [-1.0489,  1.8657,  3.5957,  ...,  1.0342, -0.6784, -0.6646],\n",
      "          [-1.1668,  1.8200,  3.4720,  ...,  1.0428, -0.6742, -0.6066]]]],\n",
      "       grad_fn=<CloneBackward0>), tensor([[[[-0.6930, -0.0102,  0.4216,  ..., -0.7001, -0.1178,  1.0073],\n",
      "          [-0.5928,  0.5410,  0.3116,  ..., -0.6850,  0.5043,  0.8590],\n",
      "          [ 0.2443, -0.8881,  0.3381,  ...,  0.0961, -0.0111,  0.1863],\n",
      "          ...,\n",
      "          [ 0.1685,  0.0377,  0.1064,  ..., -0.2087, -0.0386, -0.2756],\n",
      "          [ 0.1761,  0.0636,  0.1314,  ..., -0.2204, -0.0609, -0.2594],\n",
      "          [ 0.1764,  0.0946,  0.1480,  ..., -0.2065, -0.0817, -0.2338]],\n",
      "\n",
      "         [[ 1.0337,  0.1567,  0.2159,  ..., -0.3299,  0.0684, -0.0753],\n",
      "          [-0.0318,  0.3991, -0.2514,  ..., -0.4407, -0.5345,  0.1730],\n",
      "          [ 0.8431,  0.2437,  0.3801,  ..., -1.2609,  0.4162,  0.7378],\n",
      "          ...,\n",
      "          [-0.3595,  0.1303, -0.2470,  ..., -0.3096,  0.1788,  0.3601],\n",
      "          [-0.2867,  0.0616, -0.2344,  ..., -0.2745,  0.1978,  0.3130],\n",
      "          [-0.2209, -0.0182, -0.2454,  ..., -0.2662,  0.1794,  0.2565]],\n",
      "\n",
      "         [[-0.3462, -0.6593, -0.2364,  ..., -0.8181,  1.4260, -0.7723],\n",
      "          [-1.0068, -0.4973,  0.4149,  ..., -1.1797,  1.3246, -0.4912],\n",
      "          [ 0.4703, -0.4318, -0.0887,  ..., -1.0241, -0.1156,  0.2950],\n",
      "          ...,\n",
      "          [ 0.1435,  0.2474,  0.0579,  ...,  0.0234,  0.1705,  0.3542],\n",
      "          [ 0.1387,  0.2319,  0.0353,  ...,  0.0046,  0.1837,  0.2990],\n",
      "          [ 0.1066,  0.1935,  0.0188,  ..., -0.0125,  0.1798,  0.2237]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3047,  0.6432, -0.3217,  ...,  0.1817,  0.5574,  0.4355],\n",
      "          [-0.6898,  1.3400, -0.5321,  ...,  0.4161,  0.2021, -0.1124],\n",
      "          [-0.2368, -0.1199, -0.5214,  ..., -0.4396, -0.1980,  0.0676],\n",
      "          ...,\n",
      "          [ 0.2140, -0.0781, -0.0738,  ...,  0.1275, -0.0978, -0.1617],\n",
      "          [ 0.2093, -0.0651, -0.0614,  ...,  0.1512, -0.0927, -0.1348],\n",
      "          [ 0.2050, -0.0390, -0.0458,  ...,  0.1593, -0.0678, -0.0830]],\n",
      "\n",
      "         [[-0.6404,  0.0469,  0.2768,  ..., -0.7162, -0.2354,  0.0205],\n",
      "          [-0.3452,  0.0869,  0.5593,  ..., -0.5275, -0.4521,  0.5171],\n",
      "          [ 0.1060,  0.0132, -0.1137,  ..., -0.1617, -0.1507,  0.1510],\n",
      "          ...,\n",
      "          [-0.0471, -0.0437,  0.6131,  ...,  0.0551,  0.2652, -0.4873],\n",
      "          [-0.0343, -0.0036,  0.5526,  ...,  0.0300,  0.2583, -0.4178],\n",
      "          [-0.0107,  0.0408,  0.4960,  ...,  0.0203,  0.2179, -0.3454]],\n",
      "\n",
      "         [[-0.2259,  0.2139, -1.0304,  ...,  0.5254,  1.1048, -0.6426],\n",
      "          [-0.4548,  1.2023, -0.8123,  ..., -0.4187,  0.0345, -0.5413],\n",
      "          [ 0.1381, -0.0242,  0.3294,  ..., -0.2231,  0.9081,  0.1900],\n",
      "          ...,\n",
      "          [ 0.2224, -0.3482, -0.0512,  ...,  0.1474,  0.4908, -0.1973],\n",
      "          [ 0.2188, -0.3208, -0.1111,  ...,  0.0992,  0.4327, -0.1726],\n",
      "          [ 0.1955, -0.2981, -0.1352,  ...,  0.0547,  0.3553, -0.1407]]]],\n",
      "       grad_fn=<CloneBackward0>), tensor([[[[-2.2849e-01,  3.3183e-01,  9.8724e-01,  ...,  3.7437e-01,\n",
      "            2.2736e-01,  2.8337e-01],\n",
      "          [ 3.9655e-01,  1.3573e-01,  7.7301e-01,  ...,  3.7835e-01,\n",
      "            5.1188e-01,  5.0425e-01],\n",
      "          [ 1.6349e-01,  3.6001e-01,  2.5146e-01,  ..., -9.8512e-02,\n",
      "            3.6832e-01, -2.0241e-01],\n",
      "          ...,\n",
      "          [-1.9337e-01, -3.7722e-01,  2.7639e-01,  ..., -4.6022e-01,\n",
      "           -2.6817e-01,  1.9414e-01],\n",
      "          [-2.3696e-01, -1.1662e-01,  6.9786e-01,  ..., -4.9412e-01,\n",
      "           -2.1103e-01, -3.7977e-02],\n",
      "          [-2.0663e-01, -7.0070e-02,  9.9476e-01,  ..., -4.1799e-01,\n",
      "           -2.3376e-01, -1.1991e-01]],\n",
      "\n",
      "         [[ 2.2859e-01,  1.1156e+00,  3.0649e-01,  ...,  8.8447e-01,\n",
      "           -7.2221e-01,  3.9845e-01],\n",
      "          [-3.5498e-01,  6.9919e-01, -1.7137e-01,  ...,  5.8933e-01,\n",
      "           -4.4317e-01,  3.4903e-03],\n",
      "          [-6.8233e-01,  5.9536e-01, -3.0190e-01,  ...,  2.7950e-01,\n",
      "           -5.9366e-01,  2.3378e-01],\n",
      "          ...,\n",
      "          [-5.7305e-02, -2.5395e-01,  6.1028e-01,  ..., -1.2271e-01,\n",
      "            1.8526e-01,  7.2208e-01],\n",
      "          [-1.9805e-01,  2.8378e-02,  4.4755e-01,  ..., -2.2348e-01,\n",
      "            5.3549e-02,  7.4505e-01],\n",
      "          [-1.7841e-01,  1.4070e-01,  3.0598e-01,  ..., -3.3612e-01,\n",
      "           -1.2549e-02,  6.2478e-01]],\n",
      "\n",
      "         [[-5.1430e-01,  3.3748e-01, -2.7997e-01,  ...,  1.2736e-01,\n",
      "            2.0563e-01,  1.0679e+00],\n",
      "          [-5.6730e-01,  5.7432e-01, -6.2332e-02,  ..., -1.3713e-01,\n",
      "           -5.0837e-01,  2.1801e-01],\n",
      "          [-5.1449e-01, -7.6050e-02, -8.9814e-01,  ..., -9.4780e-02,\n",
      "           -1.5187e+00,  8.1602e-03],\n",
      "          ...,\n",
      "          [-2.9920e-01, -5.0776e-01,  3.8105e-01,  ...,  4.1123e-01,\n",
      "            5.4554e-03,  1.5880e-01],\n",
      "          [-4.2281e-01, -7.0205e-01,  2.6864e-01,  ...,  5.3778e-01,\n",
      "           -2.1267e-01,  2.8682e-01],\n",
      "          [-3.9665e-01, -7.6222e-01,  2.2558e-01,  ...,  4.2651e-01,\n",
      "           -3.7592e-01,  5.5418e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.9190e-01,  5.9555e-01,  1.6660e+00,  ..., -8.7760e-01,\n",
      "            7.5073e-01, -7.0855e-01],\n",
      "          [-7.6511e-01,  7.1619e-01,  5.1833e-01,  ..., -3.6741e-01,\n",
      "            2.6923e-01, -4.7246e-01],\n",
      "          [-1.0645e-01,  5.6554e-01,  6.2298e-01,  ...,  4.7555e-01,\n",
      "           -5.5156e-01, -8.8085e-01],\n",
      "          ...,\n",
      "          [ 8.3871e-02, -3.3966e-01,  1.1651e+00,  ...,  4.8167e-01,\n",
      "            1.2094e-01,  3.9781e-01],\n",
      "          [ 7.8378e-02, -5.7373e-02,  9.4062e-01,  ...,  4.8264e-01,\n",
      "           -1.8138e-01,  2.1876e-01],\n",
      "          [ 7.0790e-04, -1.8542e-01,  7.9913e-01,  ...,  4.7817e-01,\n",
      "           -3.9894e-02,  6.8764e-02]],\n",
      "\n",
      "         [[ 4.5367e-01,  1.3791e+00,  2.0396e+00,  ...,  1.2656e+00,\n",
      "            3.9055e-01,  3.9210e-01],\n",
      "          [ 3.4568e-01,  1.1892e+00,  2.9478e+00,  ...,  1.6070e+00,\n",
      "           -4.4154e-01,  1.1153e+00],\n",
      "          [-1.7169e-01, -1.0954e-01,  1.5156e-01,  ...,  1.2518e+00,\n",
      "            6.8004e-02,  1.4359e+00],\n",
      "          ...,\n",
      "          [ 5.9739e-01,  1.2398e+00,  1.9402e+00,  ...,  2.4456e+00,\n",
      "            4.0169e-02,  1.8329e+00],\n",
      "          [ 4.5050e-01,  1.0639e+00,  1.7065e+00,  ...,  2.4177e+00,\n",
      "            1.9232e-01,  1.5442e+00],\n",
      "          [ 4.8903e-01,  9.2076e-01,  1.6500e+00,  ...,  2.3084e+00,\n",
      "            2.0560e-01,  1.3010e+00]],\n",
      "\n",
      "         [[ 9.0351e-01, -1.2361e+00,  1.4552e-01,  ...,  2.4679e-02,\n",
      "           -1.2853e+00, -4.8367e-01],\n",
      "          [ 2.0067e+00, -6.3463e-01,  3.5683e-01,  ..., -8.3560e-02,\n",
      "           -7.0920e-01, -3.4120e-01],\n",
      "          [ 5.1382e-01, -1.8861e+00, -1.5364e-01,  ...,  5.3256e-01,\n",
      "           -5.6887e-01,  2.3154e-01],\n",
      "          ...,\n",
      "          [ 3.2511e-01, -1.4900e+00, -6.5792e-02,  ..., -6.9919e-01,\n",
      "           -1.5245e+00,  2.6831e-01],\n",
      "          [ 4.0273e-01, -1.4026e+00, -1.5080e-01,  ..., -4.9932e-01,\n",
      "           -1.7868e+00,  1.8905e-01],\n",
      "          [ 4.3730e-01, -1.3377e+00, -3.5600e-01,  ..., -3.7885e-01,\n",
      "           -1.8761e+00,  3.4672e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 0.5017, -0.5827, -0.6784,  ..., -0.7934,  0.0612, -0.1736],\n",
      "          [ 0.0310, -0.0178, -0.2334,  ..., -0.1624,  0.1807,  0.1210],\n",
      "          [ 1.0187, -0.2220, -0.6740,  ..., -0.6095, -0.4298, -0.1558],\n",
      "          ...,\n",
      "          [ 0.1188,  0.0143, -0.0643,  ..., -1.0325, -0.4454, -0.1955],\n",
      "          [-0.2111,  0.2498,  0.0514,  ..., -0.8736, -0.7163,  0.0394],\n",
      "          [-0.3805,  0.3513,  0.0312,  ..., -0.6536, -0.6843,  0.1818]],\n",
      "\n",
      "         [[ 0.4341,  0.0869,  0.6817,  ..., -0.4477,  0.2531,  0.6425],\n",
      "          [ 0.5391,  0.0270,  0.6345,  ..., -0.8552,  0.4790,  0.2026],\n",
      "          [ 0.4969,  0.1445,  0.7671,  ...,  0.3984,  0.1482, -0.2708],\n",
      "          ...,\n",
      "          [-0.0103,  0.2633, -0.3636,  ...,  0.8251, -0.0675,  0.0846],\n",
      "          [-0.1650,  0.2945, -0.0989,  ...,  0.6172,  0.0939,  0.0997],\n",
      "          [-0.1739,  0.2899,  0.2550,  ...,  0.4556,  0.3172,  0.1689]],\n",
      "\n",
      "         [[-0.7857,  0.7586, -0.0972,  ..., -0.4792, -0.1399, -0.4609],\n",
      "          [ 0.1198,  0.3813,  0.4375,  ...,  0.2028, -0.0687,  0.2086],\n",
      "          [ 0.3664, -0.0235,  0.3520,  ...,  0.1336, -0.1358, -0.6286],\n",
      "          ...,\n",
      "          [-0.0570,  0.1554,  0.1091,  ..., -0.0835,  0.2129,  0.3089],\n",
      "          [ 0.1722, -0.1646,  0.2691,  ...,  0.1634,  0.2526,  0.3362],\n",
      "          [ 0.2658, -0.2281,  0.2362,  ...,  0.2067,  0.3682,  0.2895]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.8336, -0.0680, -1.3254,  ...,  0.3141,  0.6003,  1.3199],\n",
      "          [ 1.5824,  0.1575, -0.8792,  ..., -0.1905,  0.1340,  1.1900],\n",
      "          [ 0.2867, -1.0944, -0.4720,  ..., -0.0118,  0.4764,  0.6398],\n",
      "          ...,\n",
      "          [ 0.4538, -0.2766, -0.1839,  ..., -0.3267, -0.8697,  0.0850],\n",
      "          [ 0.3117, -0.5190, -0.2264,  ..., -0.2894, -0.9256,  0.3216],\n",
      "          [ 0.2650, -0.4940, -0.3829,  ..., -0.1024, -0.8707,  0.3188]],\n",
      "\n",
      "         [[-0.0262,  0.3667, -0.6896,  ..., -0.3247, -0.0358, -0.9976],\n",
      "          [ 0.2923,  0.2988, -0.0990,  ...,  0.0510, -0.4007, -0.9709],\n",
      "          [-0.3654, -0.1002, -0.3666,  ...,  0.1530, -0.1790, -0.0851],\n",
      "          ...,\n",
      "          [ 0.4299, -0.5972, -0.0088,  ...,  0.3904,  0.1445,  0.3252],\n",
      "          [ 0.1864, -0.6817, -0.2209,  ...,  0.4515,  0.0233,  0.4238],\n",
      "          [-0.0511, -0.7180, -0.2731,  ...,  0.4480, -0.0557,  0.3561]],\n",
      "\n",
      "         [[-0.4905,  0.9218,  0.6212,  ...,  0.2300, -0.0361, -0.3509],\n",
      "          [ 0.0616,  0.8214,  1.1070,  ...,  0.0810, -0.1532, -0.2130],\n",
      "          [ 0.0582, -0.1104, -0.1013,  ...,  0.5470,  1.3475, -0.2437],\n",
      "          ...,\n",
      "          [-0.3024,  0.4875, -0.0149,  ...,  0.6028, -0.4401, -0.8110],\n",
      "          [ 0.1811,  0.3844, -0.1689,  ...,  0.7348, -0.1605, -0.9132],\n",
      "          [ 0.3980,  0.2802, -0.1268,  ...,  0.7811, -0.1611, -0.8919]]]],\n",
      "       grad_fn=<CloneBackward0>)), (tensor([[[[ 0.1763, -0.1379, -1.0852,  ...,  0.3899,  0.4536, -0.0398],\n",
      "          [ 0.5228,  0.6784, -1.3510,  ...,  2.8675,  0.9267,  1.5747],\n",
      "          [ 0.9670,  0.3494, -1.5489,  ...,  1.9972,  2.3041,  0.5584],\n",
      "          ...,\n",
      "          [-1.5307,  0.8424, -1.1737,  ...,  0.7359, -0.8896,  0.9850],\n",
      "          [-1.5368,  0.8479, -1.1898,  ...,  0.7215, -0.9048,  0.9645],\n",
      "          [-1.5419,  0.8521, -1.2050,  ...,  0.7097, -0.9187,  0.9430]],\n",
      "\n",
      "         [[ 1.3615, -0.4165,  0.4218,  ..., -0.3910, -1.3020,  0.5620],\n",
      "          [ 1.9869,  1.1568,  0.6641,  ...,  0.7882, -2.3114,  0.4096],\n",
      "          [ 1.1405,  0.0453,  0.4922,  ...,  0.7319, -2.0067,  0.7690],\n",
      "          ...,\n",
      "          [-0.0359, -0.7521, -0.3494,  ...,  0.4532, -0.7590, -1.2729],\n",
      "          [-0.0601, -0.7757, -0.3448,  ...,  0.4611, -0.7547, -1.2706],\n",
      "          [-0.0897, -0.7927, -0.3415,  ...,  0.4712, -0.7443, -1.2700]],\n",
      "\n",
      "         [[-0.4695, -1.1555, -0.3745,  ..., -1.0830,  0.7005, -0.5454],\n",
      "          [-1.3976, -0.7144,  0.3550,  ..., -0.8188,  0.6359, -1.1827],\n",
      "          [-1.2543, -0.5737, -0.3033,  ..., -0.1683,  0.7561, -0.3796],\n",
      "          ...,\n",
      "          [-3.5866,  0.0563,  0.2994,  ...,  0.9506,  0.5259, -0.6246],\n",
      "          [-3.5764,  0.0598,  0.2995,  ...,  0.9447,  0.5346, -0.6251],\n",
      "          [-3.5625,  0.0601,  0.3031,  ...,  0.9406,  0.5460, -0.6254]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2674, -0.5593,  0.4201,  ..., -1.7131,  0.9278, -0.2530],\n",
      "          [-0.2776, -0.6214, -0.2096,  ..., -1.7792,  1.6072, -0.6993],\n",
      "          [ 0.7342, -0.4282,  0.4229,  ..., -1.5459,  1.7050, -1.1965],\n",
      "          ...,\n",
      "          [ 1.1850, -0.6519,  0.0337,  ...,  1.5869,  1.5941, -0.0050],\n",
      "          [ 1.2051, -0.6692,  0.0232,  ...,  1.5886,  1.5705,  0.0226],\n",
      "          [ 1.2284, -0.6876,  0.0114,  ...,  1.5911,  1.5467,  0.0522]],\n",
      "\n",
      "         [[ 0.0711,  0.5442,  0.3248,  ...,  0.1591, -0.1884, -0.0691],\n",
      "          [-0.5335,  0.7892,  0.8685,  ...,  1.1205, -0.4272,  0.6926],\n",
      "          [-1.1782,  0.9229,  0.3709,  ...,  0.0519, -0.5219,  0.2484],\n",
      "          ...,\n",
      "          [-0.8539, -2.5313,  0.1042,  ..., -0.2085, -2.6916,  0.2289],\n",
      "          [-0.8231, -2.5372,  0.0785,  ..., -0.2309, -2.6804,  0.2235],\n",
      "          [-0.7895, -2.5360,  0.0570,  ..., -0.2609, -2.6655,  0.2182]],\n",
      "\n",
      "         [[-1.3183, -0.6089, -0.3217,  ...,  1.3868,  0.2805,  0.5257],\n",
      "          [-1.2737, -1.9467, -0.4744,  ...,  0.8821, -0.4793,  1.3492],\n",
      "          [-0.7765, -1.1905, -0.2687,  ...,  0.7716, -0.5271,  0.4831],\n",
      "          ...,\n",
      "          [-1.0247, -0.0046, -2.0442,  ..., -0.0157, -0.8852,  1.2180],\n",
      "          [-1.0220,  0.0076, -2.0321,  ..., -0.0223, -0.8951,  1.2109],\n",
      "          [-1.0113,  0.0299, -2.0104,  ..., -0.0276, -0.9036,  1.2107]]]],\n",
      "       grad_fn=<CloneBackward0>), tensor([[[[-2.0319e-01,  1.0402e+00,  2.2877e-01,  ...,  5.8931e-01,\n",
      "            2.3376e-01, -4.1057e-01],\n",
      "          [ 8.4527e-02,  8.2783e-01,  1.9754e-01,  ...,  2.6577e-01,\n",
      "            5.4997e-01, -4.6682e-01],\n",
      "          [ 2.7683e-01, -1.6244e-02,  5.1603e-01,  ...,  9.1001e-01,\n",
      "           -1.4142e-01, -8.3918e-01],\n",
      "          ...,\n",
      "          [ 1.7152e-03,  2.1671e-02,  6.2651e-02,  ...,  1.1991e-01,\n",
      "            5.7610e-02,  1.3987e-01],\n",
      "          [ 1.0782e-02,  2.1747e-02,  5.1870e-02,  ...,  1.1812e-01,\n",
      "            5.3205e-02,  1.3737e-01],\n",
      "          [ 1.6944e-02,  2.1961e-02,  4.1843e-02,  ...,  1.1640e-01,\n",
      "            4.7563e-02,  1.3452e-01]],\n",
      "\n",
      "         [[ 7.9500e-01, -3.8860e-01,  2.9239e-01,  ...,  4.2632e-01,\n",
      "           -3.5264e-01, -1.2195e+00],\n",
      "          [ 1.5604e-01, -6.9792e-01, -3.3409e-01,  ...,  1.0758e+00,\n",
      "            1.2377e-01,  3.1420e-01],\n",
      "          [ 2.6963e-01,  2.6851e-01,  4.3046e-01,  ...,  5.9356e-01,\n",
      "            8.7730e-01,  3.4380e-01],\n",
      "          ...,\n",
      "          [ 3.8208e-02,  5.2936e-02,  5.2935e-02,  ...,  9.1699e-02,\n",
      "            1.0606e-01,  2.9470e-02],\n",
      "          [ 3.3483e-02,  5.5764e-02,  5.5191e-02,  ...,  7.6645e-02,\n",
      "            1.0997e-01,  2.4886e-02],\n",
      "          [ 2.4161e-02,  5.8650e-02,  5.5492e-02,  ...,  6.1653e-02,\n",
      "            1.1609e-01,  2.2096e-02]],\n",
      "\n",
      "         [[ 1.3261e+00, -2.6276e-01, -1.3198e+00,  ..., -5.1435e-02,\n",
      "            5.1200e-02,  5.0782e-02],\n",
      "          [ 1.9346e+00, -1.6000e+00, -5.7051e-01,  ..., -8.2437e-01,\n",
      "           -6.0519e-01,  6.6489e-01],\n",
      "          [ 1.3129e+00,  8.7951e-02,  3.5245e-01,  ..., -4.1493e-01,\n",
      "            6.0039e-01,  3.3241e-01],\n",
      "          ...,\n",
      "          [ 1.0960e-01, -6.3668e-02,  8.2225e-02,  ...,  3.1584e-02,\n",
      "            2.2578e-01, -5.5064e-02],\n",
      "          [ 1.1095e-01, -5.8036e-02,  8.3326e-02,  ...,  3.7004e-02,\n",
      "            2.1139e-01, -4.7275e-02],\n",
      "          [ 1.1206e-01, -5.1486e-02,  8.2280e-02,  ...,  4.4708e-02,\n",
      "            1.9666e-01, -3.8847e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.5973e-01, -7.1297e-01, -3.6525e-01,  ..., -7.8194e-02,\n",
      "           -8.9000e-01, -1.7223e-02],\n",
      "          [-1.1341e-01, -4.0668e-01,  3.2207e-02,  ..., -1.4296e-01,\n",
      "           -7.4712e-01, -4.1083e-01],\n",
      "          [-2.1113e-02, -2.9255e-01,  2.0266e-01,  ..., -4.9856e-01,\n",
      "           -6.4191e-01, -1.1327e-01],\n",
      "          ...,\n",
      "          [ 1.6171e-02,  7.7098e-02, -3.9752e-02,  ..., -6.5617e-02,\n",
      "           -3.8403e-02, -4.2187e-02],\n",
      "          [ 7.0010e-03,  7.7943e-02, -4.9133e-02,  ..., -5.8591e-02,\n",
      "           -4.3330e-02, -4.9053e-02],\n",
      "          [ 1.2649e-03,  7.8892e-02, -5.2014e-02,  ..., -5.3283e-02,\n",
      "           -4.7203e-02, -5.3863e-02]],\n",
      "\n",
      "         [[-5.6582e-01,  2.3502e-01, -1.7045e-01,  ...,  3.8634e-01,\n",
      "           -2.7793e-01, -1.7562e-01],\n",
      "          [-7.8050e-02,  5.4837e-01,  4.1311e-02,  ...,  6.9413e-01,\n",
      "           -6.3422e-02,  3.7317e-01],\n",
      "          [-3.3857e-01,  6.8933e-01,  4.5255e-02,  ...,  5.2059e-01,\n",
      "           -4.4097e-01,  7.0852e-01],\n",
      "          ...,\n",
      "          [ 1.6572e-01, -1.2616e-01,  8.0682e-02,  ..., -5.5177e-02,\n",
      "           -9.8655e-02,  1.0701e-01],\n",
      "          [ 1.6074e-01, -1.1654e-01,  7.4815e-02,  ..., -5.7394e-02,\n",
      "           -9.5722e-02,  9.9424e-02],\n",
      "          [ 1.5916e-01, -1.0956e-01,  6.9408e-02,  ..., -6.0747e-02,\n",
      "           -9.2806e-02,  9.3385e-02]],\n",
      "\n",
      "         [[-5.4021e-01,  6.7669e-03,  1.0885e-01,  ..., -9.6784e-01,\n",
      "            1.9009e-01,  7.1214e-01],\n",
      "          [ 2.5780e-01,  3.2419e-01, -2.9532e-01,  ..., -5.0098e-01,\n",
      "            2.1423e-01,  4.5059e-02],\n",
      "          [-7.2464e-01, -2.1283e-01, -1.4096e-01,  ..., -6.5549e-01,\n",
      "            3.0954e-01,  8.8497e-01],\n",
      "          ...,\n",
      "          [ 1.3648e-01,  5.5476e-03,  6.1790e-02,  ...,  4.6726e-02,\n",
      "           -2.8636e-02,  1.0313e-01],\n",
      "          [ 1.2389e-01,  9.2481e-03,  6.4631e-02,  ...,  5.2459e-02,\n",
      "           -2.2686e-02,  9.7031e-02],\n",
      "          [ 1.1170e-01,  3.9610e-03,  6.1646e-02,  ...,  6.3987e-02,\n",
      "           -1.9520e-02,  9.5826e-02]]]], grad_fn=<CloneBackward0>), tensor([[[[-8.8398e-01,  6.6747e-01, -8.3046e-01,  ..., -7.7053e-01,\n",
      "            2.0660e-01, -6.2395e-02],\n",
      "          [-1.3359e+00,  2.6873e-01, -2.8624e-01,  ..., -1.7334e-01,\n",
      "           -1.2576e-03,  2.1756e-01],\n",
      "          [-6.7170e-01,  1.3275e-01, -3.7100e-01,  ...,  1.4059e-02,\n",
      "           -8.0236e-03,  3.4159e-02],\n",
      "          ...,\n",
      "          [-3.7990e-01, -6.3276e-02,  9.1923e-01,  ...,  9.0677e-01,\n",
      "            1.0621e-02,  8.1234e-02],\n",
      "          [-4.5224e-01, -4.0984e-01,  7.0939e-01,  ...,  1.0220e+00,\n",
      "            2.7095e-02,  4.7421e-01],\n",
      "          [-4.5242e-01, -3.7126e-01,  5.8368e-01,  ...,  1.1286e+00,\n",
      "           -7.8575e-02,  6.7801e-01]],\n",
      "\n",
      "         [[-6.5921e-01,  4.7387e-01, -5.3567e-02,  ..., -6.6407e-01,\n",
      "            8.0634e-01,  5.4275e-01],\n",
      "          [-9.2387e-01,  2.1050e-01, -7.5259e-01,  ..., -4.3606e-02,\n",
      "            4.2050e-01,  3.9165e-01],\n",
      "          [-4.0137e-01, -5.9787e-02, -1.5531e-01,  ..., -4.2685e-01,\n",
      "            8.1601e-01, -1.1389e-01],\n",
      "          ...,\n",
      "          [-9.7511e-01,  5.2966e-01, -1.4611e-01,  ...,  1.9996e-01,\n",
      "            5.1192e-01, -1.4267e-01],\n",
      "          [-1.1792e+00,  3.4277e-01, -1.9565e-01,  ...,  7.0388e-02,\n",
      "            5.3438e-01, -4.8555e-01],\n",
      "          [-1.3178e+00,  7.4636e-02, -8.6776e-02,  ...,  1.9944e-01,\n",
      "            5.2279e-01, -7.1424e-01]],\n",
      "\n",
      "         [[-5.1483e-01,  6.4113e-01,  1.9304e-01,  ..., -9.2278e-01,\n",
      "            4.0342e-01,  7.6082e-01],\n",
      "          [ 2.2743e-01,  2.2132e-01,  9.8197e-02,  ..., -8.5527e-01,\n",
      "            1.1485e-01,  9.1990e-01],\n",
      "          [-3.7289e-01, -3.2613e-01, -1.8399e-01,  ..., -1.3858e-01,\n",
      "            3.3645e-01,  1.4654e-01],\n",
      "          ...,\n",
      "          [ 4.4604e-01,  2.7641e-01,  2.5485e-01,  ..., -1.5455e+00,\n",
      "            2.5810e-01,  2.9118e-01],\n",
      "          [ 3.0861e-01,  2.2433e-01,  4.7955e-01,  ..., -1.5669e+00,\n",
      "            2.6343e-01,  5.0619e-01],\n",
      "          [ 1.8723e-01,  2.9714e-01,  3.9852e-01,  ..., -1.6452e+00,\n",
      "            1.1109e-01,  5.8471e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.2624e-01, -1.1131e+00,  5.3539e-02,  ..., -5.7859e-01,\n",
      "            4.5153e+00, -2.2991e-01],\n",
      "          [ 7.2273e-01, -1.7707e+00,  1.6076e-01,  ..., -2.6071e-01,\n",
      "            5.1108e+00, -3.6807e-01],\n",
      "          [-5.9156e-02, -2.2776e+00, -1.8202e-01,  ..., -5.5686e-01,\n",
      "            4.3482e+00, -1.4285e+00],\n",
      "          ...,\n",
      "          [ 1.3453e-01, -2.1995e+00, -8.0088e-01,  ..., -1.1486e+00,\n",
      "            4.0978e+00, -2.0610e-01],\n",
      "          [ 1.3695e-01, -2.2946e+00, -1.1141e+00,  ..., -1.3940e+00,\n",
      "            3.9360e+00, -3.3229e-01],\n",
      "          [ 2.0721e-01, -2.4381e+00, -1.2362e+00,  ..., -1.6409e+00,\n",
      "            3.8152e+00, -2.3589e-01]],\n",
      "\n",
      "         [[ 2.2941e-01, -5.4106e-01,  7.5633e-01,  ...,  5.7067e-01,\n",
      "           -1.5817e-01, -6.0530e-01],\n",
      "          [ 4.6364e-01, -1.0692e+00,  3.8124e-01,  ...,  2.5314e-01,\n",
      "            6.7280e-02, -1.2321e-01],\n",
      "          [ 1.1385e-01,  3.2269e-01,  8.8768e-01,  ..., -6.0442e-02,\n",
      "            2.4451e-01, -2.5106e-01],\n",
      "          ...,\n",
      "          [-1.3213e+00,  2.5436e-01,  6.1458e-01,  ..., -4.4777e-02,\n",
      "            6.3532e-02, -1.2263e-01],\n",
      "          [-1.3631e+00,  8.9426e-02,  3.6742e-01,  ..., -3.0212e-01,\n",
      "            3.3752e-01,  8.0391e-02],\n",
      "          [-1.2318e+00, -1.4250e-02,  3.5059e-01,  ..., -4.0122e-01,\n",
      "            5.0018e-01,  1.9730e-01]],\n",
      "\n",
      "         [[-3.9510e-01, -3.6196e-01, -3.0599e-01,  ...,  5.9633e-01,\n",
      "            2.1837e-01,  5.4105e-01],\n",
      "          [ 6.3055e-02, -7.1568e-01, -7.4774e-01,  ..., -5.5113e-01,\n",
      "           -8.2692e-03,  1.8469e-01],\n",
      "          [-8.1204e-02, -1.0082e+00, -4.2958e-01,  ..., -4.5139e-01,\n",
      "           -5.8222e-02,  2.4090e+00],\n",
      "          ...,\n",
      "          [ 1.3141e-01,  2.2918e-01, -1.7258e+00,  ...,  2.2524e-01,\n",
      "           -1.2139e+00,  2.0976e+00],\n",
      "          [ 1.6570e-02,  4.5274e-01, -1.5892e+00,  ...,  2.7730e-01,\n",
      "           -1.3412e+00,  2.1068e+00],\n",
      "          [ 1.5731e-01,  8.3775e-01, -1.4719e+00,  ...,  2.3035e-01,\n",
      "           -1.2735e+00,  1.9805e+00]]]], grad_fn=<CloneBackward0>), tensor([[[[-2.2077e-01, -3.4137e-01,  1.7996e-01,  ...,  5.0446e-03,\n",
      "            4.8720e-01,  2.9855e-01],\n",
      "          [-2.3404e-01, -3.2634e-02,  3.4618e-01,  ...,  6.9903e-01,\n",
      "           -4.0763e-01,  3.8090e-01],\n",
      "          [-8.0373e-01, -4.2138e-01, -4.3231e-01,  ..., -1.8627e-01,\n",
      "           -1.3301e-01,  4.8159e-01],\n",
      "          ...,\n",
      "          [ 3.7937e-01,  6.3270e-01,  7.5518e-01,  ...,  9.3434e-01,\n",
      "           -2.6209e-01, -5.1779e-01],\n",
      "          [ 5.3814e-01,  4.4454e-01,  7.0576e-01,  ...,  7.8696e-01,\n",
      "           -2.5241e-01, -7.6721e-01],\n",
      "          [ 4.2391e-01,  3.0068e-01,  6.3252e-01,  ...,  5.8176e-01,\n",
      "           -2.5173e-01, -5.3186e-01]],\n",
      "\n",
      "         [[-1.2066e+00, -8.0730e-02, -5.8048e-01,  ...,  7.1123e-02,\n",
      "           -5.4524e-02, -2.0091e-02],\n",
      "          [-1.1161e+00,  7.3169e-01, -6.5384e-01,  ...,  4.9665e-02,\n",
      "           -4.3841e-02,  4.3987e-01],\n",
      "          [-3.7159e-01, -1.6374e-01,  2.3126e-01,  ...,  6.1083e-01,\n",
      "           -6.7562e-02, -2.3664e-01],\n",
      "          ...,\n",
      "          [ 5.7099e-01,  2.6734e-01,  3.4013e-01,  ...,  1.5497e-01,\n",
      "           -6.0435e-01,  1.1953e+00],\n",
      "          [ 6.5049e-01,  2.5520e-01,  3.6328e-01,  ...,  3.6178e-01,\n",
      "           -9.2252e-01,  8.4066e-01],\n",
      "          [ 5.1639e-01,  1.9833e-01,  5.8595e-02,  ...,  6.1117e-01,\n",
      "           -1.1349e+00,  5.9490e-01]],\n",
      "\n",
      "         [[ 5.9372e-01,  9.3477e-01, -7.3769e-01,  ...,  2.0419e-01,\n",
      "            1.7844e-01, -8.7862e-01],\n",
      "          [ 7.0159e-01,  2.3855e-01, -2.5505e-01,  ..., -9.0846e-03,\n",
      "           -2.2762e-01, -5.9351e-01],\n",
      "          [-4.7733e-01, -1.3251e-01, -4.0892e-01,  ..., -2.8678e-02,\n",
      "           -2.1178e-01, -8.0466e-01],\n",
      "          ...,\n",
      "          [ 6.6102e-01,  4.8973e-01,  1.3892e-02,  ...,  2.0306e-01,\n",
      "            2.7055e-01,  8.2973e-01],\n",
      "          [ 4.2994e-01,  2.8357e-01,  1.5172e-02,  ...,  1.0071e-01,\n",
      "            1.3496e-02,  6.2212e-01],\n",
      "          [ 4.7066e-01,  2.6374e-01, -1.4292e-01,  ...,  1.0438e-01,\n",
      "            1.0226e-01,  6.3655e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.7963e-01,  3.4435e-01, -2.5403e-01,  ..., -1.1789e-01,\n",
      "            2.9184e-01,  3.7894e-02],\n",
      "          [-1.0039e+00, -5.2732e-02,  2.3098e-01,  ...,  4.1904e-02,\n",
      "           -2.2548e-01,  6.1730e-02],\n",
      "          [ 4.7216e-01, -4.8908e-01,  4.6940e-02,  ...,  1.8471e-01,\n",
      "           -4.8708e-01,  3.8123e-01],\n",
      "          ...,\n",
      "          [-3.3265e-01,  6.4884e-01,  4.0117e-01,  ..., -6.9239e-01,\n",
      "           -1.6943e-01, -1.8189e-01],\n",
      "          [-8.4761e-02,  4.8232e-01,  3.3993e-01,  ..., -4.1031e-01,\n",
      "           -2.7746e-02, -2.1951e-01],\n",
      "          [-1.2458e-01,  4.4714e-01,  2.2037e-01,  ..., -3.3179e-01,\n",
      "            8.9549e-04, -2.0181e-01]],\n",
      "\n",
      "         [[-1.7979e-01, -8.9153e-01,  8.2313e-01,  ...,  3.0339e-01,\n",
      "            1.3638e-01,  8.3105e-01],\n",
      "          [-5.3624e-01, -6.0551e-01,  1.5632e+00,  ..., -8.5577e-02,\n",
      "           -5.1959e-01,  9.6718e-02],\n",
      "          [ 4.1372e-01,  5.4204e-02,  3.6921e-01,  ...,  2.1407e-01,\n",
      "            4.3747e-01,  1.4917e-01],\n",
      "          ...,\n",
      "          [-9.8795e-02, -3.1939e-01, -1.2019e-01,  ...,  6.6542e-01,\n",
      "           -1.3964e+00, -1.1890e+00],\n",
      "          [ 2.1291e-01, -4.3581e-01,  1.7832e-01,  ...,  4.6582e-01,\n",
      "           -1.4799e+00, -9.8484e-01],\n",
      "          [ 3.4329e-01, -5.8131e-01,  2.9117e-01,  ...,  3.9046e-01,\n",
      "           -1.5581e+00, -8.8868e-01]],\n",
      "\n",
      "         [[ 1.2135e+00,  6.3766e-01,  1.6164e-01,  ...,  1.7492e-01,\n",
      "           -1.0545e+00,  4.0230e-01],\n",
      "          [ 5.6738e-01,  5.1046e-02, -4.2664e-01,  ...,  2.0131e-01,\n",
      "           -4.2381e-01,  3.7144e-01],\n",
      "          [ 7.1270e-02, -5.7635e-02,  3.4118e-01,  ..., -1.5310e-02,\n",
      "           -6.9834e-02, -4.2584e-02],\n",
      "          ...,\n",
      "          [ 2.6373e-01, -4.3388e-01,  3.5497e-01,  ...,  4.9940e-01,\n",
      "            6.4136e-02,  1.9531e-01],\n",
      "          [ 1.8570e-01, -2.7319e-01,  1.6569e-01,  ...,  5.0223e-01,\n",
      "            2.8446e-01,  8.2139e-02],\n",
      "          [ 2.1326e-01, -1.1604e-01, -1.1993e-02,  ...,  3.8622e-01,\n",
      "            2.8113e-01,  2.0572e-01]]]], grad_fn=<CloneBackward0>)), (tensor([[[[-1.0021,  0.6180, -2.0834,  ..., -1.0992, -0.1219,  0.2216],\n",
      "          [-1.6790,  0.3466, -2.7691,  ..., -0.7514, -0.0673,  0.3066],\n",
      "          [-1.3654,  1.0232, -1.8892,  ..., -1.0239, -0.5347,  0.3321],\n",
      "          ...,\n",
      "          [ 0.7012, -0.4609, -0.9687,  ..., -1.9933, -0.5850, -0.2504],\n",
      "          [ 0.6987, -0.4612, -0.9680,  ..., -1.9907, -0.5846, -0.2539],\n",
      "          [ 0.6950, -0.4615, -0.9674,  ..., -1.9876, -0.5848, -0.2561]],\n",
      "\n",
      "         [[-0.9523, -0.6728,  0.4819,  ...,  0.9748,  3.0600,  0.9084],\n",
      "          [-1.2076, -1.3434,  0.2769,  ...,  1.5552,  2.9067,  1.3275],\n",
      "          [-0.9676,  0.0349, -0.0602,  ...,  1.3067,  2.4103,  1.1792],\n",
      "          ...,\n",
      "          [ 0.0746,  0.5274, -0.1477,  ...,  1.5499,  0.1700,  0.5745],\n",
      "          [ 0.0731,  0.5299, -0.1498,  ...,  1.5494,  0.1680,  0.5753],\n",
      "          [ 0.0720,  0.5325, -0.1519,  ...,  1.5481,  0.1659,  0.5769]],\n",
      "\n",
      "         [[-0.5456, -1.5343, -1.6079,  ...,  0.5801, -1.0623, -3.1363],\n",
      "          [-0.6132, -2.2069, -1.7574,  ...,  1.4840, -2.2158, -2.8881],\n",
      "          [-0.2898, -0.5124, -1.8416,  ..., -0.1389, -1.5765, -3.9012],\n",
      "          ...,\n",
      "          [ 1.0464,  1.1683, -0.8197,  ...,  1.4204, -1.6557, -1.4016],\n",
      "          [ 1.0478,  1.1722, -0.8212,  ...,  1.4234, -1.6513, -1.3979],\n",
      "          [ 1.0488,  1.1765, -0.8216,  ...,  1.4264, -1.6463, -1.3930]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6834, -2.3686,  1.5838,  ..., -2.1897,  0.3334, -0.0361],\n",
      "          [-0.5326, -2.6584,  1.8149,  ..., -2.1950,  1.1318,  0.9679],\n",
      "          [-0.7079, -3.0974,  1.9218,  ..., -1.9310,  0.3686,  0.3013],\n",
      "          ...,\n",
      "          [ 0.8951, -0.6444,  0.3863,  ..., -2.7570,  0.8744,  0.5467],\n",
      "          [ 0.8975, -0.6424,  0.3881,  ..., -2.7557,  0.8739,  0.5472],\n",
      "          [ 0.8991, -0.6399,  0.3884,  ..., -2.7536,  0.8739,  0.5471]],\n",
      "\n",
      "         [[ 2.4798,  1.2337,  0.3571,  ..., -2.6021,  2.6758,  0.9562],\n",
      "          [ 2.3047, -0.3188,  1.2664,  ..., -0.9658,  3.2616,  0.7100],\n",
      "          [ 2.3724,  0.8194,  1.1455,  ..., -1.0892,  2.2662,  0.5493],\n",
      "          ...,\n",
      "          [ 0.1398, -0.3975,  0.1422,  ...,  0.4082,  0.8129,  0.0458],\n",
      "          [ 0.1347, -0.3963,  0.1432,  ...,  0.4076,  0.8185,  0.0505],\n",
      "          [ 0.1287, -0.3965,  0.1445,  ...,  0.4079,  0.8240,  0.0556]],\n",
      "\n",
      "         [[-1.2288, -0.7343, -0.8155,  ...,  3.5509,  2.2057, -0.3203],\n",
      "          [-1.5608, -0.6277, -1.8668,  ...,  2.9928,  2.5749,  0.4325],\n",
      "          [-0.8948, -0.9267, -2.0973,  ...,  2.4689,  2.4076,  0.4154],\n",
      "          ...,\n",
      "          [ 0.4210, -0.5470, -1.3089,  ...,  0.3818,  0.7423,  0.9682],\n",
      "          [ 0.4210, -0.5431, -1.3081,  ...,  0.3771,  0.7425,  0.9694],\n",
      "          [ 0.4225, -0.5412, -1.3082,  ...,  0.3707,  0.7425,  0.9719]]]],\n",
      "       grad_fn=<CloneBackward0>), tensor([[[[ 1.2138e-01,  9.3923e-01,  3.1421e-01,  ..., -2.4269e-01,\n",
      "            1.2123e+00,  2.9983e-01],\n",
      "          [-1.3573e-01,  6.9042e-01,  3.7442e-01,  ...,  4.0155e-01,\n",
      "            1.1694e+00,  9.7888e-01],\n",
      "          [ 1.4985e-01,  1.7020e-01,  1.0945e+00,  ...,  8.7339e-02,\n",
      "            1.3452e+00,  4.2500e-01],\n",
      "          ...,\n",
      "          [ 1.2171e-01, -3.3470e-02, -2.4578e-01,  ..., -3.6736e-02,\n",
      "            1.3220e-01, -1.2286e-02],\n",
      "          [ 1.1828e-01, -3.2673e-02, -2.4437e-01,  ..., -3.2475e-02,\n",
      "            1.2968e-01, -1.0435e-02],\n",
      "          [ 1.1496e-01, -3.2520e-02, -2.4464e-01,  ..., -2.6938e-02,\n",
      "            1.2692e-01, -8.5965e-03]],\n",
      "\n",
      "         [[ 2.4325e-01, -2.3532e-01,  1.5622e+00,  ..., -4.9622e-02,\n",
      "            3.0737e-01, -1.8127e-02],\n",
      "          [ 5.0578e-01, -1.2928e-02,  1.1537e+00,  ...,  3.0881e-01,\n",
      "            6.5105e-01,  9.6414e-01],\n",
      "          [ 1.2581e+00,  4.5498e-01,  1.2141e+00,  ..., -7.2340e-01,\n",
      "            1.5641e-01,  2.6554e-01],\n",
      "          ...,\n",
      "          [ 8.7402e-03, -1.3918e-01, -9.9427e-03,  ..., -3.5467e-02,\n",
      "           -6.1411e-02,  1.2248e-01],\n",
      "          [ 1.0020e-02, -1.3933e-01, -5.8789e-03,  ..., -3.5182e-02,\n",
      "           -6.5179e-02,  1.2034e-01],\n",
      "          [ 1.1497e-02, -1.3827e-01, -6.8556e-04,  ..., -3.5906e-02,\n",
      "           -6.7957e-02,  1.1880e-01]],\n",
      "\n",
      "         [[ 7.0125e-01, -2.9303e-01,  3.1925e-01,  ...,  6.1942e-01,\n",
      "            4.9216e-01,  2.5452e-01],\n",
      "          [ 6.6950e-01, -5.1723e-01,  2.3104e-01,  ...,  4.8782e-01,\n",
      "            4.6598e-01,  3.1191e-01],\n",
      "          [ 2.3991e-01, -9.0481e-01, -3.3925e-01,  ...,  7.2813e-01,\n",
      "            8.4995e-01,  2.1693e-01],\n",
      "          ...,\n",
      "          [-3.8132e-03, -4.5911e-02, -2.4177e-02,  ...,  1.1241e-01,\n",
      "            2.1823e-02, -1.5002e-01],\n",
      "          [-3.8238e-03, -4.4445e-02, -2.4679e-02,  ...,  1.1221e-01,\n",
      "            2.2193e-02, -1.4926e-01],\n",
      "          [-4.0870e-03, -4.2175e-02, -2.4884e-02,  ...,  1.1082e-01,\n",
      "            2.2508e-02, -1.4865e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0094e+00,  8.9296e-02,  2.1212e-01,  ...,  1.8916e-01,\n",
      "            8.8819e-01,  3.8370e-01],\n",
      "          [-5.9131e-01,  4.5124e-01,  5.8093e-01,  ...,  6.0094e-01,\n",
      "            1.2338e+00,  1.2545e+00],\n",
      "          [-9.1118e-01,  2.9155e-01,  3.6969e-01,  ...,  1.2622e+00,\n",
      "            1.1446e+00,  2.3608e-01],\n",
      "          ...,\n",
      "          [-1.4444e-02, -3.0370e-02, -3.4505e-02,  ...,  1.6161e-03,\n",
      "            3.0287e-02,  5.5130e-04],\n",
      "          [-1.3948e-02, -2.9087e-02, -3.5455e-02,  ...,  1.9066e-03,\n",
      "            3.0988e-02,  6.7397e-04],\n",
      "          [-1.3628e-02, -2.9169e-02, -3.5788e-02,  ...,  1.4450e-03,\n",
      "            3.1063e-02,  1.2707e-03]],\n",
      "\n",
      "         [[ 1.6292e-01,  9.8170e-02, -1.0971e-01,  ..., -8.2333e-01,\n",
      "            4.0503e-01,  1.3888e-01],\n",
      "          [ 1.3706e-01,  3.1126e-01, -1.3000e-01,  ..., -7.3050e-01,\n",
      "           -9.4381e-02,  5.3365e-01],\n",
      "          [ 3.0602e-01,  5.2564e-01, -2.3322e-01,  ...,  1.4342e-01,\n",
      "            4.8848e-01,  2.7745e-02],\n",
      "          ...,\n",
      "          [ 5.2220e-02, -1.0123e-02, -1.6384e-01,  ...,  1.2480e-02,\n",
      "            3.5626e-02,  1.0925e-01],\n",
      "          [ 5.1407e-02, -1.2068e-02, -1.6271e-01,  ...,  1.4775e-02,\n",
      "            3.4108e-02,  1.1061e-01],\n",
      "          [ 5.0481e-02, -1.2396e-02, -1.6246e-01,  ...,  1.7020e-02,\n",
      "            3.2081e-02,  1.1210e-01]],\n",
      "\n",
      "         [[-2.6748e-01, -1.7025e+00,  5.1469e-02,  ..., -1.2428e+00,\n",
      "           -2.4549e+00,  1.1522e+00],\n",
      "          [ 1.6964e-03, -1.2115e+00, -1.0368e-01,  ..., -9.1492e-01,\n",
      "           -2.1319e+00,  6.8533e-02],\n",
      "          [ 2.9550e-02, -1.5085e+00, -2.5001e-01,  ..., -4.2819e-01,\n",
      "           -9.5622e-01,  1.3181e+00],\n",
      "          ...,\n",
      "          [-6.8926e-02,  1.4441e-02,  1.3974e-02,  ...,  1.8576e-02,\n",
      "           -4.1673e-02,  1.6239e-02],\n",
      "          [-6.5776e-02,  1.6143e-02,  1.4439e-02,  ...,  1.9108e-02,\n",
      "           -4.1071e-02,  1.5136e-02],\n",
      "          [-6.3842e-02,  1.7452e-02,  1.5535e-02,  ...,  2.0382e-02,\n",
      "           -4.0419e-02,  1.3608e-02]]]], grad_fn=<CloneBackward0>), tensor([[[[ 1.1293, -0.3462, -0.3732,  ...,  0.5926,  0.7812, -0.4148],\n",
      "          [ 0.7510, -0.0254, -0.5259,  ...,  0.8366,  0.5815, -0.5686],\n",
      "          [-0.2422,  0.2089,  0.4589,  ...,  0.2325, -0.0509, -0.9703],\n",
      "          ...,\n",
      "          [ 0.2086, -0.3104,  0.2101,  ...,  0.2062, -0.3851, -0.8758],\n",
      "          [-0.2273, -0.0501,  0.2581,  ..., -0.2382, -0.4439, -0.7973],\n",
      "          [-0.2253, -0.0436,  0.2718,  ..., -0.4463, -0.4388, -0.8130]],\n",
      "\n",
      "         [[-0.8634,  0.1109,  0.0669,  ..., -0.3938, -0.2305,  0.0083],\n",
      "          [-0.8774,  0.2737,  0.0839,  ..., -0.6209,  0.4074, -1.0181],\n",
      "          [-0.3421, -0.2056,  0.9072,  ..., -1.2327,  0.1921, -0.0164],\n",
      "          ...,\n",
      "          [-0.5329,  0.6801,  1.5773,  ..., -0.3120, -1.2555, -0.4942],\n",
      "          [-0.5438,  0.6120,  1.7088,  ..., -0.2394, -1.2213, -0.6854],\n",
      "          [-0.4161,  0.5767,  1.7298,  ..., -0.3002, -1.2227, -0.8086]],\n",
      "\n",
      "         [[-0.0253, -0.0391, -0.6674,  ..., -0.1305, -0.0133,  0.2235],\n",
      "          [-0.7589,  0.4665, -0.6497,  ..., -0.0307, -0.6618, -0.1693],\n",
      "          [ 0.0151,  0.1495, -0.5433,  ...,  0.2310,  0.6789,  0.0479],\n",
      "          ...,\n",
      "          [-0.9089, -0.2286, -0.1881,  ..., -0.3933, -0.7751, -0.1990],\n",
      "          [-0.6703,  0.0785, -0.4741,  ..., -0.4584, -0.8341, -0.3992],\n",
      "          [-0.4879,  0.1819, -0.6165,  ..., -0.4382, -0.8065, -0.5467]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4239, -0.9535,  0.3519,  ..., -0.7349, -0.2406, -1.1344],\n",
      "          [ 0.6338, -0.9685,  0.2037,  ..., -0.6365, -0.4304, -1.3538],\n",
      "          [ 0.7559, -1.4811, -1.0519,  ..., -0.1644,  0.2767, -1.7572],\n",
      "          ...,\n",
      "          [-0.2019, -1.3701, -0.0717,  ..., -1.5800, -1.4760, -2.4818],\n",
      "          [ 0.0686, -1.2296, -0.2982,  ..., -1.3333, -1.2977, -2.4605],\n",
      "          [ 0.3114, -1.0404, -0.5018,  ..., -1.3419, -1.2466, -2.4538]],\n",
      "\n",
      "         [[-0.8893, -0.9156,  0.8059,  ...,  1.0868, -1.2108, -1.3908],\n",
      "          [-0.5737, -0.1901,  0.9037,  ...,  1.5473, -0.9658, -1.4860],\n",
      "          [ 0.1969, -0.2260,  0.8777,  ...,  0.7684, -0.3183, -1.4050],\n",
      "          ...,\n",
      "          [-1.1370, -0.0787,  1.6409,  ...,  0.9408, -1.7989, -1.5206],\n",
      "          [-0.9814, -0.2295,  1.3871,  ...,  0.4419, -1.8873, -1.5750],\n",
      "          [-0.9723, -0.1648,  1.1330,  ...,  0.2362, -1.9310, -1.7582]],\n",
      "\n",
      "         [[ 0.1331,  0.9008, -0.4509,  ...,  0.1466, -0.3027,  1.3705],\n",
      "          [ 0.6812,  1.4410, -0.7503,  ..., -0.1885, -0.2610,  1.3627],\n",
      "          [-0.0329,  0.2353,  0.2005,  ...,  0.3822, -0.5128, -0.0290],\n",
      "          ...,\n",
      "          [-0.3073,  0.7453, -0.3588,  ..., -1.0156, -1.1314,  2.1866],\n",
      "          [-0.4224,  0.7769, -0.1737,  ..., -0.8182, -1.1360,  1.9161],\n",
      "          [-0.3149,  0.6191, -0.1507,  ..., -0.5705, -1.1407,  1.7754]]]],\n",
      "       grad_fn=<CloneBackward0>), tensor([[[[-0.5221, -0.3275, -0.0130,  ...,  0.2856,  1.1506, -0.3369],\n",
      "          [-0.8150,  0.5207,  0.0759,  ..., -0.3753,  1.1473,  0.1203],\n",
      "          [-0.2201, -0.0127,  0.4524,  ..., -0.8704,  0.1767, -0.6021],\n",
      "          ...,\n",
      "          [-0.1439, -0.3730,  0.1623,  ..., -0.5658, -0.1373, -0.1703],\n",
      "          [-0.1715, -0.2127,  0.3080,  ..., -0.5442, -0.3948, -0.3232],\n",
      "          [-0.1786, -0.2320,  0.4237,  ..., -0.5383, -0.3970, -0.3963]],\n",
      "\n",
      "         [[-0.1179, -0.0550,  1.2829,  ...,  0.9050,  0.5023,  0.6142],\n",
      "          [-0.6304,  0.5461,  0.3508,  ...,  0.5900, -0.1306,  1.8483],\n",
      "          [-0.1637, -0.3014,  0.0308,  ...,  0.4728,  0.6483, -0.1857],\n",
      "          ...,\n",
      "          [-0.5409, -0.6050,  0.5730,  ...,  0.0195, -0.2215, -0.3039],\n",
      "          [-0.5051, -0.5107,  0.6715,  ..., -0.3021, -0.3273, -0.3265],\n",
      "          [-0.5071, -0.3480,  0.7490,  ..., -0.3332, -0.2592, -0.3750]],\n",
      "\n",
      "         [[ 0.1575,  0.2050,  0.4868,  ...,  0.8484, -0.9524, -0.7325],\n",
      "          [ 0.1049,  0.7956, -0.4515,  ...,  0.7314,  0.1853, -1.0688],\n",
      "          [ 0.8279,  0.1757, -0.1962,  ...,  0.4893, -0.2531, -0.2496],\n",
      "          ...,\n",
      "          [ 0.1220,  0.3600,  0.0317,  ...,  0.4157, -0.5204, -0.3599],\n",
      "          [ 0.1005,  0.2823,  0.0687,  ...,  0.3693, -0.6100, -0.3121],\n",
      "          [ 0.1479,  0.3380,  0.0039,  ...,  0.4637, -0.6344, -0.3493]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5717,  0.6177,  0.7940,  ...,  0.0746,  0.8641, -0.6648],\n",
      "          [-0.3575,  0.1935,  0.5461,  ..., -0.1901,  1.5946, -0.2333],\n",
      "          [ 0.0726,  0.3827, -0.2728,  ..., -0.0799,  0.5033, -0.9041],\n",
      "          ...,\n",
      "          [-0.0627,  0.4050,  0.1924,  ...,  0.2432, -0.3762,  0.3149],\n",
      "          [ 0.0435,  0.3750,  0.0769,  ...,  0.0185, -0.2745,  0.1463],\n",
      "          [ 0.1300,  0.3881,  0.0179,  ..., -0.0769, -0.1646,  0.0871]],\n",
      "\n",
      "         [[ 0.8207, -0.4508, -0.0637,  ...,  0.2657, -0.0244, -0.2849],\n",
      "          [ 1.0922, -0.9684,  0.3262,  ...,  0.3965,  0.0315, -0.1479],\n",
      "          [ 0.0209,  0.0091, -0.4958,  ...,  0.2173, -0.2465, -0.2510],\n",
      "          ...,\n",
      "          [ 0.1934,  0.1992,  0.2382,  ..., -0.0257, -0.1239, -0.0166],\n",
      "          [ 0.2786,  0.1039,  0.1426,  ..., -0.1938, -0.2539, -0.0525],\n",
      "          [ 0.1325,  0.0258,  0.0692,  ..., -0.2420, -0.2242,  0.0222]],\n",
      "\n",
      "         [[-0.0646, -0.0834,  0.3203,  ...,  0.6062, -0.4235, -0.1079],\n",
      "          [ 0.0617, -0.2424,  1.0341,  ...,  0.7827,  0.5649, -0.1848],\n",
      "          [ 0.0970,  0.0744,  0.2078,  ..., -0.7543,  0.5935, -0.1213],\n",
      "          ...,\n",
      "          [ 0.0295,  0.2543, -0.2620,  ...,  0.7007,  0.6821,  0.2879],\n",
      "          [-0.1086,  0.3040, -0.0517,  ...,  0.5283,  0.7245,  0.2463],\n",
      "          [-0.2635,  0.3722, -0.0325,  ...,  0.4500,  0.5979,  0.1977]]]],\n",
      "       grad_fn=<CloneBackward0>)), (tensor([[[[-2.8246, -0.0701, -2.3983,  ..., -0.5460,  2.1630,  0.1580],\n",
      "          [-3.2481,  0.0344, -2.5297,  ..., -1.1796,  2.7761,  0.2881],\n",
      "          [-3.4704,  0.3521, -1.9870,  ...,  0.1310,  2.8254, -0.4765],\n",
      "          ...,\n",
      "          [-0.5220, -0.0819, -1.2043,  ...,  1.2279,  1.6588,  0.1002],\n",
      "          [-0.5195, -0.0829, -1.2023,  ...,  1.2275,  1.6595,  0.0976],\n",
      "          [-0.5173, -0.0829, -1.2002,  ...,  1.2261,  1.6600,  0.0953]],\n",
      "\n",
      "         [[-0.3626,  0.7415, -1.7218,  ..., -0.5565, -0.7612, -0.1647],\n",
      "          [-0.5833,  0.0049, -2.5385,  ..., -0.5933, -0.5539, -0.7579],\n",
      "          [-0.7643,  0.3940, -2.4996,  ...,  0.9377, -1.2760, -1.1408],\n",
      "          ...,\n",
      "          [ 0.7519,  0.2335, -1.0402,  ...,  0.5051, -0.9637,  0.1040],\n",
      "          [ 0.7534,  0.2333, -1.0391,  ...,  0.5059, -0.9638,  0.1067],\n",
      "          [ 0.7551,  0.2322, -1.0384,  ...,  0.5066, -0.9637,  0.1095]],\n",
      "\n",
      "         [[ 0.6685, -0.1082, -0.0061,  ..., -0.5687, -0.9937,  1.3929],\n",
      "          [ 0.5959,  0.5554, -0.2001,  ..., -0.5450, -0.5030,  1.2457],\n",
      "          [ 0.7586,  0.0784, -0.7827,  ..., -0.4934, -0.3774,  1.1174],\n",
      "          ...,\n",
      "          [-0.4613, -0.6369, -1.4198,  ..., -0.6091, -1.6911, -1.5837],\n",
      "          [-0.4624, -0.6365, -1.4185,  ..., -0.6083, -1.6908, -1.5855],\n",
      "          [-0.4629, -0.6361, -1.4184,  ..., -0.6081, -1.6901, -1.5876]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7840,  0.6967,  1.9747,  ..., -2.3559, -0.3577,  0.9857],\n",
      "          [ 0.3191, -0.1355,  1.3500,  ..., -2.6101,  0.6263,  2.3093],\n",
      "          [ 0.4277,  0.7379,  1.9629,  ..., -1.8157,  0.9474,  1.8580],\n",
      "          ...,\n",
      "          [-0.8542,  0.1563,  0.6788,  ..., -0.5230, -0.7516,  2.8238],\n",
      "          [-0.8548,  0.1557,  0.6789,  ..., -0.5249, -0.7516,  2.8232],\n",
      "          [-0.8559,  0.1549,  0.6785,  ..., -0.5256, -0.7526,  2.8224]],\n",
      "\n",
      "         [[-2.6038, -0.2814,  0.8700,  ...,  2.4495,  1.6194,  1.1469],\n",
      "          [-2.9579, -0.3112,  1.0253,  ...,  2.9691,  1.4108,  0.9915],\n",
      "          [-2.4584, -0.4946,  0.2251,  ...,  3.2491,  1.2209,  1.1755],\n",
      "          ...,\n",
      "          [ 1.1074, -1.5341,  0.2463,  ..., -0.2184,  1.6805,  0.7716],\n",
      "          [ 1.1082, -1.5329,  0.2446,  ..., -0.2220,  1.6827,  0.7719],\n",
      "          [ 1.1095, -1.5318,  0.2429,  ..., -0.2253,  1.6832,  0.7726]],\n",
      "\n",
      "         [[-0.2355, -2.0037, -0.7001,  ..., -0.2944,  2.4622, -0.4624],\n",
      "          [-0.9386, -3.0879, -0.0056,  ..., -0.0687,  3.1249, -1.6206],\n",
      "          [-1.0322, -2.7960,  0.1962,  ..., -0.9987,  2.1640, -1.8565],\n",
      "          ...,\n",
      "          [-1.3348, -0.9185, -0.4607,  ..., -1.3237,  1.4517, -2.1121],\n",
      "          [-1.3355, -0.9181, -0.4599,  ..., -1.3255,  1.4513, -2.1121],\n",
      "          [-1.3358, -0.9175, -0.4595,  ..., -1.3265,  1.4503, -2.1111]]]],\n",
      "       grad_fn=<CloneBackward0>), tensor([[[[-2.6017e-01, -9.5550e-01, -6.6637e-01,  ..., -3.1480e-01,\n",
      "           -1.6557e-01,  2.6130e-01],\n",
      "          [-3.7766e-01, -9.5815e-01, -6.2225e-01,  ..., -5.0890e-01,\n",
      "           -2.3371e-01,  2.8659e-01],\n",
      "          [-5.2501e-01, -9.5833e-01, -7.3654e-01,  ..., -6.0078e-01,\n",
      "            8.9426e-02,  9.6288e-02],\n",
      "          ...,\n",
      "          [ 2.1583e-03, -1.0593e-01,  4.0756e-02,  ..., -1.0855e-02,\n",
      "            1.1780e-01,  5.5622e-02],\n",
      "          [ 2.7963e-03, -1.0603e-01,  3.9617e-02,  ..., -1.0185e-02,\n",
      "            1.1791e-01,  5.5473e-02],\n",
      "          [ 3.7385e-03, -1.0625e-01,  3.8535e-02,  ..., -1.0351e-02,\n",
      "            1.1798e-01,  5.4887e-02]],\n",
      "\n",
      "         [[ 3.6656e-01, -6.9300e-01, -5.5998e-02,  ..., -3.5897e-01,\n",
      "           -1.0781e-01, -1.4472e-01],\n",
      "          [ 1.9343e-01, -6.0823e-01,  2.1653e-01,  ..., -6.4312e-01,\n",
      "           -4.2983e-01, -4.2318e-01],\n",
      "          [ 2.3240e-01, -7.2230e-01,  2.6060e-02,  ..., -8.9580e-03,\n",
      "           -5.2856e-01, -5.1998e-01],\n",
      "          ...,\n",
      "          [ 5.7219e-02,  2.4131e-02,  9.9474e-02,  ...,  8.4745e-03,\n",
      "           -2.9408e-02,  1.6149e-01],\n",
      "          [ 5.6812e-02,  2.4289e-02,  9.9810e-02,  ...,  7.5370e-03,\n",
      "           -2.9783e-02,  1.6179e-01],\n",
      "          [ 5.6402e-02,  2.4131e-02,  1.0001e-01,  ...,  6.9989e-03,\n",
      "           -2.9528e-02,  1.6182e-01]],\n",
      "\n",
      "         [[ 6.3029e-01,  5.1350e-01, -4.4593e-01,  ..., -8.1685e-01,\n",
      "           -4.4257e-01,  4.5773e-01],\n",
      "          [ 3.6454e-01,  6.2002e-01, -5.9781e-01,  ..., -7.2008e-01,\n",
      "           -2.6635e-01,  2.3195e-01],\n",
      "          [ 5.0915e-01,  5.5309e-01, -5.0930e-01,  ..., -4.4237e-01,\n",
      "           -3.3053e-01,  2.9337e-02],\n",
      "          ...,\n",
      "          [-3.4223e-03, -1.3979e-01, -7.2418e-02,  ...,  1.4077e-01,\n",
      "           -5.5375e-02,  1.3262e-01],\n",
      "          [-4.0995e-03, -1.4022e-01, -7.0940e-02,  ...,  1.3974e-01,\n",
      "           -5.5812e-02,  1.3321e-01],\n",
      "          [-4.6918e-03, -1.4054e-01, -6.9215e-02,  ...,  1.3921e-01,\n",
      "           -5.6360e-02,  1.3366e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.5181e-01, -1.6503e-01,  2.5068e-01,  ...,  5.6933e-01,\n",
      "           -1.1918e-01, -1.9101e-01],\n",
      "          [ 6.7154e-01,  3.7980e-02, -4.9071e-03,  ...,  7.8871e-01,\n",
      "           -3.3362e-01,  2.4400e-01],\n",
      "          [ 3.7878e-01,  1.2485e-01,  7.2427e-02,  ...,  3.4891e-01,\n",
      "            8.0542e-02,  2.4441e-01],\n",
      "          ...,\n",
      "          [-5.4068e-03, -4.4216e-02, -3.3204e-02,  ..., -5.0170e-02,\n",
      "            2.1201e-01,  1.3092e-01],\n",
      "          [-5.2623e-03, -4.3632e-02, -3.3508e-02,  ..., -5.0609e-02,\n",
      "            2.1110e-01,  1.2846e-01],\n",
      "          [-4.8602e-03, -4.2926e-02, -3.3958e-02,  ..., -5.0376e-02,\n",
      "            2.0999e-01,  1.2608e-01]],\n",
      "\n",
      "         [[-7.4712e-01,  8.8823e-01,  5.7123e-01,  ..., -5.9617e-01,\n",
      "           -3.1263e-01,  3.3684e-01],\n",
      "          [-9.8395e-01,  7.2746e-01,  5.7152e-01,  ..., -2.7761e-01,\n",
      "           -7.8409e-01,  3.6495e-01],\n",
      "          [-3.1553e-01,  7.7699e-01,  7.5479e-01,  ..., -3.7628e-01,\n",
      "           -6.6337e-01,  5.0001e-01],\n",
      "          ...,\n",
      "          [ 3.6535e-02,  3.7993e-02,  1.6979e-01,  ...,  3.9841e-02,\n",
      "           -1.5570e-01,  5.8319e-02],\n",
      "          [ 3.6791e-02,  3.8116e-02,  1.6980e-01,  ...,  3.9536e-02,\n",
      "           -1.5570e-01,  5.7897e-02],\n",
      "          [ 3.6988e-02,  3.8535e-02,  1.6959e-01,  ...,  3.9310e-02,\n",
      "           -1.5595e-01,  5.7855e-02]],\n",
      "\n",
      "         [[ 6.0030e-01,  2.1721e-01,  5.7246e-01,  ...,  8.7242e-01,\n",
      "           -4.9424e-01,  1.3478e+00],\n",
      "          [ 6.0311e-01,  4.5596e-01,  4.6649e-01,  ...,  9.5352e-01,\n",
      "           -4.9166e-01,  1.0958e+00],\n",
      "          [-8.4464e-02,  3.0614e-01,  6.2210e-01,  ...,  7.9563e-01,\n",
      "           -6.4421e-01,  1.3069e+00],\n",
      "          ...,\n",
      "          [-7.2162e-02,  1.1216e-02, -1.6560e-02,  ..., -1.6233e-02,\n",
      "           -4.2974e-02, -3.0738e-03],\n",
      "          [-7.2533e-02,  1.0208e-02, -1.6256e-02,  ..., -1.5509e-02,\n",
      "           -4.2960e-02, -1.2460e-03],\n",
      "          [-7.2663e-02,  8.7417e-03, -1.5883e-02,  ..., -1.5438e-02,\n",
      "           -4.2769e-02,  7.5326e-04]]]], grad_fn=<CloneBackward0>), tensor([[[[ 4.0018e-01,  1.1920e+00, -8.6061e-02,  ...,  7.6161e-02,\n",
      "           -3.4738e-02,  5.1802e-01],\n",
      "          [ 1.9415e-01,  1.7629e+00, -4.0144e-01,  ..., -7.5315e-01,\n",
      "            4.1664e-01,  1.7349e-01],\n",
      "          [ 1.9497e-02, -2.6249e-01,  2.6372e-01,  ..., -7.3124e-01,\n",
      "           -4.2140e-01, -1.5002e-01],\n",
      "          ...,\n",
      "          [ 6.9399e-01,  1.2856e+00, -2.5337e-01,  ..., -1.3611e+00,\n",
      "            3.5584e-01,  6.3143e-02],\n",
      "          [ 6.5889e-01,  9.2130e-01,  1.1061e-01,  ..., -1.4893e+00,\n",
      "            3.5712e-01,  2.0551e-01],\n",
      "          [ 5.9256e-01,  7.2877e-01,  2.2932e-01,  ..., -1.7854e+00,\n",
      "            3.0529e-01,  3.1892e-01]],\n",
      "\n",
      "         [[-7.0845e-01, -3.7949e-01,  5.1411e-01,  ..., -5.4370e-02,\n",
      "            6.0689e-01,  2.5664e-01],\n",
      "          [-2.2920e-01, -3.1891e-02,  1.4094e+00,  ...,  2.8212e-01,\n",
      "            1.1978e+00,  9.4020e-01],\n",
      "          [-5.0053e-01, -6.8401e-01, -4.5881e-01,  ...,  9.4699e-01,\n",
      "           -6.4939e-01, -8.9217e-01],\n",
      "          ...,\n",
      "          [ 7.1196e-01, -6.9876e-01, -7.9330e-03,  ..., -3.2063e-01,\n",
      "            1.3299e+00,  5.3731e-01],\n",
      "          [ 6.3638e-01, -6.9038e-01, -1.0074e-01,  ..., -2.9419e-01,\n",
      "            9.7059e-01,  1.8956e-01],\n",
      "          [ 3.5520e-01, -1.0141e+00, -1.8210e-01,  ..., -2.2015e-01,\n",
      "            7.7066e-01,  9.6777e-03]],\n",
      "\n",
      "         [[-4.2823e-01, -8.0521e-01,  2.2570e-01,  ..., -5.8035e-01,\n",
      "            9.5534e-02,  1.4372e-01],\n",
      "          [-1.5382e-01, -8.6847e-01,  2.4079e-01,  ..., -3.4951e-02,\n",
      "            1.3803e-01,  2.2719e-01],\n",
      "          [ 6.7696e-01, -5.0891e-01,  3.6600e-01,  ..., -4.0958e-01,\n",
      "            3.5518e-01,  1.9883e-01],\n",
      "          ...,\n",
      "          [ 1.6394e-01,  1.1609e-01,  8.0030e-01,  ...,  2.2633e-02,\n",
      "            1.8287e-02,  3.5261e-01],\n",
      "          [ 5.8476e-01, -1.9390e-01,  8.3995e-01,  ...,  6.8678e-02,\n",
      "            4.0219e-02,  3.0055e-01],\n",
      "          [ 9.8272e-01, -2.5887e-01,  9.0029e-01,  ...,  1.3305e-01,\n",
      "            2.3385e-01,  3.7434e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.0922e-01, -2.4457e-01, -3.4727e-01,  ...,  1.1793e-02,\n",
      "           -6.7300e-01,  4.0283e-01],\n",
      "          [ 1.3658e+00,  9.0442e-02, -1.8090e-03,  ..., -2.2984e-01,\n",
      "           -1.1010e+00,  2.3977e-01],\n",
      "          [-3.2772e-01, -7.4253e-01, -6.6212e-01,  ..., -6.1553e-01,\n",
      "            5.2893e-01, -2.7849e-01],\n",
      "          ...,\n",
      "          [ 5.5186e-01,  4.3780e-01,  1.2022e-01,  ..., -1.7471e+00,\n",
      "           -1.3174e+00, -1.4102e+00],\n",
      "          [ 3.7191e-01,  2.8626e-02, -6.6170e-03,  ..., -1.9091e+00,\n",
      "           -1.0055e+00, -1.1992e+00],\n",
      "          [ 2.3122e-01, -1.8973e-01, -2.9924e-01,  ..., -2.0047e+00,\n",
      "           -7.0117e-01, -1.0963e+00]],\n",
      "\n",
      "         [[-1.3622e-01,  4.4667e-02,  3.9963e-01,  ...,  6.2070e-01,\n",
      "            9.9460e-02, -5.4372e-01],\n",
      "          [ 1.3695e-01, -2.3171e-01,  5.7073e-01,  ...,  4.9904e-01,\n",
      "            5.9053e-01,  1.4276e-01],\n",
      "          [-4.3429e-01, -1.3419e-01, -4.5610e-01,  ...,  7.9278e-01,\n",
      "           -5.6293e-02,  2.0831e-01],\n",
      "          ...,\n",
      "          [-8.3060e-01, -4.1984e-01,  5.8545e-01,  ..., -2.0543e-01,\n",
      "            1.3720e+00, -1.6959e+00],\n",
      "          [-7.9290e-01, -5.4702e-01,  6.5228e-01,  ..., -5.4666e-01,\n",
      "            1.0869e+00, -1.5869e+00],\n",
      "          [-5.4686e-01, -7.7204e-01,  6.2960e-01,  ..., -7.6948e-01,\n",
      "            1.0634e+00, -1.5827e+00]],\n",
      "\n",
      "         [[ 1.1750e+00,  3.1674e-01, -3.4225e-01,  ...,  1.4173e-01,\n",
      "            1.9281e-01,  2.6040e-01],\n",
      "          [ 9.6905e-01, -1.7408e-01, -1.1341e+00,  ..., -3.6863e-01,\n",
      "            2.4468e-01,  3.2629e-01],\n",
      "          [-7.0930e-01,  1.8192e-02, -1.8165e-01,  ...,  7.9401e-02,\n",
      "           -3.8041e-01,  7.7518e-01],\n",
      "          ...,\n",
      "          [ 1.1098e+00,  1.3346e+00, -2.1279e-01,  ...,  3.6846e-01,\n",
      "            1.0278e+00,  7.7966e-01],\n",
      "          [ 9.0392e-01,  1.5115e+00, -5.1153e-01,  ...,  4.7135e-01,\n",
      "            7.4598e-01,  6.6957e-01],\n",
      "          [ 9.4720e-01,  1.5145e+00, -6.7036e-01,  ...,  4.1289e-01,\n",
      "            4.8880e-01,  6.3417e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-0.2618, -0.1788, -0.0187,  ..., -0.6547, -0.2166,  0.3274],\n",
      "          [-0.0101, -0.1875,  0.0201,  ..., -0.2938, -0.1203,  0.6018],\n",
      "          [ 0.0822, -0.1531,  0.3726,  ...,  0.0899, -0.1192,  0.2539],\n",
      "          ...,\n",
      "          [ 0.3868, -0.4435, -0.1419,  ...,  0.4250, -0.2823,  0.0162],\n",
      "          [ 0.4619, -0.4061,  0.0324,  ...,  0.5417, -0.1890, -0.0030],\n",
      "          [ 0.4807, -0.4429,  0.0418,  ...,  0.6018, -0.1595, -0.1391]],\n",
      "\n",
      "         [[ 0.0595,  0.5588, -0.3854,  ...,  0.1113, -0.4634,  0.4258],\n",
      "          [ 0.8726,  0.0722, -0.5558,  ...,  0.5419, -0.2724,  0.7119],\n",
      "          [ 0.5780,  0.7797, -0.1450,  ..., -0.0914, -0.0419,  0.1542],\n",
      "          ...,\n",
      "          [ 0.3868,  0.6993,  0.4645,  ...,  0.4146,  0.0504, -0.0127],\n",
      "          [ 0.3712,  0.6121,  0.2394,  ...,  0.3587,  0.0372,  0.0897],\n",
      "          [ 0.3254,  0.5687,  0.2003,  ...,  0.2651,  0.0376,  0.0064]],\n",
      "\n",
      "         [[-0.2103,  0.8009, -0.7300,  ..., -0.1902, -0.2721, -0.4695],\n",
      "          [-0.4739,  1.2269, -0.6490,  ..., -0.9651, -0.5905,  0.0208],\n",
      "          [-0.1187, -0.0425, -0.2604,  ...,  0.3064, -0.2069,  0.5281],\n",
      "          ...,\n",
      "          [ 0.2753, -0.9525,  0.1943,  ...,  0.8355, -0.2417, -0.2833],\n",
      "          [ 0.1547, -0.9835, -0.1379,  ...,  0.9779, -0.2469, -0.1847],\n",
      "          [ 0.1607, -0.8677, -0.3979,  ...,  0.9820, -0.2725, -0.1880]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1034, -0.3994,  0.2347,  ...,  0.5693, -0.2869,  0.0642],\n",
      "          [-0.4942,  0.3563,  0.2248,  ...,  0.0087, -0.2092, -0.2374],\n",
      "          [-0.0782,  0.2651,  0.2847,  ...,  0.2255,  0.5944, -1.3682],\n",
      "          ...,\n",
      "          [-0.3664,  0.0490, -0.4832,  ...,  0.4347, -0.3760,  0.1030],\n",
      "          [-0.2444,  0.0260, -0.2968,  ...,  0.4469,  0.0443, -0.1394],\n",
      "          [-0.1982, -0.0028, -0.3470,  ...,  0.5185,  0.1771, -0.2698]],\n",
      "\n",
      "         [[ 0.4909, -0.5238,  1.1390,  ...,  0.8124, -0.4660,  0.1073],\n",
      "          [-0.5930, -0.8458,  0.1281,  ..., -0.1750, -0.9882,  0.0348],\n",
      "          [-0.0225, -0.2783,  0.2276,  ...,  0.1555,  0.2235, -0.4112],\n",
      "          ...,\n",
      "          [-0.4645, -0.1706, -0.5615,  ..., -0.9281, -0.2794, -0.1371],\n",
      "          [-0.2538, -0.2972, -0.6487,  ..., -0.4298, -0.2735, -0.2949],\n",
      "          [-0.1871, -0.4334, -0.5568,  ..., -0.3711, -0.2605, -0.4158]],\n",
      "\n",
      "         [[ 0.1921,  0.0783, -0.6618,  ...,  0.1667, -1.3435,  0.4753],\n",
      "          [ 0.1722,  0.0641,  0.0193,  ...,  0.4084, -0.5926,  0.9254],\n",
      "          [-0.1798, -0.8838, -1.0836,  ..., -0.3554, -0.7445,  0.7948],\n",
      "          ...,\n",
      "          [-0.7462, -0.0943, -0.1384,  ...,  0.0342,  0.3333, -0.5938],\n",
      "          [-0.8395, -0.0075, -0.0339,  ..., -0.0328,  0.6674, -0.4885],\n",
      "          [-0.8028, -0.0587,  0.1134,  ..., -0.0963,  0.6973, -0.4161]]]],\n",
      "       grad_fn=<CloneBackward0>)))\n"
     ]
    }
   ],
   "source": [
    "generator2_train_out = bert_train(input_ids=sentence_to_translate_bert.input_ids, attention_mask=sentence_to_translate_bert.attention_mask, decoder_input_ids=tgt_sentence_bert.input_ids , output_hidden_states=True, return_dict=True)\n",
    "# print(\"generator2_train_out shape \", generator2_train_out.shape)\n",
    "print(\"type of generator2_train_out\", type(generator2_train_out))\n",
    "print(\" generator2_train_keys() \", generator2_train_out.keys())\n",
    "# print(\"generator2_train_out to_tuple\", generator2_train_out.to_tuple())\n",
    "# dict_keys(['logits', 'past_key_values', 'decoder_hidden_states', 'encoder_last_hidden_state', 'encoder_hidden_states'])\n",
    "print(\"generator2_train_out logits shape \", generator2_train_out.logits.shape)\n",
    "print(\"generator2_train_out decoder_hidden_states shape \", generator2_train_out.decoder_hidden_states)\n",
    "print(\"generator2_train_out encoder_last_hidden_state shape \", generator2_train_out.encoder_last_hidden_state.shape)\n",
    "print(\"generator2_train_out encoder_hidden_states shape \", generator2_train_out.encoder_hidden_states)\n",
    "print(\"generator2_train_out past_key_values shape \", generator2_train_out.past_key_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [100], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m      2\u001b[0m fake_tgt_sentences_probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(generator2_train_out\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake_tgt_sentences_probs shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, fake_tgt_sentences_probs,\u001b[43mshape\u001b[49m)\n\u001b[1;32m      5\u001b[0m fake_tgt_sentences_probs \u001b[38;5;241m=\u001b[39m fake_tgt_sentences_probs\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m      6\u001b[0m                 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, fake_tgt_sentences_probs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m             )  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'shape' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "fake_tgt_sentences_probs = F.log_softmax(generator2_train_out.logits, dim=-1)\n",
    "print(\"fake_tgt_sentences_probs shape\", fake_tgt_sentences_probs.shape)\n",
    "\n",
    "fake_tgt_sentences_probs = fake_tgt_sentences_probs.view(\n",
    "                -1, fake_tgt_sentences_probs.size(-1)\n",
    "            )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, prediction = fake_tgt_sentences_probs.topk(1)\n",
    "# print(\"prediction shape \", prediction.shape\n",
    "prediction = prediction.squeeze(1)\n",
    "# print(\"prediction shape after squeeze \", prediction.shape)\n",
    "fake_tgt_sentences = torch.reshape(prediction, sentence_to_translate_bert.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    2,    0, 1960,    3,   34,    0, 7097,    0, 7097, 7097, 7097,\n",
       "            0,    0,    0, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_tgt_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_tgt_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_decoded_bert.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    2,    0, 1960,    3,   34,    0, 7097,    0, 7097, 7097, 7097,\n",
       "            0,    0,    0, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703,\n",
       "         8703, 8703, 8703, 8703, 8703, 8703, 8703, 8703]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_tgt_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3982, 10243,    49,     2,     5, 12207,    49,     2,   157,   740,\n",
       "           900,  3921,    51,    34,   357,  1754,  8941,    21,  1802,    49,\n",
       "         10669,     0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_decoded_bert.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected common dtype to be floating point, yet common dtype is Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [91], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you are using embeddings or directly computing a similarity\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfr_decoded_bert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_tgt_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected common dtype to be floating point, yet common dtype is Long"
     ]
    }
   ],
   "source": [
    "# Assuming you are using embeddings or directly computing a similarity\n",
    "loss = torch.nn.functional.cosine_similarity(fr_decoded_bert.input_ids, fake_tgt_sentences, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(59514, 512, padding_idx=59513)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=59514, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tgt_sentences_embeds = bert_train.model.shared(fake_tgt_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_decoded_bert_embeds = bert_train.model.shared(fr_decoded_bert.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.021239859983325005\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity \n",
    "cosine_sim = torch.nn.functional.cosine_similarity(fake_tgt_sentences_embeds, fr_decoded_bert_embeds, dim=-1)\n",
    "print(\"Cosine Similarity:\", cosine_sim.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor of target labels\n",
    "# Assuming 1 indicates the embeddings should be similar\n",
    "# target = torch.ones(cosine_sim.size(0), device=cosine_sim.device)\n",
    "# Example dimensions\n",
    "batch_size = fake_tgt_sentences_embeds.size(0)\n",
    "seq_length = fake_tgt_sentences_embeds.size(1)\n",
    "\n",
    "# Create a target tensor for batch\n",
    "target = torch.ones((batch_size, seq_length), dtype=torch.float, device=fake_tgt_sentences_embeds.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Embedding Loss for Batch: 0.9787601232528687\n"
     ]
    }
   ],
   "source": [
    "# Cosine embedding loss\n",
    "# Compute the cosine embedding loss with batch\n",
    "cosine_loss = torch.nn.functional.cosine_embedding_loss(\n",
    "    fake_tgt_sentences_embeds.view(-1, fake_tgt_sentences_embeds.size(2)),  # Reshape to [batch_size * seq_length, embedding_size]\n",
    "    fr_decoded_bert_embeds.view(-1, fr_decoded_bert_embeds.size(2)),  # Reshape similarly\n",
    "    target.view(-1),  # Flatten the target to match the reshaped embeddings\n",
    "    margin=0.5\n",
    ")\n",
    "print(\"Cosine Embedding Loss for Batch:\", cosine_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize embeddings to probability distributions\n",
    "prob_fairseq = torch.nn.functional.softmax(fr_decoded_bert_embeds, dim=-1)\n",
    "prob_marian = torch.nn.functional.softmax(fake_tgt_sentences_embeds, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence (Marian || Fairseq): 0.6088168025016785\n",
      "KL Divergence (Fairseq || Marian): 0.5906202793121338\n"
     ]
    }
   ],
   "source": [
    "# Compute KL divergence from Marian to Fairseq\n",
    "kl_divergence = torch.nn.functional.kl_div(torch.log(prob_marian), prob_fairseq, reduction='batchmean')\n",
    "print(\"KL Divergence (Marian || Fairseq):\", kl_divergence.item())\n",
    "\n",
    "# Compute KL divergence from Fairseq to Marian\n",
    "kl_divergence_reverse = torch.nn.functional.kl_div(torch.log(prob_fairseq), prob_marian, reduction='batchmean')\n",
    "print(\"KL Divergence (Fairseq || Marian):\", kl_divergence_reverse.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research References\n",
    "https://arxiv.org/pdf/2006.05525.pdf\n",
    "\n",
    "Here are a few papers where KL divergence and other divergence measures are used in the context of embeddings or model outputs, particularly for tasks like knowledge distillation or model comparison:\n",
    "\n",
    "\"Distilling the Knowledge in a Neural Network\" by Geoffrey Hinton et al. - This paper introduces the concept of knowledge distillation using KL divergence to transfer knowledge from a larger model to a smaller model.\n",
    "Link to paper https://arxiv.org/abs/1503.02531\n",
    "\n",
    "\"BERT and PALs: Projected Attention Layers for Enhanced Masked Language Modeling\" by Gunel et al. - Although not directly using KL divergence, this paper discusses the projection of embeddings and could be adapted for divergence-based comparisons.\n",
    "Link to paper https://arxiv.org/abs/1902.02671\n",
    "\n",
    "\"Improving Neural Language Models with a Continuous Cache\" by Grave et al. - Discusses using divergence measures to improve model performance.\n",
    "Link to paper https://arxiv.org/abs/1612.04426\n",
    "\n",
    "-------\n",
    "\n",
    "Hinton, G., Vinyals, O., & Dean, J. (2015). \"Distilling the Knowledge in a Neural Network\" - This foundational paper on knowledge distillation by Hinton et al. introduces the concept of distilling the knowledge from a large, cumbersome model into a smaller, more efficient one. While it primarily focuses on using softmax outputs, the ideas can be adapted to use cosine similarity for comparing embedding layers.\n",
    "\n",
    "Park, S. W., & Kim, S. (2019). \"Relational Knowledge Distillation\" - This paper introduces relational knowledge distillation, where relationships between data points are used for distillation. Cosine similarity is a potential metric for measuring these relationships in the embedding space.\n",
    "\n",
    "Passalis, N., & Tefas, A. (2018). \"Learning Deep Representations with Probabilistic Knowledge Transfer\" - This work discusses probabilistic knowledge transfer techniques that could conceptually include cosine similarity for embedding alignment between teacher and student networks.\n",
    "\n",
    "Chen, H., Wang, Y., Xu, C., Zhang, Z., & Cui, J. (2020). \"Cross-layer Distillation with Semantic Calibration\" - This paper uses cross-layer distillation where embedding layers from different depths are aligned. Techniques like cosine similarity can be used to compare these embeddings more effectively.\n",
    "\n",
    "=======================\n",
    "\n",
    "\"Distilling the Knowledge in a Neural Network\" by Geoffrey Hinton, Oriol Vinyals, and Jeff Dean (2015) - This seminal paper introduces the concept of knowledge distillation using temperature-scaled softmax probabilities. The use of KL divergence in this paper is to compare the softened outputs of the teacher and student models.\n",
    "\n",
    "\"Improved Knowledge Distillation via Teacher Assistant\" by Seyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Hassan Ghasemzadeh (2020) - This paper discusses an intermediate 'teacher assistant' that helps bridge the gap between a very large teacher and a much smaller student. They use KL divergence to measure the difference between the teacher's and the assistant's outputs, as well as between the assistant's and the student's outputs.\n",
    "\n",
    "\"Sequence-Level Knowledge Distillation\" by Yoon Kim and Alexander M. Rush (2016) - While this paper focuses on sequence-to-sequence models, it discusses the use of KL divergence for sequence-level training by comparing the output distributions over sequences generated by teacher and student models\n",
    "\n",
    "================ Below papers provide some references on how the input_ids from Fairseq and Helsinki models are converted to a shared space embeddings and then calculated the KL divergence . It is a common approach in Domain Adaptation and transfer learning approach. \n",
    "\n",
    "\"A Survey on Transfer Learning\" by Sinno Jialin Pan and Qiang Yang (2010) - This paper provides a comprehensive survey of transfer learning methods, introducing basic concepts, discussing various approaches, and summarizing applications. It can offer foundational knowledge that might be adapted for measuring divergences in transformed spaces.\n",
    "\n",
    "\"Domain-Adaptive Discriminative One-Shot Learning of Gestures\" by E. Kodirov, T. Xiang, Z. Fu, and S. Gong in ECCV (2016) - While focused on gesture recognition, this paper discusses using domain adaptation techniques to align different domains in a shared space, which can be related to aligning model outputs before applying divergence measures like KL divergence.\n",
    "\n",
    "\"Deep Coral: Correlation Alignment for Deep Domain Adaptation\" by Baochen Sun and Kate Saenko in ECCV (2016) - This paper presents a method for deep domain adaptation that aligns the second-order statistics of source and target distributions, which can be adapted to align the outputs of different models in a shared embedding space.\n",
    "\n",
    "\"Unsupervised Domain Adaptation by Backpropagation\" by Yaroslav Ganin and Victor Lempitsky at ICML (2015) - This work introduces a gradient reversal layer that promotes domain invariance in deep neural networks, which might inspire techniques for embedding or output alignment in your scenario.\n",
    "\n",
    "\"Domain-Adversarial Training of Neural Networks\" by Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky, JMLR (2016) - This paper expands on domain adaptation in neural networks, using adversarial training to learn features that are invariant across domains, which could be analogous to learning model outputs that are invariant to specific modeling choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################## A little detour for KD ENDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tokens': tensor([ 9804,   391,     4, 11548,     4,   471, 13772,   184,    33, 24715,\n",
       "             17, 35429,     2]),\n",
       "  'score': tensor(-0.5770),\n",
       "  'attention': tensor([[4.9452e-01, 2.5984e-02, 1.1782e-01, 1.0228e-01, 2.8326e-02, 8.5267e-02,\n",
       "           8.3771e-03, 5.0618e-03, 3.4623e-03, 8.7738e-03, 8.2156e-04, 5.5009e-04,\n",
       "           3.4776e-02],\n",
       "          [2.1300e-01, 1.4918e-01, 4.2657e-01, 6.8923e-02, 6.1092e-02, 8.3069e-02,\n",
       "           1.0876e-02, 6.8625e-03, 4.8859e-03, 3.6718e-03, 7.6182e-03, 2.4095e-03,\n",
       "           4.1339e-02],\n",
       "          [6.7006e-03, 1.3594e-02, 7.3953e-02, 2.1945e-03, 2.7774e-03, 3.4765e-03,\n",
       "           2.0275e-03, 5.9349e-04, 4.6940e-04, 3.2411e-03, 1.1718e-02, 4.8259e-03,\n",
       "           2.6926e-03],\n",
       "          [1.1468e-02, 5.9789e-03, 6.1731e-02, 1.9828e-03, 1.6616e-03, 1.5582e-03,\n",
       "           1.2446e-03, 1.2807e-03, 8.3491e-04, 1.0579e-03, 4.3197e-03, 2.3071e-03,\n",
       "           2.3031e-03],\n",
       "          [1.5785e-02, 5.5890e-03, 1.4403e-02, 8.2340e-03, 7.9631e-03, 6.1776e-03,\n",
       "           5.5518e-03, 4.1026e-03, 1.4576e-02, 1.5530e-03, 4.5018e-03, 3.2129e-03,\n",
       "           6.7994e-03],\n",
       "          [1.7507e-02, 8.9580e-03, 6.0052e-03, 4.8391e-02, 1.0376e-01, 6.9804e-02,\n",
       "           1.5341e-02, 2.0594e-02, 1.7130e-02, 3.8096e-03, 2.9733e-03, 1.0807e-03,\n",
       "           1.2778e-02],\n",
       "          [7.1104e-03, 1.9473e-03, 1.2723e-03, 2.4874e-02, 3.3783e-02, 2.9038e-02,\n",
       "           3.2005e-02, 6.2606e-03, 4.3090e-03, 1.3220e-03, 9.7481e-04, 2.4712e-04,\n",
       "           4.1332e-03],\n",
       "          [6.8949e-03, 1.9310e-03, 1.0688e-03, 3.9204e-02, 8.0575e-02, 7.0729e-02,\n",
       "           1.2969e-01, 1.9317e-02, 4.7092e-03, 1.8119e-03, 1.0530e-03, 3.3923e-04,\n",
       "           3.8464e-03],\n",
       "          [4.3452e-03, 3.9618e-04, 6.1907e-04, 1.3932e-02, 4.1178e-02, 3.7711e-02,\n",
       "           9.6978e-02, 4.7279e-02, 9.7814e-03, 4.6797e-03, 2.0221e-03, 9.3256e-04,\n",
       "           7.3301e-03],\n",
       "          [2.5610e-03, 3.0437e-04, 4.9595e-04, 7.8539e-03, 2.1054e-02, 1.8220e-02,\n",
       "           7.7381e-03, 2.2172e-02, 2.7235e-02, 8.1564e-03, 3.2817e-03, 1.1809e-03,\n",
       "           6.4816e-03],\n",
       "          [1.2490e-02, 5.8952e-03, 2.5250e-03, 4.1282e-02, 1.6353e-01, 1.5761e-01,\n",
       "           2.7632e-01, 3.8985e-01, 1.5627e-01, 4.8658e-01, 3.8582e-02, 1.5673e-02,\n",
       "           2.8120e-02],\n",
       "          [6.2965e-03, 2.4159e-03, 3.1994e-03, 9.8439e-03, 4.5678e-02, 3.6751e-02,\n",
       "           1.0965e-01, 1.9164e-01, 7.4367e-02, 1.8930e-01, 2.1785e-01, 1.3573e-01,\n",
       "           1.6520e-02],\n",
       "          [5.2392e-02, 2.6608e-02, 5.6384e-03, 4.1731e-01, 6.5586e-02, 1.1789e-01,\n",
       "           2.4526e-02, 5.4493e-02, 1.0737e-01, 2.2131e-02, 2.2896e-02, 3.5656e-03,\n",
       "           1.3558e-01],\n",
       "          [1.4893e-01, 7.5122e-01, 2.8470e-01, 2.1369e-01, 3.4304e-01, 2.8270e-01,\n",
       "           2.7968e-01, 2.3049e-01, 5.7460e-01, 2.6391e-01, 6.8139e-01, 8.2794e-01,\n",
       "           6.9730e-01]]),\n",
       "  'alignment': tensor([]),\n",
       "  'positional_scores': tensor([-0.9569, -0.1617, -1.3670, -1.3469, -0.7934, -0.2061, -0.3429, -0.8463,\n",
       "          -0.1570, -0.1277, -0.1216, -0.0506, -1.0229])},\n",
       " {'tokens': tensor([ 9804,   391,    16,   173,     4, 11548,     4,   471, 13772,   184,\n",
       "             33, 24715,    17, 35429,     7,     2]),\n",
       "  'score': tensor(-0.5930),\n",
       "  'attention': tensor([[4.9452e-01, 2.5984e-02, 1.1782e-01, 2.2427e-01, 6.1300e-02, 3.7056e-02,\n",
       "           9.7818e-03, 2.1638e-02, 4.6338e-03, 3.4064e-03, 2.8894e-03, 7.5879e-03,\n",
       "           7.2845e-04, 4.7471e-04, 1.7726e-02, 3.2793e-02],\n",
       "          [2.1300e-01, 1.4918e-01, 4.2657e-01, 2.6486e-01, 1.3356e-01, 2.0533e-02,\n",
       "           1.6008e-02, 1.7002e-02, 5.7308e-03, 4.7128e-03, 4.0647e-03, 3.0632e-03,\n",
       "           6.6408e-03, 2.0188e-03, 2.0701e-02, 2.1882e-02],\n",
       "          [6.7006e-03, 1.3594e-02, 7.3953e-02, 2.1870e-02, 7.1546e-03, 8.4012e-04,\n",
       "           1.0892e-03, 1.1244e-03, 1.1165e-03, 4.0900e-04, 4.0220e-04, 2.6992e-03,\n",
       "           1.0871e-02, 4.2000e-03, 1.4843e-03, 2.3753e-03],\n",
       "          [1.1468e-02, 5.9789e-03, 6.1731e-02, 4.1392e-02, 7.2530e-03, 5.8079e-04,\n",
       "           5.7874e-04, 4.3766e-04, 6.4070e-04, 7.2260e-04, 6.6873e-04, 8.2884e-04,\n",
       "           3.7195e-03, 1.9574e-03, 1.2795e-03, 1.4458e-03],\n",
       "          [1.5785e-02, 5.5890e-03, 1.4403e-02, 1.7385e-02, 2.6247e-02, 5.3051e-03,\n",
       "           4.9850e-03, 4.1419e-03, 3.9486e-03, 3.2141e-03, 1.4274e-02, 1.4337e-03,\n",
       "           4.0708e-03, 3.2441e-03, 5.0423e-03, 1.0905e-02],\n",
       "          [1.7507e-02, 8.9580e-03, 6.0052e-03, 9.1000e-03, 6.7429e-02, 6.6657e-02,\n",
       "           9.6376e-02, 8.0693e-02, 1.4786e-02, 2.0008e-02, 1.8349e-02, 3.8394e-03,\n",
       "           2.7070e-03, 1.0727e-03, 1.2737e-02, 1.5511e-02],\n",
       "          [7.1104e-03, 1.9473e-03, 1.2723e-03, 9.8049e-04, 1.4408e-02, 3.3217e-02,\n",
       "           3.6739e-02, 3.4006e-02, 3.1072e-02, 6.5388e-03, 4.3547e-03, 1.3773e-03,\n",
       "           9.3635e-04, 2.4880e-04, 4.1701e-03, 4.6471e-03],\n",
       "          [6.8949e-03, 1.9310e-03, 1.0688e-03, 1.3952e-03, 1.0235e-02, 5.4362e-02,\n",
       "           9.9740e-02, 8.8957e-02, 1.3102e-01, 2.1106e-02, 4.8345e-03, 1.8795e-03,\n",
       "           1.0516e-03, 3.4511e-04, 4.1233e-03, 4.5326e-03],\n",
       "          [4.3452e-03, 3.9618e-04, 6.1907e-04, 8.3980e-04, 5.8609e-03, 2.1920e-02,\n",
       "           4.9901e-02, 5.2771e-02, 9.8675e-02, 4.8289e-02, 9.9549e-03, 4.8066e-03,\n",
       "           1.9919e-03, 9.1963e-04, 8.3761e-03, 2.9821e-03],\n",
       "          [2.5610e-03, 3.0437e-04, 4.9595e-04, 2.3865e-03, 2.4017e-03, 1.1626e-02,\n",
       "           2.6873e-02, 2.4521e-02, 8.8594e-03, 2.2016e-02, 2.7488e-02, 8.8608e-03,\n",
       "           3.1331e-03, 1.1509e-03, 8.1033e-03, 3.1262e-03],\n",
       "          [1.2490e-02, 5.8952e-03, 2.5250e-03, 3.2356e-03, 1.2262e-02, 7.4054e-02,\n",
       "           1.9337e-01, 1.9971e-01, 2.8362e-01, 3.8402e-01, 1.6022e-01, 4.8539e-01,\n",
       "           3.9346e-02, 1.5509e-02, 3.3212e-02, 9.6052e-03],\n",
       "          [6.2965e-03, 2.4159e-03, 3.1994e-03, 2.5585e-03, 3.6606e-03, 1.4835e-02,\n",
       "           5.9049e-02, 4.7501e-02, 1.1893e-01, 1.9298e-01, 7.6351e-02, 1.8848e-01,\n",
       "           2.2724e-01, 1.3566e-01, 1.9629e-02, 4.2680e-03],\n",
       "          [5.2392e-02, 2.6608e-02, 5.6384e-03, 3.7852e-03, 8.4266e-02, 4.4486e-01,\n",
       "           7.0072e-02, 1.1362e-01, 2.7599e-02, 6.0244e-02, 1.0866e-01, 2.4138e-02,\n",
       "           2.4043e-02, 3.5086e-03, 1.6752e-01, 3.9772e-02],\n",
       "          [1.4893e-01, 7.5122e-01, 2.8470e-01, 4.0594e-01, 5.6396e-01, 2.1416e-01,\n",
       "           3.3543e-01, 3.1388e-01, 2.6936e-01, 2.3234e-01, 5.6749e-01, 2.6561e-01,\n",
       "           6.7352e-01, 8.2969e-01, 6.9589e-01, 8.4615e-01]]),\n",
       "  'alignment': tensor([]),\n",
       "  'positional_scores': tensor([-0.9569, -0.1617, -2.1894, -0.2832, -0.5154, -1.5030, -1.0885, -0.1961,\n",
       "          -0.3272, -0.8740, -0.1678, -0.1266, -0.1252, -0.0528, -0.8152, -0.1055])},\n",
       " {'tokens': tensor([  255,  7243, 33597,   342,     4, 11548,   471, 13772, 24715,    17,\n",
       "           2511,  6062,     2]),\n",
       "  'score': tensor(-0.7311),\n",
       "  'attention': tensor([[4.9452e-01, 1.3585e-01, 4.4123e-02, 1.2497e-02, 2.9693e-02, 1.6742e-02,\n",
       "           6.4132e-03, 3.2266e-03, 2.4082e-03, 6.4964e-04, 5.5629e-04, 2.3096e-03,\n",
       "           1.9518e-02],\n",
       "          [2.1300e-01, 1.0203e-01, 3.4556e-01, 1.8764e-02, 3.6897e-02, 7.9515e-03,\n",
       "           8.3280e-03, 3.2478e-03, 2.6372e-03, 4.4293e-03, 1.8642e-03, 2.1742e-03,\n",
       "           1.6817e-02],\n",
       "          [6.7006e-03, 1.4069e-02, 4.7352e-02, 9.6709e-03, 3.3410e-03, 4.1474e-04,\n",
       "           7.5571e-04, 8.4526e-04, 2.8618e-04, 8.3541e-03, 3.1534e-03, 4.0001e-03,\n",
       "           1.8327e-03],\n",
       "          [1.1468e-02, 9.0962e-03, 4.7761e-02, 3.6612e-02, 7.4676e-03, 2.5475e-04,\n",
       "           3.9529e-04, 4.4294e-04, 5.3031e-04, 1.9742e-03, 1.4852e-03, 4.3964e-03,\n",
       "           2.0791e-03],\n",
       "          [1.5785e-02, 1.1480e-02, 1.1597e-02, 4.6172e-02, 3.1943e-02, 3.4167e-03,\n",
       "           4.0220e-03, 3.1314e-03, 2.5396e-03, 3.8614e-03, 3.0320e-03, 3.0717e-03,\n",
       "           7.1614e-03],\n",
       "          [1.7507e-02, 4.9424e-03, 3.4959e-03, 4.9837e-03, 9.4726e-02, 7.7688e-02,\n",
       "           1.0192e-01, 1.6006e-02, 1.9243e-02, 2.9924e-03, 1.7322e-03, 6.0436e-03,\n",
       "           1.5599e-02],\n",
       "          [7.1104e-03, 2.1022e-03, 1.2851e-03, 1.7082e-03, 1.3429e-02, 3.2266e-02,\n",
       "           3.2165e-02, 3.1169e-02, 5.7192e-03, 6.8131e-04, 2.8157e-04, 1.1455e-03,\n",
       "           5.4016e-03],\n",
       "          [6.8949e-03, 3.2747e-03, 2.2398e-03, 8.8357e-04, 9.9391e-03, 6.3334e-02,\n",
       "           8.6630e-02, 1.4277e-01, 2.0394e-02, 1.0376e-03, 4.7074e-04, 1.6685e-03,\n",
       "           4.9515e-03],\n",
       "          [4.3452e-03, 8.7168e-04, 2.1475e-03, 1.0176e-03, 8.2549e-03, 3.1301e-02,\n",
       "           5.0311e-02, 9.9352e-02, 4.5420e-02, 1.9549e-03, 1.3160e-03, 2.3691e-03,\n",
       "           8.2044e-03],\n",
       "          [2.5610e-03, 1.9285e-03, 5.5399e-04, 1.1661e-03, 2.9941e-03, 1.6388e-02,\n",
       "           2.8564e-02, 9.3760e-03, 2.4587e-02, 2.4072e-03, 1.2974e-03, 2.3617e-03,\n",
       "           1.0838e-02],\n",
       "          [1.2490e-02, 1.9890e-03, 6.9380e-03, 1.1110e-03, 8.3135e-03, 1.5091e-01,\n",
       "           2.1451e-01, 2.9324e-01, 4.0789e-01, 2.5622e-02, 8.9431e-03, 7.2472e-03,\n",
       "           1.9608e-02],\n",
       "          [6.2965e-03, 1.3842e-03, 1.6471e-02, 3.5582e-03, 4.5438e-03, 3.0249e-02,\n",
       "           6.6143e-02, 1.2819e-01, 2.0219e-01, 1.4877e-01, 9.0957e-02, 3.7003e-02,\n",
       "           1.4134e-02],\n",
       "          [5.2392e-02, 3.5404e-03, 1.4162e-02, 5.5330e-03, 1.1304e-01, 3.5947e-01,\n",
       "           4.7788e-02, 2.1750e-02, 4.1887e-02, 1.7978e-02, 5.3857e-03, 1.4980e-02,\n",
       "           6.0272e-02],\n",
       "          [1.4893e-01, 7.0745e-01, 4.5632e-01, 8.5632e-01, 6.3542e-01, 2.0962e-01,\n",
       "           3.5205e-01, 2.4726e-01, 2.2427e-01, 7.7928e-01, 8.7953e-01, 9.1123e-01,\n",
       "           8.1358e-01]]),\n",
       "  'alignment': tensor([]),\n",
       "  'positional_scores': tensor([-2.5025, -0.0329, -1.3654, -0.0914, -0.1681, -1.8778, -0.7552, -0.2605,\n",
       "          -1.3514, -0.1690, -0.2029, -0.0859, -0.6414])},\n",
       " {'tokens': tensor([  330,  9599,     4,   471, 13772,   613,  1766,    16,  2100, 24715,\n",
       "             17,  2511,    30, 11548,     7,     2]),\n",
       "  'score': tensor(-1.4728),\n",
       "  'attention': tensor([[4.9452e-01, 3.7554e-01, 2.9943e-01, 2.1153e-01, 3.7620e-02, 1.3655e-02,\n",
       "           1.0591e-02, 9.2783e-03, 1.1494e-02, 2.9785e-02, 1.2891e-03, 5.1925e-04,\n",
       "           2.6793e-03, 1.6594e-02, 5.3180e-02, 6.9167e-02],\n",
       "          [2.1300e-01, 2.0713e-01, 1.3619e-01, 1.0523e-01, 3.6103e-02, 1.3860e-02,\n",
       "           1.2281e-02, 9.4122e-03, 8.9538e-03, 6.8628e-03, 1.0576e-02, 2.4426e-03,\n",
       "           2.7903e-03, 1.0862e-02, 7.2372e-02, 4.4577e-02],\n",
       "          [6.7006e-03, 1.1097e-02, 4.6503e-03, 3.3071e-03, 4.1161e-03, 6.8116e-04,\n",
       "           2.5890e-03, 2.5849e-04, 6.2511e-04, 2.5306e-03, 1.6034e-02, 5.9949e-03,\n",
       "           3.8503e-03, 4.3977e-04, 4.4042e-03, 4.9723e-03],\n",
       "          [1.1468e-02, 1.3790e-02, 1.6560e-02, 3.7970e-03, 4.1354e-03, 2.2762e-03,\n",
       "           1.2842e-03, 7.5745e-04, 1.0691e-03, 1.3514e-03, 4.8707e-03, 2.1308e-03,\n",
       "           5.6880e-03, 5.8239e-04, 3.8652e-03, 3.6958e-03],\n",
       "          [1.5785e-02, 1.0376e-02, 2.0598e-02, 1.4755e-02, 1.4317e-02, 8.9064e-03,\n",
       "           7.5483e-03, 5.9920e-03, 5.1063e-03, 2.9464e-03, 5.1026e-03, 2.9868e-03,\n",
       "           4.6222e-03, 4.4139e-03, 1.1597e-02, 1.4984e-02],\n",
       "          [1.7507e-02, 9.5585e-03, 6.1962e-02, 7.6546e-02, 2.5511e-02, 2.6947e-02,\n",
       "           1.3270e-02, 1.1245e-02, 2.6872e-02, 7.1430e-03, 3.0213e-03, 9.7909e-04,\n",
       "           5.7564e-03, 1.3030e-02, 1.1898e-02, 1.4489e-02],\n",
       "          [7.1104e-03, 1.4554e-03, 1.0230e-02, 3.9487e-02, 4.5566e-02, 8.7089e-03,\n",
       "           1.3913e-02, 4.0823e-03, 4.7661e-03, 5.3995e-03, 1.2087e-03, 2.1235e-04,\n",
       "           1.1483e-03, 5.3801e-03, 5.1523e-03, 4.4333e-03],\n",
       "          [6.8949e-03, 1.6040e-03, 8.7721e-03, 6.0788e-02, 1.2222e-01, 2.4009e-02,\n",
       "           1.7281e-02, 4.1051e-03, 9.5227e-03, 6.7331e-03, 1.1666e-03, 2.2215e-04,\n",
       "           7.8600e-04, 5.9918e-03, 4.1103e-03, 3.8829e-03],\n",
       "          [4.3452e-03, 1.3551e-03, 4.3735e-03, 3.0054e-02, 8.8164e-02, 4.6668e-02,\n",
       "           8.2891e-02, 1.0335e-02, 1.8871e-02, 1.3138e-02, 2.4523e-03, 7.1571e-04,\n",
       "           1.2279e-03, 9.2084e-03, 8.0291e-03, 2.5238e-03],\n",
       "          [2.5610e-03, 1.9818e-03, 4.5965e-03, 1.4230e-02, 6.8634e-03, 1.9224e-02,\n",
       "           1.6402e-02, 2.4714e-02, 5.4510e-02, 1.5025e-02, 3.5624e-03, 1.0336e-03,\n",
       "           1.7147e-03, 7.2362e-03, 5.2394e-03, 2.3470e-03],\n",
       "          [1.2490e-02, 5.8837e-03, 1.0906e-02, 6.3146e-02, 2.3591e-01, 3.7683e-01,\n",
       "           1.5395e-01, 1.4485e-01, 1.9095e-01, 4.1684e-01, 5.1315e-02, 1.0143e-02,\n",
       "           5.2155e-03, 4.3009e-02, 1.9825e-02, 8.7595e-03],\n",
       "          [6.2965e-03, 6.3901e-03, 7.5919e-03, 2.0596e-02, 8.4834e-02, 1.6582e-01,\n",
       "           7.0561e-02, 7.0120e-02, 1.4321e-01, 2.3240e-01, 2.2625e-01, 1.1795e-01,\n",
       "           2.9042e-02, 2.3409e-02, 1.0335e-02, 4.1289e-03],\n",
       "          [5.2392e-02, 6.9642e-03, 6.7493e-02, 1.3669e-01, 3.8218e-02, 1.1355e-01,\n",
       "           2.5946e-02, 3.7382e-01, 6.9286e-02, 1.5444e-02, 2.2190e-02, 2.7216e-03,\n",
       "           1.2591e-02, 5.1361e-01, 3.3335e-02, 2.5348e-02],\n",
       "          [1.4893e-01, 3.4687e-01, 3.4665e-01, 2.1985e-01, 2.5642e-01, 1.7886e-01,\n",
       "           5.7149e-01, 3.3104e-01, 4.5476e-01, 2.4440e-01, 6.5096e-01, 8.5195e-01,\n",
       "           9.2289e-01, 3.4623e-01, 7.5666e-01, 7.9669e-01]]),\n",
       "  'alignment': tensor([]),\n",
       "  'positional_scores': tensor([-5.1777, -3.7706, -0.2636, -0.4439, -0.6683, -2.7314, -0.2357, -0.3727,\n",
       "          -3.5801, -0.2437, -0.1322, -4.0542, -0.3719, -0.7036, -0.7097, -0.1054])},\n",
       " {'tokens': tensor([15671,  9255,     4,   471,  3740,   109, 13772, 19481,    11, 10680,\n",
       "           9255,     7,     2]),\n",
       "  'score': tensor(-2.1489),\n",
       "  'attention': tensor([[4.9452e-01, 5.6259e-02, 1.9477e-02, 1.9461e-02, 7.3392e-03, 6.6423e-03,\n",
       "           4.6061e-03, 1.0623e-02, 1.6758e-02, 1.7165e-02, 1.4411e-02, 1.6709e-02,\n",
       "           4.6999e-02],\n",
       "          [2.1300e-01, 3.5108e-01, 2.7485e-02, 1.1486e-02, 8.1609e-03, 3.8599e-03,\n",
       "           4.1037e-03, 6.4435e-03, 4.2753e-02, 1.7727e-02, 4.4695e-02, 2.6083e-02,\n",
       "           2.8783e-02],\n",
       "          [6.7006e-03, 5.5886e-02, 2.5912e-03, 5.1184e-04, 1.1061e-03, 1.3246e-03,\n",
       "           9.0015e-04, 2.2280e-04, 1.7765e-03, 1.3839e-03, 2.4601e-03, 1.4648e-03,\n",
       "           3.0954e-03],\n",
       "          [1.1468e-02, 4.1106e-02, 3.6995e-03, 2.9827e-04, 8.1164e-04, 1.0841e-03,\n",
       "           7.3864e-04, 3.8588e-04, 1.2826e-03, 9.4682e-04, 1.6214e-03, 1.0989e-03,\n",
       "           1.5759e-03],\n",
       "          [1.5785e-02, 1.1369e-02, 2.0424e-02, 4.1021e-03, 5.4424e-03, 1.1460e-02,\n",
       "           9.4493e-03, 4.3151e-03, 8.7175e-03, 9.6031e-03, 8.5951e-03, 1.1439e-02,\n",
       "           1.2720e-02],\n",
       "          [1.7507e-02, 4.2045e-03, 1.0837e-01, 7.4446e-02, 2.1161e-02, 1.9225e-02,\n",
       "           1.6854e-02, 5.6337e-03, 1.7064e-02, 1.4446e-02, 2.0542e-02, 2.9354e-02,\n",
       "           2.6347e-02],\n",
       "          [7.1104e-03, 2.0348e-03, 1.6628e-02, 3.2105e-02, 4.0238e-02, 2.9595e-02,\n",
       "           2.4080e-02, 4.9363e-03, 9.3932e-03, 8.3641e-03, 1.5271e-02, 1.1353e-02,\n",
       "           9.4145e-03],\n",
       "          [6.8949e-03, 3.6693e-03, 1.5626e-02, 6.5765e-02, 1.4696e-01, 1.1128e-01,\n",
       "           5.8376e-02, 6.2734e-03, 9.4177e-03, 1.8711e-02, 1.0895e-02, 9.1160e-03,\n",
       "           1.0397e-02],\n",
       "          [4.3452e-03, 2.9986e-03, 7.4859e-03, 2.4783e-02, 9.3963e-02, 1.1714e-01,\n",
       "           7.7841e-02, 1.1079e-02, 1.5946e-02, 3.7541e-03, 1.9271e-02, 2.2282e-02,\n",
       "           8.7548e-03],\n",
       "          [2.5610e-03, 9.6910e-04, 2.8692e-03, 1.3800e-02, 7.4511e-03, 1.0800e-02,\n",
       "           1.0511e-02, 1.4958e-02, 1.5132e-02, 1.1812e-02, 1.3530e-02, 1.5892e-02,\n",
       "           7.8233e-03],\n",
       "          [1.2490e-02, 1.3569e-02, 8.6237e-03, 7.9213e-02, 2.5827e-01, 3.7448e-02,\n",
       "           5.2619e-02, 1.4040e-01, 8.9385e-02, 2.7077e-02, 5.3075e-02, 5.7392e-02,\n",
       "           3.7110e-02],\n",
       "          [6.2965e-03, 1.7730e-02, 4.3749e-03, 1.6377e-02, 9.4457e-02, 2.3992e-02,\n",
       "           2.0904e-02, 4.5783e-02, 3.9150e-02, 9.5804e-03, 2.5236e-02, 2.3780e-02,\n",
       "           1.2867e-02],\n",
       "          [5.2392e-02, 2.0911e-02, 7.7788e-02, 4.0167e-01, 4.8674e-02, 1.7678e-02,\n",
       "           3.8139e-02, 4.7801e-01, 1.9201e-01, 8.7281e-02, 8.2378e-02, 6.6214e-02,\n",
       "           6.2403e-02],\n",
       "          [1.4893e-01, 4.1821e-01, 6.8455e-01, 2.5598e-01, 2.6597e-01, 6.0847e-01,\n",
       "           6.8088e-01, 2.7094e-01, 5.4121e-01, 7.7215e-01, 6.8802e-01, 7.0782e-01,\n",
       "           7.3171e-01]]),\n",
       "  'alignment': tensor([]),\n",
       "  'positional_scores': tensor([-1.6289, -0.5985, -0.1416, -0.3728, -4.0680, -2.9950, -3.8845, -6.1824,\n",
       "          -3.1776, -1.8890, -1.7141, -1.1746, -0.1090])}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_sentences_using_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data import Dictionary\n",
    "\n",
    "# Specify the path to your dictionary file\n",
    "dict_path = '/home/paperspace/google_drive_v1/Research_Thesis/2024/Adversarial_NMT_th/pretrained_models/wmt14.en-fr.joined-dict.transformer/dict.fr.txt'\n",
    "\n",
    "# Load the dictionary\n",
    "dictionary = Dictionary.load(dict_path)\n",
    "\n",
    "# Now you can access various attributes of the dictionary\n",
    "pad_idx = dictionary.pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/paperspace/.cache/torch/hub/pytorch_fairseq_main\n",
      "2024-04-16 18:44:26 | INFO | fairseq.file_utils | loading archive file https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2 from cache at /home/paperspace/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/fairseq/checkpoint_utils.py:425: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/fairseq/models/fairseq_model.py:267: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "2024-04-16 18:44:30 | INFO | fairseq.tasks.translation | [en] dictionary: 44512 types\n",
      "2024-04-16 18:44:30 | INFO | fairseq.tasks.translation | [fr] dictionary: 44512 types\n",
      "2024-04-16 18:44:33 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair0253:58342', 'distributed_port': 58342, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 5120, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5120, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0007], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=10, log_format='json', seed=2, data='/home/paperspace/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', source_lang='en', target_lang='fr', max_source_positions=1024, max_target_positions=1024, skip_invalid_size_inputs_valid_test=False, max_tokens=5120, max_sentences=None, train_subset='train', valid_subset='valid', max_sentences_valid=None, sample_without_replacement=0, distributed_world_size=128, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://learnfair0253:58342', distributed_port=58342, device_id=0, arch='transformer_vaswani_wmt_en_de_big', criterion='label_smoothed_cross_entropy', max_epoch=0, max_update=80000, clip_norm=0.0, sentence_avg=False, update_freq=[1.0], fp16=True, optimizer='adam', lr=[0.0007], momentum=0.99, weight_decay=0.0, lr_scheduler='inverse_sqrt', lr_shrink=0.1, save_dir='/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', restore_file='checkpoint_last.pt', save_interval=1, no_save=False, no_epoch_checkpoints=False, validate_interval=1, dropout=0.1, attention_dropout=0.0, relu_dropout=0.0, encoder_normalize_before=False, encoder_learned_pos=False, decoder_learned_pos=False, decoder_normalize_before=False, share_decoder_input_output_embed=True, share_all_embeddings=True, label_smoothing=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, warmup_updates=4000, warmup_init_lr=1e-07, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_attention_heads=16, tokenizer='moses', bpe='subword_nmt', bpe_codes='/home/paperspace/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', task='translation', stop_min_lr=1e-09, _name='transformer_vaswani_wmt_en_de_big', encoder_embed_path=None, decoder_embed_path=None, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0), 'task': {'_name': 'translation', 'data': '/home/paperspace/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0007]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0007]}, 'scoring': None, 'bpe': {'_name': 'subword_nmt', 'bpe_codes': '/home/paperspace/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', 'bpe_separator': '@@'}, 'tokenizer': {'_name': 'moses', 'source_lang': 'en', 'target_lang': 'fr', 'moses_no_dash_splits': False, 'moses_no_escape': False}, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [53], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# If you have a simple forward method available\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 29\u001b[0m     logits \u001b[38;5;241m=\u001b[39m generator_pt\u001b[38;5;241m.\u001b[39mmodels[\u001b[38;5;241m0\u001b[39m](\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnet_input\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnet_input\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_lengths\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     30\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# assuming the logits are the first return value\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Assuming 'en2fr' is your FairSeq model loaded and ready\n",
    "import torch\n",
    "from fairseq.data.data_utils import collate_tokens\n",
    "generator_pt_hub = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n",
    "\n",
    "# Prepare your input text\n",
    "text = \"Hello world!\"\n",
    "tokens = generator_pt_hub.tokenize(text)\n",
    "bpe_tokens = generator_pt_hub.apply_bpe(tokens)\n",
    "binarized_tokens = generator_pt_hub.binarize(bpe_tokens)\n",
    "\n",
    "# Assuming generator_pt is your GeneratorHubInterface loaded model\n",
    "# if hasattr(generator_pt.models[0], 'dictionary'):\n",
    "#     dictionary = generator_pt.models[0].dictionary\n",
    "# else:\n",
    "#     raise Exception(\"Dictionary not found in the model\")\n",
    "\n",
    "# Use this dictionary to get the padding index\n",
    "pad_idx = dictionary.pad()\n",
    "\n",
    "# Now you can collate tokens\n",
    "batch = collate_tokens([binarized_tokens], pad_idx=pad_idx)\n",
    "\n",
    "\n",
    "generator_pt.models[0].eval()\n",
    "\n",
    "# If you have a simple forward method available\n",
    "with torch.no_grad():\n",
    "    logits = generator_pt.models[0](batch['net_input']['src_tokens'], batch['net_input']['src_lengths'])\n",
    "    logits = logits[0]  # assuming the logits are the first return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_pt.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/fairseq/zipball/main\" to /home/paperspace/.cache/torch/hub/main.zip\n",
      "2024-04-16 16:54:59 | INFO | fairseq.file_utils | https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2 not found in cache, downloading to /tmp/tmp_tt6rcr9\n",
      "100%|██████████| 2316140317/2316140317 [00:59<00:00, 38829048.86B/s]\n",
      "2024-04-16 16:55:59 | INFO | fairseq.file_utils | copying /tmp/tmp_tt6rcr9 to cache at /home/paperspace/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "2024-04-16 16:56:02 | INFO | fairseq.file_utils | creating metadata file for /home/paperspace/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "2024-04-16 16:56:02 | INFO | fairseq.file_utils | removing temp file /tmp/tmp_tt6rcr9\n",
      "2024-04-16 16:56:02 | INFO | fairseq.file_utils | loading archive file https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2 from cache at /home/paperspace/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "2024-04-16 16:56:02 | INFO | fairseq.file_utils | extracting archive file /home/paperspace/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae to temp dir /tmp/tmp3t8_n3cb\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/fairseq/checkpoint_utils.py:425: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/fairseq/models/fairseq_model.py:267: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "2024-04-16 17:05:20 | INFO | fairseq.tasks.translation | [en] dictionary: 44512 types\n",
      "2024-04-16 17:05:20 | INFO | fairseq.tasks.translation | [fr] dictionary: 44512 types\n",
      "2024-04-16 17:05:25 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair0253:58342', 'distributed_port': 58342, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 5120, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5120, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0007], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=10, log_format='json', seed=2, data='/home/paperspace/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', source_lang='en', target_lang='fr', max_source_positions=1024, max_target_positions=1024, skip_invalid_size_inputs_valid_test=False, max_tokens=5120, max_sentences=None, train_subset='train', valid_subset='valid', max_sentences_valid=None, sample_without_replacement=0, distributed_world_size=128, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://learnfair0253:58342', distributed_port=58342, device_id=0, arch='transformer_vaswani_wmt_en_de_big', criterion='label_smoothed_cross_entropy', max_epoch=0, max_update=80000, clip_norm=0.0, sentence_avg=False, update_freq=[1.0], fp16=True, optimizer='adam', lr=[0.0007], momentum=0.99, weight_decay=0.0, lr_scheduler='inverse_sqrt', lr_shrink=0.1, save_dir='/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', restore_file='checkpoint_last.pt', save_interval=1, no_save=False, no_epoch_checkpoints=False, validate_interval=1, dropout=0.1, attention_dropout=0.0, relu_dropout=0.0, encoder_normalize_before=False, encoder_learned_pos=False, decoder_learned_pos=False, decoder_normalize_before=False, share_decoder_input_output_embed=True, share_all_embeddings=True, label_smoothing=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, warmup_updates=4000, warmup_init_lr=1e-07, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_attention_heads=16, tokenizer='moses', bpe='subword_nmt', bpe_codes='/home/paperspace/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', task='translation', stop_min_lr=1e-09, _name='transformer_vaswani_wmt_en_de_big', encoder_embed_path=None, decoder_embed_path=None, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0), 'task': {'_name': 'translation', 'data': '/home/paperspace/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0007]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0007]}, 'scoring': None, 'bpe': {'_name': 'subword_nmt', 'bpe_codes': '/home/paperspace/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', 'bpe_separator': '@@'}, 'tokenizer': {'_name': 'moses', 'source_lang': 'en', 'target_lang': 'fr', 'moses_no_dash_splits': False, 'moses_no_escape': False}, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load an En-Fr Transformer model trained on WMT'14 data :\n",
    "en2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n",
    "\n",
    "# Use the GPU (optional):\n",
    "en2fr.cuda()\n",
    "\n",
    "# Translate with beam search:\n",
    "fr = en2fr.translate('Hello world!', beam=5)\n",
    "assert fr == 'Bonjour à tous !'\n",
    "\n",
    "# Manually tokenize:\n",
    "en_toks = en2fr.tokenize('Hello world!')\n",
    "assert en_toks == 'Hello world !'\n",
    "\n",
    "# Manually apply BPE:\n",
    "en_bpe = en2fr.apply_bpe(en_toks)\n",
    "assert en_bpe == 'H@@ ello world !'\n",
    "\n",
    "# Manually binarize:\n",
    "en_bin = en2fr.binarize(en_bpe)\n",
    "assert en_bin.tolist() == [329, 14044, 682, 812, 2]\n",
    "\n",
    "# Generate five translations with top-k sampling:\n",
    "fr_bin = en2fr.generate(en_bin, beam=5, sampling=True, sampling_topk=20)\n",
    "assert len(fr_bin) == 5\n",
    "\n",
    "# Convert one of the samples to a string and detokenize\n",
    "fr_sample = fr_bin[0]['tokens']\n",
    "fr_bpe = en2fr.string(fr_sample)\n",
    "fr_toks = en2fr.remove_bpe(fr_bpe)\n",
    "fr = en2fr.detokenize(fr_toks)\n",
    "assert fr == en2fr.decode(fr_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bon matin , je vais faire un pique @-@ nique demain .', 'Bon après @-@ midi']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([44512, 1024])\n",
      "torch.Size([44512, 1024])\n"
     ]
    }
   ],
   "source": [
    "encoder_embeddings = generator_pt.models[0].encoder.embed_tokens\n",
    "decoder_embeddings = generator_pt.models[0].decoder.embed_tokens\n",
    "\n",
    "# Example: print the size of the embedding layer\n",
    "print(encoder_embeddings.weight.size())\n",
    "print(decoder_embeddings.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoderLayerBase(\n",
      "  (dropout_module): FairseqDropout()\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_dropout_module): FairseqDropout()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): MultiheadAttention(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Accessing all decoder layers\n",
    "decoder_layers = generator_pt.models[0].decoder.layers\n",
    "\n",
    "# Access a specific layer, e.g., the first layer\n",
    "first_decoder_layer = generator_pt.models[0].decoder.layers[0]\n",
    "\n",
    "# Example: print the model of the first decoder layer\n",
    "print(first_decoder_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiheadAttention(\n",
      "  (dropout_module): FairseqDropout()\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Access components of the first decoder layer\n",
    "self_attn = first_decoder_layer.self_attn\n",
    "feed_forward = first_decoder_layer.fc1 # This accesses the first fully connected layer in the feedforward network\n",
    "\n",
    "# Example: print the self-attention module\n",
    "print(self_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__dataclass', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_backward_hooks', '_get_name', '_is_full_backward_hook', '_is_generation_fast', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'add_args', 'add_module', 'apply', 'args', 'bfloat16', 'buffers', 'build_decoder', 'build_embedding', 'build_encoder', 'build_model', 'cfg', 'children', 'cpu', 'cuda', 'decoder', 'double', 'dump_patches', 'encoder', 'eval', 'extra_repr', 'extract_features', 'float', 'forward', 'forward_decoder', 'from_pretrained', 'get_buffer', 'get_extra_state', 'get_normalized_probs', 'get_normalized_probs_scriptable', 'get_parameter', 'get_submodule', 'get_targets', 'half', 'hub_models', 'ipu', 'load_state_dict', 'make_generation_fast_', 'max_decoder_positions', 'max_positions', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'output_layer', 'parameters', 'prepare_for_inference_', 'prepare_for_onnx_export_', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'requires_grad_', 'set_extra_state', 'set_num_updates', 'share_memory', 'state_dict', 'supports_align_args', 'to', 'to_empty', 'train', 'train', 'training', 'type', 'upgrade_state_dict', 'upgrade_state_dict_named', 'xpu', 'zero_grad']\n",
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_backward_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_reorder_encoder_out', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'buffers', 'build_encoder_layer', 'cfg', 'children', 'cpu', 'cuda', 'dictionary', 'double', 'dropout_module', 'dump_patches', 'embed_positions', 'embed_scale', 'embed_tokens', 'encoder_layerdrop', 'eval', 'extra_repr', 'float', 'forward', 'forward_embedding', 'forward_non_torchscript', 'forward_scriptable', 'forward_torchscript', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'layer_norm', 'layernorm_embedding', 'layers', 'load_state_dict', 'max_positions', 'max_source_positions', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'normalize', 'num_layers', 'padding_idx', 'parameters', 'quant_noise', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'reorder_encoder_out', 'requires_grad_', 'return_fc', 'set_extra_state', 'set_num_updates', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'upgrade_state_dict_named', 'version', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "print(dir(generator_pt.models[0]))\n",
    "print(dir(generator_pt.models[0].encoder))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints_path_dict_format = '/home/paperspace/google_drive_v1/Research_Thesis/2024/git_repo/best_checkpoint_dict_format_2.pt'\n",
    "checkpoints_path_dict_format = '/home/paperspace/best_checkpoint_dict_format_2.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m checkpoint_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoints_path_dict_format\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/torch/serialization.py:705\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m--> 705\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m    707\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    708\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/torch/serialization.py:242\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_zipfile_reader, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "checkpoint_dict = torch.load(checkpoints_path_dict_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path_reg_format = '/home/paperspace/google_drive_v1/Research_Thesis/2024/git_repo/best_discriminator_at_2.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "checkpoint_reg = torch.load(checkpoints_path_reg_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = discriminator_cnn.state_dict()\n",
    "model = torch.load(checkpoints_path)\n",
    "pretrained_dict = model.state_dict()\n",
    "print(\"pretrained_dict type: \", type(pretrained_dict))\n",
    "print(\"model : \",model)\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k,\n",
    "                    v in pretrained_dict.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict)\n",
    "# 3. load the new state dict\n",
    "discriminator_cnn.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# August-2024- Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_main\n",
      "2024-08-13 20:03:49 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/tasks/multires_hubert_pretraining.py:154: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  dictionaries = [ (Dictionary.load(f\"{label_dir}/dict.{label}.txt\") if label is not \"\" else None ) for label in self.cfg.labels]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 20:03:51 | INFO | fairseq.file_utils | loading archive file https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2 from cache at /root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/fairseq_model.py:272: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "2024-08-13 20:03:53 | INFO | fairseq.tasks.translation | [en] dictionary: 44512 types\n",
      "2024-08-13 20:03:53 | INFO | fairseq.tasks.translation | [fr] dictionary: 44512 types\n",
      "2024-08-13 20:03:55 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair0253:58342', 'distributed_port': 58342, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 5120, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5120, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0007], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=10, log_format='json', seed=2, data='/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', source_lang='en', target_lang='fr', max_source_positions=1024, max_target_positions=1024, skip_invalid_size_inputs_valid_test=False, max_tokens=5120, max_sentences=None, train_subset='train', valid_subset='valid', max_sentences_valid=None, sample_without_replacement=0, distributed_world_size=128, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://learnfair0253:58342', distributed_port=58342, device_id=0, arch='transformer_vaswani_wmt_en_de_big', criterion='label_smoothed_cross_entropy', max_epoch=0, max_update=80000, clip_norm=0.0, sentence_avg=False, update_freq=[1.0], fp16=True, optimizer='adam', lr=[0.0007], momentum=0.99, weight_decay=0.0, lr_scheduler='inverse_sqrt', lr_shrink=0.1, save_dir='/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', restore_file='checkpoint_last.pt', save_interval=1, no_save=False, no_epoch_checkpoints=False, validate_interval=1, dropout=0.1, attention_dropout=0.0, relu_dropout=0.0, encoder_normalize_before=False, encoder_learned_pos=False, decoder_learned_pos=False, decoder_normalize_before=False, share_decoder_input_output_embed=True, share_all_embeddings=True, label_smoothing=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, warmup_updates=4000, warmup_init_lr=1e-07, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_attention_heads=16, tokenizer='moses', bpe='subword_nmt', bpe_codes='/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', task='translation', stop_min_lr=1e-09, _name='transformer_vaswani_wmt_en_de_big', encoder_embed_path=None, decoder_embed_path=None, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0), 'task': {'_name': 'translation', 'data': '/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0007]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0007]}, 'scoring': None, 'bpe': {'_name': 'subword_nmt', 'bpe_codes': '/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', 'bpe_separator': '@@'}, 'tokenizer': {'_name': 'moses', 'source_lang': 'en', 'target_lang': 'fr', 'moses_no_dash_splits': False, 'moses_no_escape': False}, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 221937664\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the pretrained model using torch.hub\n",
    "model = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n",
    "\n",
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Print the total number of parameters\n",
    "print(f\"Total number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache at: /root/.cache/huggingface/datasets/wmt14/fr-en\n",
      "Dataset already downloaded, loading from cache.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05de63784b104163a8eed47dbdf46417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b5e4521cc34c6f97056e361ce0d0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 16:35:47 | INFO | fairseq.file_utils | loading archive file https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2 from cache at /root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/fairseq_model.py:272: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "2024-08-18 16:35:49 | INFO | fairseq.tasks.translation | [en] dictionary: 44512 types\n",
      "2024-08-18 16:35:49 | INFO | fairseq.tasks.translation | [fr] dictionary: 44512 types\n",
      "2024-08-18 16:35:51 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair0253:58342', 'distributed_port': 58342, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 5120, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5120, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0007], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=10, log_format='json', seed=2, data='/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', source_lang='en', target_lang='fr', max_source_positions=1024, max_target_positions=1024, skip_invalid_size_inputs_valid_test=False, max_tokens=5120, max_sentences=None, train_subset='train', valid_subset='valid', max_sentences_valid=None, sample_without_replacement=0, distributed_world_size=128, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://learnfair0253:58342', distributed_port=58342, device_id=0, arch='transformer_vaswani_wmt_en_de_big', criterion='label_smoothed_cross_entropy', max_epoch=0, max_update=80000, clip_norm=0.0, sentence_avg=False, update_freq=[1.0], fp16=True, optimizer='adam', lr=[0.0007], momentum=0.99, weight_decay=0.0, lr_scheduler='inverse_sqrt', lr_shrink=0.1, save_dir='/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', restore_file='checkpoint_last.pt', save_interval=1, no_save=False, no_epoch_checkpoints=False, validate_interval=1, dropout=0.1, attention_dropout=0.0, relu_dropout=0.0, encoder_normalize_before=False, encoder_learned_pos=False, decoder_learned_pos=False, decoder_normalize_before=False, share_decoder_input_output_embed=True, share_all_embeddings=True, label_smoothing=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, warmup_updates=4000, warmup_init_lr=1e-07, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_attention_heads=16, tokenizer='moses', bpe='subword_nmt', bpe_codes='/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', task='translation', stop_min_lr=1e-09, _name='transformer_vaswani_wmt_en_de_big', encoder_embed_path=None, decoder_embed_path=None, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0), 'task': {'_name': 'translation', 'data': '/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0007]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0007]}, 'scoring': None, 'bpe': {'_name': 'subword_nmt', 'bpe_codes': '/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', 'bpe_separator': '@@'}, 'tokenizer': {'_name': 'moses', 'source_lang': 'en', 'target_lang': 'fr', 'moses_no_dash_splits': False, 'moses_no_escape': False}, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2024-08-18 16:35:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = False\n",
      "2024-08-18 16:35:53 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2024-08-18 16:35:53 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2024-08-18 16:35:53 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Please build Cython components with: `python setup.py build_ext --inplace`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/data_utils.py:313\u001b[0m, in \u001b[0;36mbatch_by_size\u001b[0;34m(indices, num_tokens_fn, num_tokens_vec, max_tokens, max_sentences, required_batch_size_multiple, fixed_shapes)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    314\u001b[0m         batch_by_size_fn,\n\u001b[1;32m    315\u001b[0m         batch_by_size_vec,\n\u001b[1;32m    316\u001b[0m         batch_fixed_shapes_fast,\n\u001b[1;32m    317\u001b[0m     )\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fairseq.data.data_utils_fast'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 79\u001b[0m\n\u001b[1;32m     60\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [fairseq_PT_GeneratorTeacher\u001b[38;5;241m.\u001b[39mencode(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m input_sentences]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Pad the tokenized inputs to the same length for batch processing\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# max_length = max(len(t) for t in tokens)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# max_length = 128\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Translate the sentences with a maximum length of 128 tokens\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m translated_sentences_from_fairseq_PT_GeneratorTeacher \u001b[38;5;241m=\u001b[39m \u001b[43mfairseq_PT_GeneratorTeacher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len_b\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Print the translated sentences\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m translated_sentences_from_fairseq_PT_GeneratorTeacher:\n",
      "File \u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/hub_utils.py:136\u001b[0m, in \u001b[0;36mGeneratorHubInterface.translate\u001b[0;34m(self, sentences, beam, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate\u001b[39m(\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m, sentences: List[\u001b[38;5;28mstr\u001b[39m], beam: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    135\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/hub_utils.py:144\u001b[0m, in \u001b[0;36mGeneratorHubInterface.sample\u001b[0;34m(self, sentences, beam, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample([sentences], beam\u001b[38;5;241m=\u001b[39mbeam, verbose\u001b[38;5;241m=\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    143\u001b[0m tokenized_sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[0;32m--> 144\u001b[0m batched_hypos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(hypos[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m hypos \u001b[38;5;129;01min\u001b[39;00m batched_hypos]\n",
      "File \u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/hub_utils.py:199\u001b[0m, in \u001b[0;36mGeneratorHubInterface.generate\u001b[0;34m(self, tokenized_sentences, beam, verbose, skip_invalid_size_inputs, inference_step_args, prefix_allowed_tokens_fn, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m inference_step_args \u001b[38;5;241m=\u001b[39m inference_step_args \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    198\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_invalid_size_inputs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    200\u001b[0m     batch \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mapply_to_sample(\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), batch)\n\u001b[1;32m    201\u001b[0m     translations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;241m.\u001b[39minference_step(\n\u001b[1;32m    202\u001b[0m         generator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels, batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minference_step_args\n\u001b[1;32m    203\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/hub_utils.py:286\u001b[0m, in \u001b[0;36mGeneratorHubInterface._build_batches\u001b[0;34m(self, tokens, skip_invalid_size_inputs)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_batches\u001b[39m(\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m, tokens: List[List[\u001b[38;5;28mint\u001b[39m]], skip_invalid_size_inputs: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m    284\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    285\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor([t\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens])\n\u001b[0;32m--> 286\u001b[0m     batch_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_dataset_for_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_sentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_positions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_invalid_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_invalid_size_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_iterator_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnext_epoch_itr(shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch_iterator\n",
      "File \u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/tasks/fairseq_task.py:318\u001b[0m, in \u001b[0;36mFairseqTask.get_batch_iterator\u001b[0;34m(self, dataset, max_tokens, max_sentences, max_positions, ignore_invalid_inputs, required_batch_size_multiple, seed, num_shards, shard_id, num_workers, epoch, data_buffer_size, disable_iterator_cache, skip_remainder_batch, grouped_shuffling, update_epoch_batch_itr)\u001b[0m\n\u001b[1;32m    316\u001b[0m     batch_sampler \u001b[38;5;241m=\u001b[39m make_batches\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     batch_sampler \u001b[38;5;241m=\u001b[39m \u001b[43mmake_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# return a reusable, sharded iterator\u001b[39;00m\n\u001b[1;32m    321\u001b[0m epoch_iter \u001b[38;5;241m=\u001b[39m iterators\u001b[38;5;241m.\u001b[39mEpochBatchIterator(\n\u001b[1;32m    322\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m    323\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mcollater,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    334\u001b[0m     persistent_workers\u001b[38;5;241m=\u001b[39mpersistent_workers,\n\u001b[1;32m    335\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/tasks/fairseq_task.py:300\u001b[0m, in \u001b[0;36mFairseqTask.get_batch_iterator.<locals>.make_batches\u001b[0;34m(dataset, epoch)\u001b[0m\n\u001b[1;32m    295\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_indices_by_size(\n\u001b[1;32m    296\u001b[0m         indices, dataset, max_positions, ignore_invalid_inputs\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# create mini-batches with given size constraints\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m batches \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_by_size\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_sentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequired_batch_size_multiple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequired_batch_size_multiple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batches\n",
      "File \u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/fairseq_dataset.py:145\u001b[0m, in \u001b[0;36mFairseqDataset.batch_by_size\u001b[0;34m(self, indices, max_tokens, max_sentences, required_batch_size_multiple)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     num_tokens_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_by_size\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_tokens_vec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_tokens_vec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_sentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequired_batch_size_multiple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequired_batch_size_multiple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/data_utils.py:319\u001b[0m, in \u001b[0;36mbatch_by_size\u001b[0;34m(indices, num_tokens_fn, num_tokens_vec, max_tokens, max_sentences, required_batch_size_multiple, fixed_shapes)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    314\u001b[0m         batch_by_size_fn,\n\u001b[1;32m    315\u001b[0m         batch_by_size_vec,\n\u001b[1;32m    316\u001b[0m         batch_fixed_shapes_fast,\n\u001b[1;32m    317\u001b[0m     )\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease build Cython components with: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`python setup.py build_ext --inplace`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease build (or rebuild) Cython components with `python setup.py build_ext --inplace`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: Please build Cython components with: `python setup.py build_ext --inplace`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import os\n",
    "from datasets import load_dataset  # Import load_dataset from Hugging Face\n",
    "\n",
    "# Define the path of the cache directory\n",
    "home = os.path.expanduser(\"~\")\n",
    "cache_dir = os.path.join(home, \".cache\", \"huggingface\", \"datasets\")\n",
    "\n",
    "# Define the name and configuration of the dataset\n",
    "dataset_name = \"wmt14\"\n",
    "config_name = \"fr-en\"\n",
    "\n",
    "# Build the path for the specific dataset configuration\n",
    "dataset_config_path = os.path.join(cache_dir, dataset_name, config_name)\n",
    "\n",
    "print(f\"Checking cache at: {dataset_config_path}\")\n",
    "\n",
    "# Check if the dataset configuration is already cached\n",
    "if os.path.exists(dataset_config_path) and len(os.listdir(dataset_config_path)) > 0:\n",
    "    print(\"Dataset already downloaded, loading from cache.\")\n",
    "    # If the dataset is already downloaded, load it from the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "else:\n",
    "    print(\"Downloading the dataset.\")\n",
    "    # Download the dataset and specify the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "\n",
    "# Keep the full valid and test datasets\n",
    "valid_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# Extract English and French translations\n",
    "texts = []\n",
    "labels = []\n",
    "for element in test_dataset[\"translation\"]:\n",
    "    texts.append(element[\"en\"])\n",
    "    labels.append(element[\"fr\"])\n",
    "\n",
    "\n",
    "### Generator Teacher - Translations and evaluating bleu score for the test translations\n",
    "\n",
    "# Load the pretrained model using torch.hub\n",
    "fairseq_PT_GeneratorTeacher = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "fairseq_PT_GeneratorTeacher.eval()\n",
    "\n",
    "# Example batch of input sentences (English)\n",
    "# input_sentences = [\n",
    "#     \"The weather is nice today.\",\n",
    "#     \"I am learning how to code.\",\n",
    "#     \"This is a test sentence.\",\n",
    "# ]\n",
    "\n",
    "input_sentences = []\n",
    "input_sentences = texts\n",
    "\n",
    "# Tokenize and encode the input sentences as a batch\n",
    "tokens = [fairseq_PT_GeneratorTeacher.encode(sentence) for sentence in input_sentences]\n",
    "\n",
    "# Pad the tokenized inputs to the same length for batch processing\n",
    "# max_length = max(len(t) for t in tokens)\n",
    "# max_length = 128\n",
    "\n",
    "# tokens_padded = [torch.cat([t, torch.tensor([fairseq_PT_GeneratorTeacher.task.source_dictionary.pad()]) * (max_length - len(t))]) for t in tokens]\n",
    "\n",
    "# # Convert the list of tensors to a single tensor (batch)\n",
    "# tokens_batch = torch.stack(tokens_padded)\n",
    "\n",
    "# # Generate translations\n",
    "# with torch.no_grad():  # Disable gradient calculation for inference\n",
    "#     translations = fairseq_PT_GeneratorTeacher.generate(tokens_batch)\n",
    "\n",
    "# # Decode the translations\n",
    "# translated_sentences = [fairseq_PT_GeneratorTeacher.decode(t) for t in translations]\n",
    "\n",
    "# Translate the sentences with a maximum length of 128 tokens\n",
    "translated_sentences_from_fairseq_PT_GeneratorTeacher = fairseq_PT_GeneratorTeacher.translate(input_sentences, max_len_a=0, max_len_b=128)\n",
    "\n",
    "# Print the translated sentences\n",
    "for sentence in translated_sentences_from_fairseq_PT_GeneratorTeacher:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the metrics\n",
    "# Load the evaluation metrics\n",
    "import evaluate\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "ter_metric = evaluate.load(\"ter\")\n",
    "comet_metric = evaluate.load(\"comet\")\n",
    "\n",
    "result_batch_fairseq_PT_GeneratorTeacher = []\n",
    "\n",
    "result_batch_fairseq_PT_GeneratorTeacher = {\n",
    "\"bleu\": bleu_metric.compute(predictions=translated_sentences_from_fairseq_PT_GeneratorTeacher, references=labels)[\"score\"],\n",
    "\"meteor\": meteor_metric.compute(predictions=translated_sentences_from_fairseq_PT_GeneratorTeacher, references=labels)[\"meteor\"],\n",
    "\"rouge\": rouge_metric.compute(predictions=translated_sentences_from_fairseq_PT_GeneratorTeacher, references=labels),\n",
    "\"ter\": ter_metric.compute(predictions=translated_sentences_from_fairseq_PT_GeneratorTeacher, references=labels)[\"score\"],\n",
    "\"comet\": comet_metric.compute(predictions=translated_sentences_from_fairseq_PT_GeneratorTeacher, references=labels, sources=texts)[\"mean_score\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result_batch_fairseq_PT_GeneratorTeacher\", \"w\") as f:\n",
    "        f.write(\"BLEU Score: \" + str(result_batch_fairseq_PT_GeneratorTeacher[\"bleu\"]) + \"\\n\")\n",
    "        f.write(\"METEOR Score: \" + str(result_batch_fairseq_PT_GeneratorTeacher[\"meteor\"]) + \"\\n\")\n",
    "        f.write(\"ROUGE Scores: \" + str(result_batch_fairseq_PT_GeneratorTeacher[\"rouge\"]) + \"\\n\")\n",
    "        f.write(\"TER Score: \" + str(result_batch_fairseq_PT_GeneratorTeacher[\"ter\"]) + \"\\n\")\n",
    "        f.write(\"COMET Score: \" + str(result_batch_fairseq_PT_GeneratorTeacher[\"comet\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the translations to a text file - translations\n",
    "import os\n",
    "file_path = os.path.join(os.getcwd(), \"translated_sentences_from_fairseq_PT_GeneratorTeacher\")\n",
    "with open(file_path, \"w\") as file:\n",
    "    for translation in translated_sentences_from_fairseq_PT_GeneratorTeacher:\n",
    "        file.write(translation + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getpwd = os.getcwd()\n",
    "file_path_en = os.path.join(getpwd, \"original_english_translations_fairseq_PT_GeneratorTeacher.txt\")\n",
    "# file_path = \"/path/to/translations.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_en, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for text in texts:\n",
    "        file.write(text + \"\\n\")\n",
    "\n",
    "\n",
    "file_path_fr = os.path.join(getpwd, \"original_french_translations_fairseq_PT_GeneratorTeacher.txt\")\n",
    "# file_path = \"/path/to/translations.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_fr, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for label in labels:\n",
    "        file.write(label + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Metrics for GoogleTranslate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache at: /root/.cache/huggingface/datasets/wmt14/fr-en\n",
      "Dataset already downloaded, loading from cache.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import os\n",
    "from datasets import load_dataset  # Import load_dataset from Hugging Face\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path of the cache directory\n",
    "home = os.path.expanduser(\"~\")\n",
    "cache_dir = os.path.join(home, \".cache\", \"huggingface\", \"datasets\")\n",
    "\n",
    "# Define the name and configuration of the dataset\n",
    "dataset_name = \"wmt14\"\n",
    "config_name = \"fr-en\"\n",
    "\n",
    "# Build the path for the specific dataset configuration\n",
    "dataset_config_path = os.path.join(cache_dir, dataset_name, config_name)\n",
    "\n",
    "print(f\"Checking cache at: {dataset_config_path}\")\n",
    "\n",
    "# Check if the dataset configuration is already cached\n",
    "if os.path.exists(dataset_config_path) and len(os.listdir(dataset_config_path)) > 0:\n",
    "    print(\"Dataset already downloaded, loading from cache.\")\n",
    "    # If the dataset is already downloaded, load it from the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "else:\n",
    "    print(\"Downloading the dataset.\")\n",
    "    # Download the dataset and specify the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "\n",
    "# Keep the full valid and test datasets\n",
    "valid_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# Extract English and French translations\n",
    "# import pandas as pd\n",
    "en=list()\n",
    "fr=list()\n",
    "for element in test_dataset[\"translation\"]:\n",
    "    en.append(element[\"en\"])\n",
    "    fr.append(element[\"fr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data_df = pd.DataFrame(list(zip(en,fr)), columns=['src','target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spectacular Wingsuit Jump Over Bogota</td>\n",
       "      <td>Spectaculaire saut en \"wingsuit\" au-dessus de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sportsman Jhonathan Florez jumped from a helic...</td>\n",
       "      <td>Le sportif Jhonathan Florez a sauté jeudi d'un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wearing a wingsuit, he flew past over the famo...</td>\n",
       "      <td>Equipé d'un wingsuit (une combinaison munie d'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A black box in your car?</td>\n",
       "      <td>Une boîte noire dans votre voiture ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As America's road planners struggle to find th...</td>\n",
       "      <td>Alors que les planificateurs du réseau routier...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>The Marguerite-Bourgeoys School Board has crea...</td>\n",
       "      <td>La commission scolaire Marguerite-Bourgeoys a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>Rachida Azdouz from the University of Montreal...</td>\n",
       "      <td>Rachida Azdouz, de l'Université de Montréal, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>Preparation to manage a class in a North-Ameri...</td>\n",
       "      <td>La préparation à gérer une classe dans un cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>\"The real need is for different educational st...</td>\n",
       "      <td>\"Des stratégies pédagogiques différentes, c'es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3002</th>\n",
       "      <td>The research will address inclusion from every...</td>\n",
       "      <td>Les recherches porteront sur l'inclusion sous ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3003 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    src  \\\n",
       "0                 Spectacular Wingsuit Jump Over Bogota   \n",
       "1     Sportsman Jhonathan Florez jumped from a helic...   \n",
       "2     Wearing a wingsuit, he flew past over the famo...   \n",
       "3                              A black box in your car?   \n",
       "4     As America's road planners struggle to find th...   \n",
       "...                                                 ...   \n",
       "2998  The Marguerite-Bourgeoys School Board has crea...   \n",
       "2999  Rachida Azdouz from the University of Montreal...   \n",
       "3000  Preparation to manage a class in a North-Ameri...   \n",
       "3001  \"The real need is for different educational st...   \n",
       "3002  The research will address inclusion from every...   \n",
       "\n",
       "                                                 target  \n",
       "0     Spectaculaire saut en \"wingsuit\" au-dessus de ...  \n",
       "1     Le sportif Jhonathan Florez a sauté jeudi d'un...  \n",
       "2     Equipé d'un wingsuit (une combinaison munie d'...  \n",
       "3                  Une boîte noire dans votre voiture ?  \n",
       "4     Alors que les planificateurs du réseau routier...  \n",
       "...                                                 ...  \n",
       "2998  La commission scolaire Marguerite-Bourgeoys a ...  \n",
       "2999  Rachida Azdouz, de l'Université de Montréal, e...  \n",
       "3000  La préparation à gérer une classe dans un cont...  \n",
       "3001  \"Des stratégies pédagogiques différentes, c'es...  \n",
       "3002  Les recherches porteront sur l'inclusion sous ...  \n",
       "\n",
       "[3003 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_with_max_length(text, max_length):\n",
    "    import deep_translator\n",
    "    from deep_translator import GoogleTranslator\n",
    "    translator = GoogleTranslator(source=\"auto\", target='fr')\n",
    "    # Translate the text to French\n",
    "    translated_text = translator.translate(text)\n",
    "\n",
    "    # Check if the translated text is within the desired maximum length\n",
    "    if len(translated_text) <= max_length:\n",
    "        return translated_text\n",
    "    else:\n",
    "        # Truncate the translated text to the maximum length\n",
    "        return translated_text[:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text_ = []\n",
    "for i, row in json_data_df.iterrows():\n",
    "    translated_text = translate_with_max_length(row['src'], max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La reche'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data_df['translated_text'] = json_data_df['src'].apply(lambda x: translate_with_max_length(x, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>target</th>\n",
       "      <th>translated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spectacular Wingsuit Jump Over Bogota</td>\n",
       "      <td>Spectaculaire saut en \"wingsuit\" au-dessus de ...</td>\n",
       "      <td>Saut spectaculaire en wingsuit au-dessus de Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sportsman Jhonathan Florez jumped from a helic...</td>\n",
       "      <td>Le sportif Jhonathan Florez a sauté jeudi d'un...</td>\n",
       "      <td>Le sportif Jhonathan Florez a sauté d'un hélic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wearing a wingsuit, he flew past over the famo...</td>\n",
       "      <td>Equipé d'un wingsuit (une combinaison munie d'...</td>\n",
       "      <td>Equipé d'une wingsuit, il a survolé à 160km/h ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A black box in your car?</td>\n",
       "      <td>Une boîte noire dans votre voiture ?</td>\n",
       "      <td>Une boîte noire dans votre voiture ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As America's road planners struggle to find th...</td>\n",
       "      <td>Alors que les planificateurs du réseau routier...</td>\n",
       "      <td>Alors que les planificateurs routiers américai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>The Marguerite-Bourgeoys School Board has crea...</td>\n",
       "      <td>La commission scolaire Marguerite-Bourgeoys a ...</td>\n",
       "      <td>La Commission scolaire Marguerite-Bourgeoys a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>Rachida Azdouz from the University of Montreal...</td>\n",
       "      <td>Rachida Azdouz, de l'Université de Montréal, e...</td>\n",
       "      <td>Rachida Azdouz de l'Université de Montréal ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>Preparation to manage a class in a North-Ameri...</td>\n",
       "      <td>La préparation à gérer une classe dans un cont...</td>\n",
       "      <td>Préparation à la gestion d'une classe dans un ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>\"The real need is for different educational st...</td>\n",
       "      <td>\"Des stratégies pédagogiques différentes, c'es...</td>\n",
       "      <td>« Le véritable besoin est de mettre en place d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3002</th>\n",
       "      <td>The research will address inclusion from every...</td>\n",
       "      <td>Les recherches porteront sur l'inclusion sous ...</td>\n",
       "      <td>La recherche abordera l’inclusion sous tous le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3003 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    src  \\\n",
       "0                 Spectacular Wingsuit Jump Over Bogota   \n",
       "1     Sportsman Jhonathan Florez jumped from a helic...   \n",
       "2     Wearing a wingsuit, he flew past over the famo...   \n",
       "3                              A black box in your car?   \n",
       "4     As America's road planners struggle to find th...   \n",
       "...                                                 ...   \n",
       "2998  The Marguerite-Bourgeoys School Board has crea...   \n",
       "2999  Rachida Azdouz from the University of Montreal...   \n",
       "3000  Preparation to manage a class in a North-Ameri...   \n",
       "3001  \"The real need is for different educational st...   \n",
       "3002  The research will address inclusion from every...   \n",
       "\n",
       "                                                 target  \\\n",
       "0     Spectaculaire saut en \"wingsuit\" au-dessus de ...   \n",
       "1     Le sportif Jhonathan Florez a sauté jeudi d'un...   \n",
       "2     Equipé d'un wingsuit (une combinaison munie d'...   \n",
       "3                  Une boîte noire dans votre voiture ?   \n",
       "4     Alors que les planificateurs du réseau routier...   \n",
       "...                                                 ...   \n",
       "2998  La commission scolaire Marguerite-Bourgeoys a ...   \n",
       "2999  Rachida Azdouz, de l'Université de Montréal, e...   \n",
       "3000  La préparation à gérer une classe dans un cont...   \n",
       "3001  \"Des stratégies pédagogiques différentes, c'es...   \n",
       "3002  Les recherches porteront sur l'inclusion sous ...   \n",
       "\n",
       "                                        translated_text  \n",
       "0     Saut spectaculaire en wingsuit au-dessus de Bo...  \n",
       "1     Le sportif Jhonathan Florez a sauté d'un hélic...  \n",
       "2     Equipé d'une wingsuit, il a survolé à 160km/h ...  \n",
       "3                  Une boîte noire dans votre voiture ?  \n",
       "4     Alors que les planificateurs routiers américai...  \n",
       "...                                                 ...  \n",
       "2998  La Commission scolaire Marguerite-Bourgeoys a ...  \n",
       "2999  Rachida Azdouz de l'Université de Montréal ser...  \n",
       "3000  Préparation à la gestion d'une classe dans un ...  \n",
       "3001  « Le véritable besoin est de mettre en place d...  \n",
       "3002  La recherche abordera l’inclusion sous tous le...  \n",
       "\n",
       "[3003 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(),\"github_repo_vastai_rtx\",\"checkpoints\", \"bert_dualG\", \"deep_translator_google_translator_eval_results\",\"translated_text_googleTranslate.txt\")\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(json_data_df['translated_text'].astype(str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Metrics for Google Translator Translated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(),\"github_repo_vastai_rtx\",\"checkpoints\", \"bert_dualG\", \"deep_translator_google_translator_eval_results\",\"translated_text_googleTranslate.txt\")\n",
    "\n",
    "# Read the file and store each line as an element in a list\n",
    "with open(file_path, \"r\") as f:\n",
    "    translations_GT_list = f.readlines()\n",
    "\n",
    "# Optionally, strip newline characters from each line\n",
    "translations_GT_list = [line.strip() for line in translations_GT_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translated_text_GT = json_data_df['translated_text'].tolist()\n",
    "translated_text_GT = translations_GT_list\n",
    "labels = fr\n",
    "texts = en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 34894.38it/s]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint root/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Encoder model frozen.\n",
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "ter_metric = evaluate.load(\"ter\")\n",
    "# chrf_metric = evaluate.load(\"chrf\")\n",
    "# bleurt_metric = evaluate.load(\"bleurt\")\n",
    "comet_metric = evaluate.load(\"comet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default tokenizer.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX 6000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    }
   ],
   "source": [
    "result_batch = []\n",
    "\n",
    "result_batch = {\n",
    "\"bleu\": bleu_metric.compute(predictions=translated_text_GT, references=labels)[\"score\"],\n",
    "\"meteor\": meteor_metric.compute(predictions=translated_text_GT, references=labels)[\"meteor\"],\n",
    "\"rouge\": rouge_metric.compute(predictions=translated_text_GT, references=labels),\n",
    "\"ter\": ter_metric.compute(predictions=translated_text_GT, references=labels)[\"score\"],\n",
    "\"comet\": comet_metric.compute(predictions=translated_text_GT, references=labels, sources=texts)[\"mean_score\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 31.50780329486501,\n",
       " 'meteor': 0.5699915274793543,\n",
       " 'rouge': {'rouge1': 0.6539875203483971,\n",
       "  'rouge2': 0.49583917362497787,\n",
       "  'rougeL': 0.625006161206668,\n",
       "  'rougeLsum': 0.6246781620906818},\n",
       " 'ter': 55.911878330054144,\n",
       " 'comet': 0.7887771203146352}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file_path = os.path.join(os.getcwd(),\"github_repo_vastai_rtx\",\"checkpoints\", \"bert_dualG\", \"deep_translator_google_translator_eval_results\",\"result_batch_googleTranslate.txt\")\n",
    "with open(results_file_path, \"w\") as f:\n",
    "        f.write(\"BLEU Score: \" + str(result_batch[\"bleu\"]) + \"\\n\")\n",
    "        f.write(\"METEOR Score: \" + str(result_batch[\"meteor\"]) + \"\\n\")\n",
    "        f.write(\"ROUGE Scores: \" + str(result_batch[\"rouge\"]) + \"\\n\")\n",
    "        f.write(\"TER Score: \" + str(result_batch[\"ter\"]) + \"\\n\")\n",
    "        f.write(\"COMET Score: \" + str(result_batch[\"comet\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the translations to a text file - translations\n",
    "# import os\n",
    "# file_path = os.path.join(os.getcwd(), \"translations_batch_PreTrained\")\n",
    "# with open(file_path, \"w\") as file:\n",
    "#     for translation in translations_batch_PreTrained:\n",
    "#         file.write(translation + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "getpwd = os.getcwd()\n",
    "file_path_en = os.path.join(getpwd,\"github_repo_vastai_rtx\",\"checkpoints\", \"bert_dualG\", \"deep_translator_google_translator_eval_results\", \"original_english_translations_GT.txt\")\n",
    "# file_path = \"/path/to/translations.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_en, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for text in texts:\n",
    "        file.write(text + \"\\n\")\n",
    "\n",
    "\n",
    "file_path_fr = os.path.join(getpwd, \"github_repo_vastai_rtx\",\"checkpoints\", \"bert_dualG\", \"deep_translator_google_translator_eval_results\",\"original_french_translations_GT.txt\")\n",
    "# file_path = \"/path/to/translations.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_fr, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for label in labels:\n",
    "        file.write(label + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dec-2024 Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_your_pretrained_model = '/home/paperspace/google_drive_v1/Research_Thesis/2024/Adversarial_NMT_th/pretrained_models/wmt14.en-fr.joined-dict.transformer'\n",
    "# from fairseq.models.transformer import TransformerModel\n",
    "# generator_pt = TransformerModel.from_pretrained(\n",
    "# model_name_or_path=path_to_your_pretrained_model,\n",
    "# checkpoint_file='model.pt',\n",
    "# bpe='subword_nmt',\n",
    "# # data_name_or_path='/u/prattisr/phase-2/all_repos/Adversarial_NMT/neural-machine-translation-using-gan-master/data-bin/wmt14_en_fr_raw_sm/50kLines',\n",
    "# data_name_or_path='/home/paperspace/google_drive_v1/Research_Thesis/2024/Adversarial_NMT_th/pretrained_models/wmt14.en-fr.joined-dict.transformer',\n",
    "# bpe_codes = '/home/paperspace/google_drive_v1/Research_Thesis/2024/Adversarial_NMT_th/pretrained_models/wmt14.en-fr.joined-dict.transformer/bpecodes'\n",
    "# )\n",
    "# print(\"Pretrained Generator loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer, BertTokenizerFast\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# import torch\n",
    "from fairseq.data.data_utils import collate_tokens\n",
    "\n",
    "# importing other required libraries\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import dill\n",
    "import os\n",
    "import options\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import data\n",
    "import utils\n",
    "from meters import AverageMeter\n",
    "from PGLoss import PGLoss\n",
    "from tqdm import tqdm\n",
    "# from dictionary import Dictionary\n",
    "from fairseq.data import Dictionary\n",
    "import re\n",
    "import subprocess\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_main\n",
      "2024-12-20 16:24:48 | INFO | fairseq.file_utils | loading archive file https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2 from cache at /root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/fairseq_model.py:272: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "2024-12-20 16:24:50 | INFO | fairseq.tasks.translation | [en] dictionary: 44512 types\n",
      "2024-12-20 16:24:50 | INFO | fairseq.tasks.translation | [fr] dictionary: 44512 types\n",
      "2024-12-20 16:24:53 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair0253:58342', 'distributed_port': 58342, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 5120, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5120, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0007], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=10, log_format='json', seed=2, data='/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', source_lang='en', target_lang='fr', max_source_positions=1024, max_target_positions=1024, skip_invalid_size_inputs_valid_test=False, max_tokens=5120, max_sentences=None, train_subset='train', valid_subset='valid', max_sentences_valid=None, sample_without_replacement=0, distributed_world_size=128, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://learnfair0253:58342', distributed_port=58342, device_id=0, arch='transformer_vaswani_wmt_en_de_big', criterion='label_smoothed_cross_entropy', max_epoch=0, max_update=80000, clip_norm=0.0, sentence_avg=False, update_freq=[1.0], fp16=True, optimizer='adam', lr=[0.0007], momentum=0.99, weight_decay=0.0, lr_scheduler='inverse_sqrt', lr_shrink=0.1, save_dir='/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', restore_file='checkpoint_last.pt', save_interval=1, no_save=False, no_epoch_checkpoints=False, validate_interval=1, dropout=0.1, attention_dropout=0.0, relu_dropout=0.0, encoder_normalize_before=False, encoder_learned_pos=False, decoder_learned_pos=False, decoder_normalize_before=False, share_decoder_input_output_embed=True, share_all_embeddings=True, label_smoothing=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, warmup_updates=4000, warmup_init_lr=1e-07, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_attention_heads=16, tokenizer='moses', bpe='subword_nmt', bpe_codes='/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', task='translation', stop_min_lr=1e-09, _name='transformer_vaswani_wmt_en_de_big', encoder_embed_path=None, decoder_embed_path=None, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0), 'task': {'_name': 'translation', 'data': '/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0007]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0007]}, 'scoring': None, 'bpe': {'_name': 'subword_nmt', 'bpe_codes': '/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', 'bpe_separator': '@@'}, 'tokenizer': {'_name': 'moses', 'source_lang': 'en', 'target_lang': 'fr', 'moses_no_dash_splits': False, 'moses_no_escape': False}, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the pretrained model using torch.hub\n",
    "model = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Not Required ? We can use \"source_dictionary\" instead\n",
    "# import os\n",
    "# from fairseq.data import Dictionary\n",
    "# pwd = os.getcwd()\n",
    "# dict_path = os.path.join(pwd, \"pretrained_models\", \"wmt14.en-fr.joined-dict.transformer\", \"dict.fr.txt\")\n",
    "        \n",
    "#     # dict_path = '/workspace/2024/Adversarial_NMT_th/pretrained_models/wmt14.en-fr.joined-dict.transformer/dict.fr.txt'\n",
    "\n",
    "#     # Load the dictionary\n",
    "# dictionary = Dictionary.load(dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneratorHubInterface(\n",
       "  (models): ModuleList(\n",
       "    (0): TransformerModel(\n",
       "      (encoder): TransformerEncoderBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(44512, 1024, padding_idx=1)\n",
       "        (embed_positions): SinusoidalPositionalEmbedding()\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (decoder): TransformerDecoderBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(44512, 1024, padding_idx=1)\n",
       "        (embed_positions): SinusoidalPositionalEmbedding()\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (output_projection): Linear(in_features=1024, out_features=44512, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating translations using generate model\n",
    "model.eval()\n",
    "src_sentences = [\"Good Morning, I am going to a picnic tomorrow\", \"This is in English\"]\n",
    "# translated_sentences = model.translate(sentence_to_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bon matin, je vais faire un pique-nique demain', 'Ceci est en anglais']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# translated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairseq_tokens = [model.encode(src_sentence) for src_sentence in src_sentences]\n",
    "fairseq_probs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9938,  5384,  9328,     4,    99,  1157,  3415,    13,    18, 11238,\n",
       "        36300, 13653,     2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fairseq_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the internal Fairseq model directly\n",
    "fairseq_model = model.models[0]  # Access the first (and only) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): TransformerEncoderBase(\n",
       "    (dropout_module): FairseqDropout()\n",
       "    (embed_tokens): Embedding(44512, 1024, padding_idx=1)\n",
       "    (embed_positions): SinusoidalPositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayerBase(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerEncoderLayerBase(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerEncoderLayerBase(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerEncoderLayerBase(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerEncoderLayerBase(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerEncoderLayerBase(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoderBase(\n",
       "    (dropout_module): FairseqDropout()\n",
       "    (embed_tokens): Embedding(44512, 1024, padding_idx=1)\n",
       "    (embed_positions): SinusoidalPositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerDecoderLayerBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerDecoderLayerBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerDecoderLayerBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerDecoderLayerBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerDecoderLayerBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerDecoderLayerBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=1024, out_features=44512, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fairseq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get probabilities from Fairseq model\n",
    "# fairseq_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for tokens in fairseq_tokens:\n",
    "#         # tokens = tokens.unsqueeze(0) \n",
    "#         logits = fairseq_model(tokens.unsqueeze(0))\n",
    "#         probs = F.log_softmax(logits, dim=-1)\n",
    "#         fairseq_probs.append(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for tokens in fairseq_tokens:\n",
    "        tokens = tokens.unsqueeze(0)  # Add batch dimension\n",
    "        src_lengths = torch.tensor([tokens.size(1)])  # Length of the input sequence\n",
    "        \n",
    "        # Use model.forward to get the logits\n",
    "        encoder_out = fairseq_model.encoder(tokens, src_lengths=src_lengths)\n",
    "        \n",
    "        # Prepare decoder input (start with BOS token)\n",
    "        bos_token = model.task.target_dictionary.bos()  # Beginning of Sentence token\n",
    "        prev_output_tokens = torch.full((1, 1), bos_token, dtype=torch.long)\n",
    "        \n",
    "        # Perform autoregressive decoding to extract logits\n",
    "        logits = fairseq_model.decoder(prev_output_tokens, encoder_out=encoder_out)\n",
    "        probs = F.log_softmax(logits[0], dim=-1)  # Compute log probabilities\n",
    "        fairseq_probs.append(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probabilities for sentence 1: tensor([[[-12.3325, -12.3328,  -0.3802,  ..., -12.3326, -12.3324, -12.3326]]])\n",
      "Log probabilities for sentence 2: tensor([[[-13.1561, -13.1560,  -0.1114,  ..., -13.1562, -13.1562, -13.1561]]])\n"
     ]
    }
   ],
   "source": [
    "# Print log probabilities for verification\n",
    "for idx, probs in enumerate(fairseq_probs):\n",
    "    print(f\"Log probabilities for sentence {idx + 1}: {probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator-Student "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "max_length = 128\n",
    "# bert_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "checkpoint = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "GS_translations = bert_tokenizer(sentence_to_translate, padding ='max_length' ,truncation=True ,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 3703, 20894,     2,  ..., 59513, 59513, 59513],\n",
       "        [  160,    32,    18,  ..., 59513, 59513, 59513]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GS_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_to_translate_bert = bert_tokenizer(sentence_to_translate, padding ='max_length' ,truncation=True ,return_tensors=\"pt\")\n",
    "tgt_sentence_bert = bert_tokenizer(tgt_sentence, padding ='max_length' ,truncation=True ,return_tensors=\"pt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# # Load Helsinki-NLP model and tokenizer\n",
    "# checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "# bert_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# bert_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "# # Tokenize input sentences for Helsinki-NLP model\n",
    "# sentence_to_translate = [\"Good Morning, I am going to a picnic tomorrow\", \"This is in English\"]\n",
    "# GS_translations = bert_tokenizer(sentence_to_translate, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "\n",
    "# # Get log probabilities from Helsinki-NLP model\n",
    "# with torch.no_grad():\n",
    "#     outputs = bert_model(**GS_translations, output_attentions=False, output_hidden_states=False)\n",
    "#     logits = outputs.logits\n",
    "#     GS_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "# # Tokenize input sentences for Fairseq model\n",
    "# fairseq_tokens = [model.encode(sentence) for sentence in sentence_to_translate]\n",
    "# fairseq_probs = []\n",
    "\n",
    "# # Get probabilities from Fairseq model\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for tokens in fairseq_tokens:\n",
    "#         logits = model.model.forward(tokens.unsqueeze(0))\n",
    "#         probs = F.log_softmax(logits, dim=-1)\n",
    "#         fairseq_probs.append(probs)\n",
    "\n",
    "# # Compute Reverse KLD for each token\n",
    "# reverse_kld = []\n",
    "# for gs_prob, fs_prob in zip(GS_probs, fairseq_probs):\n",
    "#     # Align lengths if necessary\n",
    "#     min_length = min(gs_prob.shape[1], fs_prob.shape[1])\n",
    "#     gs_prob = gs_prob[:, :min_length]\n",
    "#     fs_prob = fs_prob[:, :min_length]\n",
    "\n",
    "#     # Reverse KLD\n",
    "#     kld = torch.sum(gs_prob * (gs_prob - fs_prob), dim=-1)\n",
    "#     reverse_kld.append(kld)\n",
    "\n",
    "# # Print Reverse KLD for each sentence\n",
    "# for idx, kld in enumerate(reverse_kld):\n",
    "#     print(f\"Reverse KLD for sentence {idx + 1}: {kld.mean().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# # Load the pretrained models\n",
    "# fairseq_model = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt').models[0]\n",
    "# bert_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "# bert_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "# # Prepare sentences to translate\n",
    "# sentence_to_translate = [\"Good Morning, I am going to a picnic tomorrow\", \"This is in English\"]\n",
    "# fairseq_tokens = [fairseq_model.task.source_dictionary.encode_line(sentence, add_if_not_exist=False) for sentence in sentence_to_translate]\n",
    "\n",
    "# # Initialize lists to store logits\n",
    "# fairseq_logits_list = []\n",
    "# bert_logits_list = []\n",
    "\n",
    "# # Fairseq logits extraction\n",
    "# fairseq_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for tokens in fairseq_tokens:\n",
    "#         tokens = tokens.unsqueeze(0)  # Add batch dimension\n",
    "#         src_lengths = torch.tensor([tokens.size(1)])  # Sequence length\n",
    "        \n",
    "#         # Encoder output\n",
    "#         encoder_out = fairseq_model.encoder(tokens, src_lengths=src_lengths)\n",
    "        \n",
    "#         # Prepare decoder input (BOS token)\n",
    "#         bos_token = fairseq_model.task.target_dictionary.bos()\n",
    "#         prev_output_tokens = torch.full((1, 1), bos_token, dtype=torch.long)\n",
    "        \n",
    "#         # Autoregressive decoding to get logits\n",
    "#         decoder_logits = fairseq_model.decoder(prev_output_tokens, encoder_out=encoder_out)\n",
    "#         fairseq_logits_list.append(F.log_softmax(decoder_logits[0], dim=-1))\n",
    "\n",
    "# # Bert logits extraction\n",
    "# bert_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for sentence in sentence_to_translate:\n",
    "#         inputs = bert_tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "#         outputs = bert_model(**inputs)\n",
    "#         bert_logits = F.log_softmax(outputs.logits, dim=-1)\n",
    "#         bert_logits_list.append(bert_logits)\n",
    "\n",
    "# # Compute Reverse KLD\n",
    "# reverse_kld = []\n",
    "# for fs_logits, bert_logits in zip(fairseq_logits_list, bert_logits_list):\n",
    "#     # Ensure sequence lengths match (truncate to the minimum length)\n",
    "#     min_length = min(fs_logits.size(1), bert_logits.size(1))\n",
    "#     fs_logits = fs_logits[:, :min_length, :]\n",
    "#     bert_logits = bert_logits[:, :min_length, :]\n",
    "\n",
    "#     # Reverse KLD calculation\n",
    "#     r_kld = torch.sum(bert_logits * (bert_logits - fs_logits), dim=-1).mean()\n",
    "#     reverse_kld.append(r_kld)\n",
    "\n",
    "# # Print Reverse KLD for each sentence\n",
    "# for idx, r_kld in enumerate(reverse_kld):\n",
    "#     print(f\"Reverse KLD for sentence {idx + 1}: {r_kld.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_main\n",
      "2024-12-20 10:49:38 | INFO | fairseq.file_utils | loading archive file https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2 from cache at /root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "2024-12-20 10:49:41 | INFO | fairseq.tasks.translation | [en] dictionary: 44512 types\n",
      "2024-12-20 10:49:41 | INFO | fairseq.tasks.translation | [fr] dictionary: 44512 types\n",
      "2024-12-20 10:49:44 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair0253:58342', 'distributed_port': 58342, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 5120, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5120, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0007], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=10, log_format='json', seed=2, data='/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', source_lang='en', target_lang='fr', max_source_positions=1024, max_target_positions=1024, skip_invalid_size_inputs_valid_test=False, max_tokens=5120, max_sentences=None, train_subset='train', valid_subset='valid', max_sentences_valid=None, sample_without_replacement=0, distributed_world_size=128, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://learnfair0253:58342', distributed_port=58342, device_id=0, arch='transformer_vaswani_wmt_en_de_big', criterion='label_smoothed_cross_entropy', max_epoch=0, max_update=80000, clip_norm=0.0, sentence_avg=False, update_freq=[1.0], fp16=True, optimizer='adam', lr=[0.0007], momentum=0.99, weight_decay=0.0, lr_scheduler='inverse_sqrt', lr_shrink=0.1, save_dir='/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', restore_file='checkpoint_last.pt', save_interval=1, no_save=False, no_epoch_checkpoints=False, validate_interval=1, dropout=0.1, attention_dropout=0.0, relu_dropout=0.0, encoder_normalize_before=False, encoder_learned_pos=False, decoder_learned_pos=False, decoder_normalize_before=False, share_decoder_input_output_embed=True, share_all_embeddings=True, label_smoothing=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, warmup_updates=4000, warmup_init_lr=1e-07, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_attention_heads=16, tokenizer='moses', bpe='subword_nmt', bpe_codes='/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', task='translation', stop_min_lr=1e-09, _name='transformer_vaswani_wmt_en_de_big', encoder_embed_path=None, decoder_embed_path=None, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0), 'task': {'_name': 'translation', 'data': '/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0007]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0007]}, 'scoring': None, 'bpe': {'_name': 'subword_nmt', 'bpe_codes': '/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', 'bpe_separator': '@@'}, 'tokenizer': {'_name': 'moses', 'source_lang': 'en', 'target_lang': 'fr', 'moses_no_dash_splits': False, 'moses_no_escape': False}, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify either decoder_input_ids or decoder_inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb Cell 121\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B86.122.133.229/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb#Y231sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentence_to_translate:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B86.122.133.229/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb#Y231sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     inputs \u001b[39m=\u001b[39m bert_tokenizer(sentence, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B86.122.133.229/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb#Y231sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     outputs \u001b[39m=\u001b[39m bert_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B86.122.133.229/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb#Y231sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     bert_logits \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlog_softmax(outputs\u001b[39m.\u001b[39mlogits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B86.122.133.229/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb#Y231sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     bert_logits_list\u001b[39m.\u001b[39mappend(bert_logits)\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:1458\u001b[0m, in \u001b[0;36mMarianMTModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1453\u001b[0m     \u001b[39mif\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoder_inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1454\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1455\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1456\u001b[0m         )\n\u001b[0;32m-> 1458\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1459\u001b[0m     input_ids,\n\u001b[1;32m   1460\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1461\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1462\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m   1463\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1464\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1465\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1466\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1467\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1468\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1469\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1470\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1471\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1472\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1473\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1474\u001b[0m )\n\u001b[1;32m   1475\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\n\u001b[1;32m   1477\u001b[0m masked_lm_loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:1259\u001b[0m, in \u001b[0;36mMarianModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1252\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1253\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m   1254\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1256\u001b[0m     )\n\u001b[1;32m   1258\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1259\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1260\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1261\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1262\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_outputs[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m   1263\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1264\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1265\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1266\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1267\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1268\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1269\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1270\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1271\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1272\u001b[0m )\n\u001b[1;32m   1274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1275\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:984\u001b[0m, in \u001b[0;36mMarianDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    982\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    983\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 984\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either decoder_input_ids or decoder_inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    986\u001b[0m \u001b[39m# past_key_values_length\u001b[39;00m\n\u001b[1;32m    987\u001b[0m past_key_values_length \u001b[39m=\u001b[39m past_key_values[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the pretrained Fairseq model\n",
    "fairseq_hub = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n",
    "fairseq_model = fairseq_hub.models[0]  # Access the model from the hub\n",
    "\n",
    "# Access the source dictionary from the Fairseq hub object\n",
    "source_dictionary = fairseq_hub.task.source_dictionary\n",
    "\n",
    "# Prepare sentences to translate\n",
    "sentence_to_translate = [\"Good Morning, I am going to a picnic tomorrow\", \"This is in English\"]\n",
    "fairseq_tokens = [source_dictionary.encode_line(sentence, add_if_not_exist=False).long() for sentence in sentence_to_translate]\n",
    "\n",
    "# Initialize lists to store logits\n",
    "fairseq_logits_list = []\n",
    "bert_logits_list = []\n",
    "\n",
    "# Fairseq logits extraction\n",
    "fairseq_model.eval()\n",
    "with torch.no_grad():\n",
    "    for tokens in fairseq_tokens:\n",
    "        tokens = tokens.unsqueeze(0)  # Add batch dimension\n",
    "        src_lengths = torch.tensor([tokens.size(1)])  # Sequence length\n",
    "        \n",
    "        # Encoder output\n",
    "        encoder_out = fairseq_model.encoder(tokens, src_lengths=src_lengths)\n",
    "        \n",
    "        # Prepare decoder input (BOS token)\n",
    "        bos_token = fairseq_hub.task.target_dictionary.bos()\n",
    "        prev_output_tokens = torch.full((1, 1), bos_token, dtype=torch.long)\n",
    "        \n",
    "        # Autoregressive decoding to get logits\n",
    "        decoder_logits = fairseq_model.decoder(prev_output_tokens, encoder_out=encoder_out)\n",
    "        fairseq_logits_list.append(F.log_softmax(decoder_logits[0], dim=-1))\n",
    "\n",
    "# Load Helsinki-NLP model for logits comparison\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "bert_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "bert_model.eval()\n",
    "with torch.no_grad():\n",
    "    for sentence in sentence_to_translate:\n",
    "        inputs = bert_tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "        outputs = bert_model(**inputs)\n",
    "        bert_logits = F.log_softmax(outputs.logits, dim=-1)\n",
    "        bert_logits_list.append(bert_logits)\n",
    "\n",
    "# Compute Reverse KLD\n",
    "reverse_kld = []\n",
    "for fs_logits, bert_logits in zip(fairseq_logits_list, bert_logits_list):\n",
    "    # Ensure sequence lengths match (truncate to the minimum length)\n",
    "    min_length = min(fs_logits.size(1), bert_logits.size(1))\n",
    "    fs_logits = fs_logits[:, :min_length, :]\n",
    "    bert_logits = bert_logits[:, :min_length, :]\n",
    "\n",
    "    # Reverse KLD calculation\n",
    "    r_kld = torch.sum(bert_logits * (bert_logits - fs_logits), dim=-1).mean()\n",
    "    reverse_kld.append(r_kld)\n",
    "\n",
    "# Print Reverse KLD for each sentence\n",
    "for idx, r_kld in enumerate(reverse_kld):\n",
    "    print(f\"Reverse KLD for sentence {idx + 1}: {r_kld.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_main\n",
      "2024-12-20 10:52:41 | INFO | fairseq.file_utils | loading archive file https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2 from cache at /root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "2024-12-20 10:52:43 | INFO | fairseq.tasks.translation | [en] dictionary: 44512 types\n",
      "2024-12-20 10:52:43 | INFO | fairseq.tasks.translation | [fr] dictionary: 44512 types\n",
      "2024-12-20 10:52:46 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair0253:58342', 'distributed_port': 58342, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 5120, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5120, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0007], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=10, log_format='json', seed=2, data='/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', source_lang='en', target_lang='fr', max_source_positions=1024, max_target_positions=1024, skip_invalid_size_inputs_valid_test=False, max_tokens=5120, max_sentences=None, train_subset='train', valid_subset='valid', max_sentences_valid=None, sample_without_replacement=0, distributed_world_size=128, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://learnfair0253:58342', distributed_port=58342, device_id=0, arch='transformer_vaswani_wmt_en_de_big', criterion='label_smoothed_cross_entropy', max_epoch=0, max_update=80000, clip_norm=0.0, sentence_avg=False, update_freq=[1.0], fp16=True, optimizer='adam', lr=[0.0007], momentum=0.99, weight_decay=0.0, lr_scheduler='inverse_sqrt', lr_shrink=0.1, save_dir='/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', restore_file='checkpoint_last.pt', save_interval=1, no_save=False, no_epoch_checkpoints=False, validate_interval=1, dropout=0.1, attention_dropout=0.0, relu_dropout=0.0, encoder_normalize_before=False, encoder_learned_pos=False, decoder_learned_pos=False, decoder_normalize_before=False, share_decoder_input_output_embed=True, share_all_embeddings=True, label_smoothing=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, warmup_updates=4000, warmup_init_lr=1e-07, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_attention_heads=16, tokenizer='moses', bpe='subword_nmt', bpe_codes='/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', task='translation', stop_min_lr=1e-09, _name='transformer_vaswani_wmt_en_de_big', encoder_embed_path=None, decoder_embed_path=None, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0), 'task': {'_name': 'translation', 'data': '/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0007]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0007]}, 'scoring': None, 'bpe': {'_name': 'subword_nmt', 'bpe_codes': '/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', 'bpe_separator': '@@'}, 'tokenizer': {'_name': 'moses', 'source_lang': 'en', 'target_lang': 'fr', 'moses_no_dash_splits': False, 'moses_no_escape': False}, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify either decoder_input_ids or decoder_inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb Cell 122\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B86.122.133.229/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb#Y232sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentence_to_translate:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B86.122.133.229/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb#Y232sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     inputs \u001b[39m=\u001b[39m bert_tokenizer(sentence, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B86.122.133.229/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb#Y232sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     outputs \u001b[39m=\u001b[39m bert_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B86.122.133.229/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb#Y232sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     bert_logits \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlog_softmax(outputs\u001b[39m.\u001b[39mlogits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B86.122.133.229/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb#Y232sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     bert_logits_list\u001b[39m.\u001b[39mappend(bert_logits)\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:1458\u001b[0m, in \u001b[0;36mMarianMTModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1453\u001b[0m     \u001b[39mif\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoder_inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1454\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1455\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1456\u001b[0m         )\n\u001b[0;32m-> 1458\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1459\u001b[0m     input_ids,\n\u001b[1;32m   1460\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1461\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1462\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m   1463\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1464\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1465\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1466\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1467\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1468\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1469\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1470\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1471\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1472\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1473\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1474\u001b[0m )\n\u001b[1;32m   1475\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\n\u001b[1;32m   1477\u001b[0m masked_lm_loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:1259\u001b[0m, in \u001b[0;36mMarianModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1252\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1253\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m   1254\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1256\u001b[0m     )\n\u001b[1;32m   1258\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1259\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1260\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1261\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1262\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_outputs[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m   1263\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1264\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1265\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1266\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1267\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1268\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1269\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1270\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1271\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1272\u001b[0m )\n\u001b[1;32m   1274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1275\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/transformers/models/marian/modeling_marian.py:984\u001b[0m, in \u001b[0;36mMarianDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    982\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    983\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 984\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either decoder_input_ids or decoder_inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    986\u001b[0m \u001b[39m# past_key_values_length\u001b[39;00m\n\u001b[1;32m    987\u001b[0m past_key_values_length \u001b[39m=\u001b[39m past_key_values[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the pretrained Fairseq model\n",
    "fairseq_hub = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n",
    "fairseq_model = fairseq_hub.models[0]  # Access the model from the hub\n",
    "\n",
    "# Access the source dictionary from the Fairseq hub object\n",
    "source_dictionary = fairseq_hub.task.source_dictionary\n",
    "\n",
    "# Prepare sentences to translate\n",
    "sentence_to_translate = [\"Good Morning, I am going to a picnic tomorrow\", \"This is in English\"]\n",
    "fairseq_tokens = [source_dictionary.encode_line(sentence, add_if_not_exist=False).long() for sentence in sentence_to_translate]\n",
    "\n",
    "# Initialize lists to store logits\n",
    "fairseq_logits_list = []\n",
    "bert_logits_list = []\n",
    "\n",
    "# Fairseq logits extraction\n",
    "fairseq_model.eval()\n",
    "with torch.no_grad():\n",
    "    for tokens in fairseq_tokens:\n",
    "        tokens = tokens.unsqueeze(0)  # Add batch dimension\n",
    "        src_lengths = torch.tensor([tokens.size(1)])  # Sequence length\n",
    "        \n",
    "        # Encoder output\n",
    "        encoder_out = fairseq_model.encoder(tokens, src_lengths=src_lengths)\n",
    "        \n",
    "        # Prepare decoder input (BOS token)\n",
    "        bos_token = fairseq_hub.task.target_dictionary.bos()\n",
    "        prev_output_tokens = torch.full((1, 1), bos_token, dtype=torch.long)\n",
    "        \n",
    "        # Autoregressive decoding to get logits\n",
    "        decoder_logits = fairseq_model.decoder(prev_output_tokens, encoder_out=encoder_out)\n",
    "        fairseq_logits_list.append(F.log_softmax(decoder_logits[0], dim=-1))\n",
    "\n",
    "# Load Helsinki-NLP model for logits comparison\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "bert_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "bert_model.eval()\n",
    "with torch.no_grad():\n",
    "    for sentence in sentence_to_translate:\n",
    "        inputs = bert_tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "        outputs = bert_model(**inputs)\n",
    "        bert_logits = F.log_softmax(outputs.logits, dim=-1)\n",
    "        bert_logits_list.append(bert_logits)\n",
    "\n",
    "# Compute Reverse KLD\n",
    "reverse_kld = []\n",
    "for fs_logits, bert_logits in zip(fairseq_logits_list, bert_logits_list):\n",
    "    # Ensure sequence lengths match (truncate to the minimum length)\n",
    "    min_length = min(fs_logits.size(1), bert_logits.size(1))\n",
    "    fs_logits = fs_logits[:, :min_length, :]\n",
    "    bert_logits = bert_logits[:, :min_length, :]\n",
    "\n",
    "    # Reverse KLD calculation\n",
    "    r_kld = torch.sum(bert_logits * (bert_logits - fs_logits), dim=-1).mean()\n",
    "    reverse_kld.append(r_kld)\n",
    "\n",
    "# Print Reverse KLD for each sentence\n",
    "for idx, r_kld in enumerate(reverse_kld):\n",
    "    print(f\"Reverse KLD for sentence {idx + 1}: {r_kld.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Helper Function: Reverse KLD\n",
    "# def reverse_kld_loss(logits_g1, logits_g2):\n",
    "#     \"\"\"\n",
    "#     Compute the Reverse Kullback-Leibler Divergence (KLD) between two distributions.\n",
    "    \n",
    "#     Args:\n",
    "#         logits_g1 (torch.Tensor): Logits from G1 (pretrained generator).\n",
    "#         logits_g2 (torch.Tensor): Logits from G2 (trainable generator).\n",
    "    \n",
    "#     Returns:\n",
    "#         torch.Tensor: Reverse KLD loss.\n",
    "#     \"\"\"\n",
    "#     # Convert logits to probabilities\n",
    "#     probs_g1 = F.softmax(logits_g1, dim=-1)\n",
    "#     log_probs_g1 = F.log_softmax(logits_g1, dim=-1)\n",
    "#     log_probs_g2 = F.log_softmax(logits_g2, dim=-1)\n",
    "    \n",
    "#     # Reverse KLD: KL(G1 || G2)\n",
    "#     reverse_kld = F.kl_div(log_probs_g2, probs_g1, reduction='batchmean')\n",
    "#     return reverse_kld\n",
    "\n",
    "# # Training Loop\n",
    "# for epoch_i in tqdm(range(1, args.epochs + 1)):\n",
    "#     total_reverse_kld_loss = 0\n",
    "#     generator2_train.train()\n",
    "#     generator1_pretrained.eval()\n",
    "\n",
    "#     for i, sample in enumerate(train_dataloader):\n",
    "#         # Prepare inputs\n",
    "#         src_sentences = sample[\"input_ids\"].to(device)\n",
    "#         tgt_sentences = sample[\"target_ids\"].to(device)\n",
    "#         attention_mask = sample[\"attention_mask\"].to(device)\n",
    "\n",
    "#         # G2 Forward Pass (Trainable Generator)\n",
    "#         g2_output = generator2_train(\n",
    "#             input_ids=src_sentences,\n",
    "#             attention_mask=attention_mask,\n",
    "#             decoder_input_ids=tgt_sentences,\n",
    "#             output_hidden_states=True,\n",
    "#             return_dict=True,\n",
    "#         )\n",
    "#         logits_g2 = g2_output.logits  # Shape: [batch_size, seq_len, vocab_size]\n",
    "\n",
    "#         # G1 Forward Pass (Pretrained Fairseq Generator)\n",
    "#         src_sentences_g1 = ids_to_sentences_bert(src_sentences)\n",
    "#         translated_sentences_g1 = generator1_pretrained.translate(src_sentences_g1)\n",
    "#         fake_tgt_sentences_g1 = sentences_to_ids(translated_sentences_g1)[\"input_ids\"].to(device)\n",
    "\n",
    "#         # Compute logits for G1\n",
    "#         logits_g1 = generator1_pretrained.models[0](\n",
    "#             fake_tgt_sentences_g1\n",
    "#         )  # Assuming G1 returns logits directly\n",
    "\n",
    "#         # Compute Reverse KLD Loss\n",
    "#         reverse_kld = reverse_kld_loss(logits_g1, logits_g2)\n",
    "#         total_reverse_kld_loss += reverse_kld.item()\n",
    "\n",
    "#         # Backpropagation and Optimization\n",
    "#         optimizer_g.zero_grad()\n",
    "#         reverse_kld.backward()\n",
    "#         optimizer_g.step()\n",
    "\n",
    "#     print(f\"Epoch {epoch_i}: Reverse KLD Loss = {total_reverse_kld_loss / len(train_dataloader)}\")\n",
    "\n",
    "# # Save the model\n",
    "# torch.save(generator2_train.state_dict(), f\"generator2_epoch_{epoch_i}.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse KLD - V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fairseq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb Cell 127\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B86.122.133.229/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B86.122.133.229/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# import torch\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B86.122.133.229/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfairseq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m collate_tokens\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B86.122.133.229/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# importing other required libraries\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B86.122.133.229/root/git_repo_dec_2024/Adversarial_NMT_th/fairseq_pretrained.ipynb#Y244sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39margparse\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fairseq'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer, BertTokenizerFast\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# import torch\n",
    "from fairseq.data.data_utils import collate_tokens\n",
    "\n",
    "# importing other required libraries\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import dill\n",
    "import os\n",
    "import options\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import data\n",
    "import utils\n",
    "from meters import AverageMeter\n",
    "from PGLoss import PGLoss\n",
    "from tqdm import tqdm\n",
    "# from dictionary import Dictionary\n",
    "from fairseq.data import Dictionary\n",
    "import re\n",
    "import subprocess\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache at: /root/.cache/huggingface/datasets/wmt14/fr-en\n",
      "Dataset already downloaded, loading from cache.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a698c3ee9924404bd49c4794bcb48dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1c4f39d4af43218d2355979bc8c5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loading data using datasets library from the HuggingFace\n",
    "# Get user's home directory\n",
    "import os\n",
    "home = os.path.expanduser(\"~\")\n",
    "\n",
    "# Define the path of the cache directory\n",
    "cache_dir = os.path.join(home, \".cache\", \"huggingface\", \"datasets\")\n",
    "\n",
    "# Define the name and configuration of the dataset\n",
    "dataset_name = \"wmt14\"\n",
    "config_name = \"fr-en\"\n",
    "\n",
    "# Build the path for the specific dataset configuration\n",
    "dataset_config_path = os.path.join(cache_dir, dataset_name, config_name)\n",
    "\n",
    "print(f\"Checking cache at: {dataset_config_path}\")\n",
    "\n",
    "# Check if the dataset configuration is already cached\n",
    "if os.path.exists(dataset_config_path) and len(os.listdir(dataset_config_path)) > 0:\n",
    "    print(\"Dataset already downloaded, loading from cache.\")\n",
    "    # If the dataset is already downloaded, load it from the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "else:\n",
    "    print(\"Downloading the dataset.\")\n",
    "    # Download the dataset and specify the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "\n",
    "# Here, you should adjust the loading of subsets to avoid redundant downloads or loading.\n",
    "# Load 50k rows of the train dataset\n",
    "# train_dataset = dataset[\"train\"].select(range(1000000))\n",
    "# train_dataset = dataset[\"train\"].select(range(100000))\n",
    "train_dataset = dataset[\"train\"].select(range(10))\n",
    "\n",
    "# Keep the full valid and test datasets\n",
    "valid_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# Loading Bert Model\n",
    "# bert_model = \"bert-base-multilingual-cased\"\n",
    "\n",
    "# Pre-processing the data\n",
    "# To-Do : Need to change the max_length to 50 from 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "prefix = \"\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def preprocess_MarianMT(examples):\n",
    "    # Initialize the BERT tokenizer\n",
    "    # tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    \n",
    "\n",
    "    # checkpoint = \"google-t5/t5-small\"\n",
    "    # checkpoint = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    en = list()\n",
    "    fr = list()\n",
    "    for element in examples[\"translation\"]:\n",
    "        # print(\"element: \", element)\n",
    "        en.append(element[\"en\"])\n",
    "        fr.append(element[\"fr\"])\n",
    "    \n",
    "    en = [prefix + text for text in en]\n",
    "\n",
    "    # Tokenize the data\n",
    "    inputs = tokenizer(en, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        targets = tokenizer(fr, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    # Convert tokens to their corresponding IDs\n",
    "    input_ids = inputs.input_ids\n",
    "    target_ids = targets.input_ids\n",
    "\n",
    "    # Create attention masks\n",
    "    input_attention_mask = inputs.attention_mask\n",
    "    target_attention_mask = targets.attention_mask\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": input_attention_mask,\n",
    "        \"target_ids\": target_ids,\n",
    "        \"target_attention_mask\": target_attention_mask,\n",
    "    }\n",
    "\n",
    "# print(train_dataset[0])\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True) # using the other berttokenizer map function\n",
    "tokenized_train_datasets = train_dataset.map(\n",
    "    preprocess_MarianMT, batched=True\n",
    ")  # Using the bertFaSTtOKENIZER MAp function\n",
    "tokenized_valid_datasets = valid_dataset.map(\n",
    "    preprocess_MarianMT, batched=True\n",
    ")  # Using the bertFaSTtOKENIZER MAp function\n",
    "tokenized_test_datasets = test_dataset.map(\n",
    "    preprocess_MarianMT, batched=True\n",
    ")  # Using the bertFaSTtOKENIZER MAp function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# # Fairseq logits extraction function\n",
    "# def extract_fairseq_logits(fairseq_hub, sentence_to_translate):\n",
    "#     fairseq_model = fairseq_hub.models[0]\n",
    "#     source_dictionary = fairseq_hub.task.source_dictionary\n",
    "    \n",
    "#     # Prepare sentences\n",
    "#     fairseq_tokens = [source_dictionary.encode_line(sentence, add_if_not_exist=False).long() for sentence in sentence_to_translate]\n",
    "    \n",
    "#     fairseq_model.eval()\n",
    "#     fairseq_logits_list = []\n",
    "#     with torch.no_grad():\n",
    "#         for tokens in fairseq_tokens:\n",
    "#             tokens = tokens.unsqueeze(0)  # Add batch dimension\n",
    "#             src_lengths = torch.tensor([tokens.size(1)])  # Sequence length\n",
    "            \n",
    "#             # Encoder output\n",
    "#             encoder_out = fairseq_model.encoder(tokens, src_lengths=src_lengths)\n",
    "            \n",
    "#             # Prepare decoder input (BOS token)\n",
    "#             bos_token = fairseq_hub.task.target_dictionary.bos()\n",
    "#             prev_output_tokens = torch.full((1, 1), bos_token, dtype=torch.long)\n",
    "            \n",
    "#             # Autoregressive decoding to get logits\n",
    "#             decoder_logits = fairseq_model.decoder(prev_output_tokens, encoder_out=encoder_out)\n",
    "#             fairseq_logits_list.append(F.log_softmax(decoder_logits[0], dim=-1))\n",
    "#     return fairseq_logits_list\n",
    "\n",
    "# # Marian logits extraction function\n",
    "# def extract_marian_logits(generator2_train_dp, src_sentences, tgt_sentences, attention_mask):\n",
    "#     generator2_train_dp = generator2_train_dp.module if isinstance(generator2_train_dp, torch.nn.DataParallel) else generator2_train_dp\n",
    "\n",
    "#     generator2_train_out = generator2_train_dp(\n",
    "#         input_ids=src_sentences,\n",
    "#         attention_mask=attention_mask,\n",
    "#         decoder_input_ids=tgt_sentences,\n",
    "#         output_hidden_states=True,\n",
    "#         return_dict=True\n",
    "#     )\n",
    "    \n",
    "#     fake_tgt_sentences_probs = F.log_softmax(generator2_train_out.logits, dim=-1)\n",
    "#     fake_tgt_sentences_probs = fake_tgt_sentences_probs.view(-1, fake_tgt_sentences_probs.size(-1))\n",
    "#     return fake_tgt_sentences_probs\n",
    "\n",
    "# # Reverse KLD Calculation\n",
    "# def compute_reverse_kld(fairseq_logits_list, marian_logits_list):\n",
    "#     reverse_kld = []\n",
    "#     for fs_logits, marian_logits in zip(fairseq_logits_list, marian_logits_list):\n",
    "#         # Ensure sequence lengths match (truncate to the minimum length)\n",
    "#         min_length = min(fs_logits.size(1), marian_logits.size(1))\n",
    "#         fs_logits = fs_logits[:, :min_length, :]\n",
    "#         marian_logits = marian_logits[:, :min_length, :]\n",
    "\n",
    "#         # Reverse KLD calculation\n",
    "#         r_kld = torch.sum(marian_logits * (marian_logits - fs_logits), dim=-1).mean()\n",
    "#         reverse_kld.append(r_kld)\n",
    "#     return reverse_kld\n",
    "\n",
    "# # Example usage\n",
    "# generator_teacher_hub = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n",
    "# sentence_to_translate = [\"Good Morning, I am going to a picnic tomorrow\", \"This is in English\"]\n",
    "# generator_teacher_logits_list = extract_fairseq_logits(generator_teacher_hub, sentence_to_translate)\n",
    "\n",
    "# # Loading generator-student\n",
    "# from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# checkpoint = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# generator_student = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# generator_student_logits_list = extract_marian_logits(generator_student, src_sentences, tgt_sentences, attention_mask)\n",
    "# reverse_kld_values = compute_reverse_kld(generator_teacher_logits_list, generator_student_logits_list)\n",
    "\n",
    "# # Print Reverse KLD for each sentence\n",
    "# for idx, r_kld in enumerate(reverse_kld_values):\n",
    "#     print(f\"Reverse KLD for sentence {idx + 1}: {r_kld.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_main\n",
      "2024-12-21 05:10:44 | INFO | fairseq.file_utils | loading archive file https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2 from cache at /root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae\n",
      "2024-12-21 05:10:48 | INFO | fairseq.tasks.translation | [en] dictionary: 44512 types\n",
      "2024-12-21 05:10:48 | INFO | fairseq.tasks.translation | [fr] dictionary: 44512 types\n",
      "2024-12-21 05:10:51 | INFO | fairseq.models.fairseq_model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair0253:58342', 'distributed_port': 58342, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 5120, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 5120, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0007], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=10, log_format='json', seed=2, data='/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', source_lang='en', target_lang='fr', max_source_positions=1024, max_target_positions=1024, skip_invalid_size_inputs_valid_test=False, max_tokens=5120, max_sentences=None, train_subset='train', valid_subset='valid', max_sentences_valid=None, sample_without_replacement=0, distributed_world_size=128, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://learnfair0253:58342', distributed_port=58342, device_id=0, arch='transformer_vaswani_wmt_en_de_big', criterion='label_smoothed_cross_entropy', max_epoch=0, max_update=80000, clip_norm=0.0, sentence_avg=False, update_freq=[1.0], fp16=True, optimizer='adam', lr=[0.0007], momentum=0.99, weight_decay=0.0, lr_scheduler='inverse_sqrt', lr_shrink=0.1, save_dir='/checkpoint02/myleott/2018-05-19/wmt14_en_fr.fp16_allreduce.fp16.maxupd80000.transformer_vaswani_wmt_en_de_big.shareemb.adam.beta0.9,0.98.initlr1e-07.warmup4000.lr0.0007.clip0.0.drop0.1.wd0.0.ls0.1.maxtok5120.seed2.ngpu128', restore_file='checkpoint_last.pt', save_interval=1, no_save=False, no_epoch_checkpoints=False, validate_interval=1, dropout=0.1, attention_dropout=0.0, relu_dropout=0.0, encoder_normalize_before=False, encoder_learned_pos=False, decoder_learned_pos=False, decoder_normalize_before=False, share_decoder_input_output_embed=True, share_all_embeddings=True, label_smoothing=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, warmup_updates=4000, warmup_init_lr=1e-07, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_attention_heads=16, tokenizer='moses', bpe='subword_nmt', bpe_codes='/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', task='translation', stop_min_lr=1e-09, _name='transformer_vaswani_wmt_en_de_big', encoder_embed_path=None, decoder_embed_path=None, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0), 'task': {'_name': 'translation', 'data': '/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae', 'source_lang': 'en', 'target_lang': 'fr', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0007]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0007]}, 'scoring': None, 'bpe': {'_name': 'subword_nmt', 'bpe_codes': '/root/.cache/torch/pytorch_fairseq/53f403ba27ab138b06c1a8d78f5bb4f1722567ac3d3b3e41f821ec2cae2974da.7ef8ab763efda16d3c82dd8b5a574bdfe524e078bac7b444ea1a9c5d355b55ae/bpecodes', 'bpe_separator': '@@'}, 'tokenizer': {'_name': 'moses', 'source_lang': 'en', 'target_lang': 'fr', 'moses_no_dash_splits': False, 'moses_no_escape': False}, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator_teacher_logits shape: torch.Size([1, 5, 44512])\n",
      "generator_student_logits shape: torch.Size([128, 59514])\n",
      "After: generator_teacher_logits shape: torch.Size([1, 128, 59514])\n",
      "After: generator_student_logits shape: torch.Size([1, 128, 59514])\n",
      "generator_teacher_logits shape: torch.Size([1, 39, 44512])\n",
      "generator_student_logits shape: torch.Size([128, 59514])\n",
      "After: generator_teacher_logits shape: torch.Size([1, 128, 59514])\n",
      "After: generator_student_logits shape: torch.Size([1, 128, 59514])\n",
      "generator_teacher_logits shape: torch.Size([1, 31, 44512])\n",
      "generator_student_logits shape: torch.Size([128, 59514])\n",
      "After: generator_teacher_logits shape: torch.Size([1, 128, 59514])\n",
      "After: generator_student_logits shape: torch.Size([1, 128, 59514])\n",
      "generator_teacher_logits shape: torch.Size([1, 20, 44512])\n",
      "generator_student_logits shape: torch.Size([128, 59514])\n",
      "After: generator_teacher_logits shape: torch.Size([1, 128, 59514])\n",
      "After: generator_student_logits shape: torch.Size([1, 128, 59514])\n",
      "generator_teacher_logits shape: torch.Size([1, 41, 44512])\n",
      "generator_student_logits shape: torch.Size([128, 59514])\n",
      "After: generator_teacher_logits shape: torch.Size([1, 128, 59514])\n",
      "After: generator_student_logits shape: torch.Size([1, 128, 59514])\n",
      "generator_teacher_logits shape: torch.Size([1, 9, 44512])\n",
      "generator_student_logits shape: torch.Size([128, 59514])\n",
      "After: generator_teacher_logits shape: torch.Size([1, 128, 59514])\n",
      "After: generator_student_logits shape: torch.Size([1, 128, 59514])\n",
      "generator_teacher_logits shape: torch.Size([1, 10, 44512])\n",
      "generator_student_logits shape: torch.Size([128, 59514])\n",
      "After: generator_teacher_logits shape: torch.Size([1, 128, 59514])\n",
      "After: generator_student_logits shape: torch.Size([1, 128, 59514])\n",
      "generator_teacher_logits shape: torch.Size([1, 8, 44512])\n",
      "generator_student_logits shape: torch.Size([128, 59514])\n",
      "After: generator_teacher_logits shape: torch.Size([1, 128, 59514])\n",
      "After: generator_student_logits shape: torch.Size([1, 128, 59514])\n",
      "generator_teacher_logits shape: torch.Size([1, 24, 44512])\n",
      "generator_student_logits shape: torch.Size([128, 59514])\n",
      "After: generator_teacher_logits shape: torch.Size([1, 128, 59514])\n",
      "After: generator_student_logits shape: torch.Size([1, 128, 59514])\n",
      "generator_teacher_logits shape: torch.Size([1, 26, 44512])\n",
      "generator_student_logits shape: torch.Size([128, 59514])\n",
      "After: generator_teacher_logits shape: torch.Size([1, 128, 59514])\n",
      "After: generator_student_logits shape: torch.Size([1, 128, 59514])\n",
      "Reverse KLD for sentence 1: 10482622.0\n",
      "Reverse KLD for sentence 2: 8045074.0\n",
      "Reverse KLD for sentence 3: 8543807.0\n",
      "Reverse KLD for sentence 4: 8815616.0\n",
      "Reverse KLD for sentence 5: 8058681.0\n",
      "Reverse KLD for sentence 6: 9984025.0\n",
      "Reverse KLD for sentence 7: 9784019.0\n",
      "Reverse KLD for sentence 8: 10813238.0\n",
      "Reverse KLD for sentence 9: 8993316.0\n",
      "Reverse KLD for sentence 10: 8560628.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Fairseq logits extraction function\n",
    "# def extract_fairseq_logits_old(fairseq_hub, tokenized_train_datasets):\n",
    "#     fairseq_model = fairseq_hub.models[0]\n",
    "#     source_dictionary = fairseq_hub.task.source_dictionary\n",
    "    \n",
    "#     fairseq_model.eval()\n",
    "#     fairseq_logits_list = []\n",
    "#     with torch.no_grad():\n",
    "#         for tokens in tokenized_train_datasets[\"input_ids\"]:\n",
    "#             tokens = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
    "#             src_lengths = torch.tensor([tokens.size(1)])  # Sequence length\n",
    "            \n",
    "#             encoder_out = fairseq_model.encoder(tokens, src_lengths=src_lengths)\n",
    "#             bos_token = fairseq_hub.task.target_dictionary.bos()\n",
    "#             prev_output_tokens = torch.full((1, 1), bos_token, dtype=torch.long)\n",
    "            \n",
    "#             decoder_logits = fairseq_model.decoder(prev_output_tokens, encoder_out=encoder_out)\n",
    "#             fairseq_logits_list.append(F.log_softmax(decoder_logits[0], dim=-1))\n",
    "#     return fairseq_logits_list\n",
    "\n",
    "# # Fairseq logits extraction function\n",
    "# def extract_fairseq_logits_v0(fairseq_hub, tokenized_train_datasets):\n",
    "#     fairseq_model = fairseq_hub.models[0]\n",
    "#     source_dictionary = fairseq_hub.task.source_dictionary\n",
    "    \n",
    "#     fairseq_model.eval()\n",
    "#     fairseq_logits_list = []\n",
    "#     with torch.no_grad():\n",
    "#         for tokens in tokenized_train_datasets[\"input_ids\"]:\n",
    "#             # Use Fairseq dictionary to encode tokens\n",
    "#             tokens = torch.tensor(\n",
    "#                 [source_dictionary.encode_line(tokenizer.decode(t), add_if_not_exist=False).long() for t in tokens]\n",
    "#             ).unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "#             src_lengths = torch.tensor([tokens.size(1)])  # Sequence length\n",
    "            \n",
    "#             encoder_out = fairseq_model.encoder(tokens, src_lengths=src_lengths)\n",
    "#             bos_token = fairseq_hub.task.target_dictionary.bos()\n",
    "#             prev_output_tokens = torch.full((1, 1), bos_token, dtype=torch.long)\n",
    "            \n",
    "#             decoder_logits = fairseq_model.decoder(prev_output_tokens, encoder_out=encoder_out)\n",
    "#             fairseq_logits_list.append(F.log_softmax(decoder_logits[0], dim=-1))\n",
    "#     return fairseq_logits_list\n",
    "\n",
    "# checkpoint = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def ids_to_sentences_bert(input_ids):\n",
    "        \"\"\"\n",
    "        Converts lists of token IDs back into sentences using the BERT tokenizer.\n",
    "\n",
    "        Args:\n",
    "            input_ids (list[list[int]]): A list of lists containing token IDs.\n",
    "            tokenizer (BertTokenizerFast): An instance of BertTokenizerFast used for decoding.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of decoded sentences.\n",
    "        \"\"\"\n",
    "        # # tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        # from transformers import AutoTokenizer\n",
    "\n",
    "        # # checkpoint = \"google-t5/t5-small\"\n",
    "        # # checkpoint = 'sriram-sanjeev9s/T5_base_wmt14_En_Fr_1million'\n",
    "        # tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "        sentences = []\n",
    "\n",
    "        # Convert each list of token IDs back into a sentence\n",
    "        for ids in input_ids:\n",
    "            # Decode the token IDs to a sentence, skipping special tokens\n",
    "            sentence = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "            # sentence = tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=True)\n",
    "            sentence = sentence.replace(\"▁\", \" \").strip()\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "# Fairseq logits extraction function\n",
    "# def extract_fairseq_logits(fairseq_hub, train_dataset):\n",
    "    # fairseq_model = fairseq_hub.models[0]\n",
    "    # source_dictionary = fairseq_hub.task.source_dictionary\n",
    "\n",
    "    # fairseq_model.eval()\n",
    "    # fairseq_logits_list = []\n",
    "    # with torch.no_grad():\n",
    "    #     for example in train_dataset:\n",
    "    #         # Extract raw source text\n",
    "    #         raw_text = example[\"translation\"][\"en\"]\n",
    "\n",
    "    #         # Encode using Fairseq source dictionary\n",
    "    #         tokens = source_dictionary.encode_line(raw_text, add_if_not_exist=False).long().unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "    #         src_lengths = torch.tensor([tokens.size(1)])  # Sequence length\n",
    "            \n",
    "    #         encoder_out = fairseq_model.encoder(tokens, src_lengths=src_lengths)\n",
    "    #         bos_token = fairseq_hub.task.target_dictionary.bos()\n",
    "    #         prev_output_tokens = torch.full((1, 1), bos_token, dtype=torch.long)\n",
    "            \n",
    "    #         decoder_logits = fairseq_model.decoder(prev_output_tokens, encoder_out=encoder_out)\n",
    "    #         fairseq_logits_list.append(F.log_softmax(decoder_logits[0], dim=-1))\n",
    "    # return fairseq_logits_list\n",
    "\n",
    "def extract_fairseq_logits(fairseq_hub, train_dataset, src_sentences_for_G1):\n",
    "    fairseq_model = fairseq_hub.models[0]\n",
    "    source_dictionary = fairseq_hub.task.source_dictionary\n",
    "\n",
    "    fairseq_model.eval()\n",
    "    fairseq_logits_list = []\n",
    "    with torch.no_grad():\n",
    "        for raw_text in src_sentences_for_G1:\n",
    "            # Encode using Fairseq source dictionary\n",
    "            tokens = source_dictionary.encode_line(raw_text, add_if_not_exist=False).long().unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "            src_lengths = torch.tensor([tokens.size(1)])  # Sequence length\n",
    "\n",
    "            encoder_out = fairseq_model.encoder(tokens, src_lengths=src_lengths)\n",
    "            bos_token = fairseq_hub.task.target_dictionary.bos()\n",
    "            prev_output_tokens = torch.full((1, 1), bos_token, dtype=torch.long)\n",
    "            \n",
    "            # Autoregressively generate logits for each token\n",
    "            logits = []\n",
    "            for _ in range(tokens.size(1)):\n",
    "                decoder_output = fairseq_model.decoder(prev_output_tokens, encoder_out=encoder_out)\n",
    "                # Get the logits for the last predicted token\n",
    "                last_token_logits = F.log_softmax(decoder_output[0][:, -1, :], dim=-1)\n",
    "                logits.append(last_token_logits.unsqueeze(1))\n",
    "                # Append the most probable token to prev_output_tokens\n",
    "                next_token = last_token_logits.argmax(dim=-1, keepdim=True)\n",
    "                prev_output_tokens = torch.cat([prev_output_tokens, next_token], dim=1)\n",
    "\n",
    "            \n",
    "            \n",
    "            fairseq_logits_list.append(torch.cat(logits, dim=1))  # Shape: (1, seq_len, vocab_size)\n",
    "            \n",
    "    return fairseq_logits_list\n",
    "\n",
    "\n",
    "# Marian logits extraction function\n",
    "def extract_marian_logits(generator2_train_dp, tokenized_train_datasets):\n",
    "    generator2_train_dp = generator2_train_dp.module if isinstance(generator2_train_dp, torch.nn.DataParallel) else generator2_train_dp\n",
    "\n",
    "    generator2_train_out = generator2_train_dp(\n",
    "        input_ids=torch.tensor(tokenized_train_datasets[\"input_ids\"]),\n",
    "        attention_mask=torch.tensor(tokenized_train_datasets[\"attention_mask\"]),\n",
    "        decoder_input_ids=torch.tensor(tokenized_train_datasets[\"target_ids\"]),\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "    \n",
    "    fake_tgt_sentences_probs = F.log_softmax(generator2_train_out.logits, dim=-1)\n",
    "    # fake_tgt_sentences_probs = fake_tgt_sentences_probs.view(-1, fake_tgt_sentences_probs.size(-1))\n",
    "    return fake_tgt_sentences_probs\n",
    "\n",
    "# Reverse KLD Calculation\n",
    "# def compute_reverse_kld_v0(fairseq_logits_list, marian_logits_list):\n",
    "#     reverse_kld = []\n",
    "#     for fs_logits, marian_logits in zip(fairseq_logits_list, marian_logits_list):\n",
    "#         # Adjust shaoes if necessary\n",
    "#         fs_logits = fs_logits.squeeze(0)\n",
    "#         marian_logits = marian_logits.squeeze(0)\n",
    "        \n",
    "#         min_length = min(fs_logits.size(0), marian_logits.size(1))\n",
    "#         fs_logits = fs_logits[:, :min_length, :]\n",
    "#         marian_logits = marian_logits[:, :min_length, :]\n",
    "\n",
    "#         r_kld = torch.sum(marian_logits * (marian_logits - fs_logits), dim=-1).mean()\n",
    "#         reverse_kld.append(r_kld)\n",
    "#     return reverse_kld\n",
    "\n",
    "def compute_reverse_kld(generator_teacher_logits_list, generator_student_logits_list, fs_vocab_size, marian_vocab_size, max_length=128):\n",
    "    reverse_kld = []\n",
    "    for generator_teacher_logits, generator_student_logits in zip(generator_teacher_logits_list, generator_student_logits_list):\n",
    "        print(f\"generator_teacher_logits shape: {generator_teacher_logits.shape}\")\n",
    "        print(f\"generator_student_logits shape: {generator_student_logits.shape}\")\n",
    "        \n",
    "        if len(generator_student_logits.shape)==2 :\n",
    "            generator_student_logits = generator_student_logits.unsqueeze(0)\n",
    "        \n",
    "        # Adjust to the same vocabulary size\n",
    "        if fs_vocab_size < marian_vocab_size:\n",
    "            generator_teacher_logits = torch.nn.functional.pad(generator_teacher_logits, (0, marian_vocab_size - fs_vocab_size), value=0.0)\n",
    "        elif fs_vocab_size > marian_vocab_size:\n",
    "            generator_student_logits = torch.nn.functional.pad(generator_student_logits, (0, fs_vocab_size - marian_vocab_size), value=0.0)\n",
    "        \n",
    "        # fs_logits = fs_logits.squeeze(0)  # Remove batch dimension for Fairseq\n",
    "        \n",
    "        # min_length = min(fs_logits.size(0), marian_logits.size(1))\n",
    "        # fs_logits = fs_logits[:min_length, :]  # Align sequence length\n",
    "        # marian_logits = marian_logits[:min_length, :]\n",
    "        \n",
    "        # Align sequence length (if required)\n",
    "        # min_length = min(generator_teacher_logits.size(1), generator_student_logits_list.size(1))\n",
    "        # generator_teacher_logits = generator_teacher_logits[:, :min_length, :]\n",
    "        # generator_student_logits_list = generator_student_logits_list[:, :min_length, :]\n",
    "\n",
    "         # Pad generator_teacher_logits to the student's max sequence length\n",
    "        teacher_seq_length = generator_teacher_logits.size(1)\n",
    "        if teacher_seq_length < max_length:\n",
    "            generator_teacher_logits = F.pad(generator_teacher_logits, (0, 0, 0, max_length - teacher_seq_length), value=0.0)\n",
    "        \n",
    "        # Truncate generator_student_logits to the teacher's sequence length\n",
    "        student_seq_length = generator_student_logits.size(1)\n",
    "        if student_seq_length > max_length:\n",
    "            generator_student_logits = generator_student_logits[:, :max_length, :]\n",
    "        \n",
    "        \n",
    "        r_kld = torch.sum(generator_student_logits * (generator_student_logits - generator_teacher_logits), dim=-1).mean()\n",
    "        reverse_kld.append(r_kld)\n",
    "        \n",
    "        print(f\"After: generator_teacher_logits shape: {generator_teacher_logits.shape}\")\n",
    "        print(f\"After: generator_student_logits shape: {generator_student_logits.shape}\")\n",
    "    return reverse_kld\n",
    "\n",
    "\n",
    "generator_teacher_hub = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n",
    "src_sentences_for_generator_teacher = ids_to_sentences_bert(tokenized_train_datasets['input_ids'])\n",
    "generator_teacher_logits_list = extract_fairseq_logits(generator_teacher_hub, train_dataset, src_sentences_for_generator_teacher)\n",
    "\n",
    "# Loading generator-student\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# checkpoint = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "generator_student = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "generator_student_logits_list = extract_marian_logits(generator_student, tokenized_train_datasets)\n",
    "\n",
    "fs_vocab_size = len(generator_teacher_hub.task.source_dictionary) #44512   # Fairseq vocabulary size\n",
    "marian_vocab_size = generator_student.config.vocab_size #59514  # Marian vocabulary size\n",
    "\n",
    "reverse_kld_values = compute_reverse_kld(generator_teacher_logits_list, generator_student_logits_list, fs_vocab_size, marian_vocab_size)\n",
    "\n",
    "for idx, r_kld in enumerate(reverse_kld_values):\n",
    "    print(f\"Reverse KLD for sentence {idx + 1}: {r_kld.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers updation prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/preprocess_bert_udem/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "max_length = 128\n",
    "# bert_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "checkpoint = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "generator_student = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "# from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name :  model.shared.weight\n",
      "name :  model.encoder.embed_positions.weight\n",
      "name :  model.encoder.layers.0.self_attn.k_proj.weight\n",
      "name :  model.encoder.layers.0.self_attn.k_proj.bias\n",
      "name :  model.encoder.layers.0.self_attn.v_proj.weight\n",
      "name :  model.encoder.layers.0.self_attn.v_proj.bias\n",
      "name :  model.encoder.layers.0.self_attn.q_proj.weight\n",
      "name :  model.encoder.layers.0.self_attn.q_proj.bias\n",
      "name :  model.encoder.layers.0.self_attn.out_proj.weight\n",
      "name :  model.encoder.layers.0.self_attn.out_proj.bias\n",
      "name :  model.encoder.layers.0.self_attn_layer_norm.weight\n",
      "name :  model.encoder.layers.0.self_attn_layer_norm.bias\n",
      "name :  model.encoder.layers.0.fc1.weight\n",
      "name :  model.encoder.layers.0.fc1.bias\n",
      "name :  model.encoder.layers.0.fc2.weight\n",
      "name :  model.encoder.layers.0.fc2.bias\n",
      "name :  model.encoder.layers.0.final_layer_norm.weight\n",
      "name :  model.encoder.layers.0.final_layer_norm.bias\n",
      "name :  model.encoder.layers.1.self_attn.k_proj.weight\n",
      "name :  model.encoder.layers.1.self_attn.k_proj.bias\n",
      "name :  model.encoder.layers.1.self_attn.v_proj.weight\n",
      "name :  model.encoder.layers.1.self_attn.v_proj.bias\n",
      "name :  model.encoder.layers.1.self_attn.q_proj.weight\n",
      "name :  model.encoder.layers.1.self_attn.q_proj.bias\n",
      "name :  model.encoder.layers.1.self_attn.out_proj.weight\n",
      "name :  model.encoder.layers.1.self_attn.out_proj.bias\n",
      "name :  model.encoder.layers.1.self_attn_layer_norm.weight\n",
      "name :  model.encoder.layers.1.self_attn_layer_norm.bias\n",
      "name :  model.encoder.layers.1.fc1.weight\n",
      "name :  model.encoder.layers.1.fc1.bias\n",
      "name :  model.encoder.layers.1.fc2.weight\n",
      "name :  model.encoder.layers.1.fc2.bias\n",
      "name :  model.encoder.layers.1.final_layer_norm.weight\n",
      "name :  model.encoder.layers.1.final_layer_norm.bias\n",
      "name :  model.encoder.layers.2.self_attn.k_proj.weight\n",
      "name :  model.encoder.layers.2.self_attn.k_proj.bias\n",
      "name :  model.encoder.layers.2.self_attn.v_proj.weight\n",
      "name :  model.encoder.layers.2.self_attn.v_proj.bias\n",
      "name :  model.encoder.layers.2.self_attn.q_proj.weight\n",
      "name :  model.encoder.layers.2.self_attn.q_proj.bias\n",
      "name :  model.encoder.layers.2.self_attn.out_proj.weight\n",
      "name :  model.encoder.layers.2.self_attn.out_proj.bias\n",
      "name :  model.encoder.layers.2.self_attn_layer_norm.weight\n",
      "name :  model.encoder.layers.2.self_attn_layer_norm.bias\n",
      "name :  model.encoder.layers.2.fc1.weight\n",
      "name :  model.encoder.layers.2.fc1.bias\n",
      "name :  model.encoder.layers.2.fc2.weight\n",
      "name :  model.encoder.layers.2.fc2.bias\n",
      "name :  model.encoder.layers.2.final_layer_norm.weight\n",
      "name :  model.encoder.layers.2.final_layer_norm.bias\n",
      "name :  model.encoder.layers.3.self_attn.k_proj.weight\n",
      "name :  model.encoder.layers.3.self_attn.k_proj.bias\n",
      "name :  model.encoder.layers.3.self_attn.v_proj.weight\n",
      "name :  model.encoder.layers.3.self_attn.v_proj.bias\n",
      "name :  model.encoder.layers.3.self_attn.q_proj.weight\n",
      "name :  model.encoder.layers.3.self_attn.q_proj.bias\n",
      "name :  model.encoder.layers.3.self_attn.out_proj.weight\n",
      "name :  model.encoder.layers.3.self_attn.out_proj.bias\n",
      "name :  model.encoder.layers.3.self_attn_layer_norm.weight\n",
      "name :  model.encoder.layers.3.self_attn_layer_norm.bias\n",
      "name :  model.encoder.layers.3.fc1.weight\n",
      "name :  model.encoder.layers.3.fc1.bias\n",
      "name :  model.encoder.layers.3.fc2.weight\n",
      "name :  model.encoder.layers.3.fc2.bias\n",
      "name :  model.encoder.layers.3.final_layer_norm.weight\n",
      "name :  model.encoder.layers.3.final_layer_norm.bias\n",
      "name :  model.encoder.layers.4.self_attn.k_proj.weight\n",
      "name :  model.encoder.layers.4.self_attn.k_proj.bias\n",
      "name :  model.encoder.layers.4.self_attn.v_proj.weight\n",
      "name :  model.encoder.layers.4.self_attn.v_proj.bias\n",
      "name :  model.encoder.layers.4.self_attn.q_proj.weight\n",
      "name :  model.encoder.layers.4.self_attn.q_proj.bias\n",
      "name :  model.encoder.layers.4.self_attn.out_proj.weight\n",
      "name :  model.encoder.layers.4.self_attn.out_proj.bias\n",
      "name :  model.encoder.layers.4.self_attn_layer_norm.weight\n",
      "name :  model.encoder.layers.4.self_attn_layer_norm.bias\n",
      "name :  model.encoder.layers.4.fc1.weight\n",
      "name :  model.encoder.layers.4.fc1.bias\n",
      "name :  model.encoder.layers.4.fc2.weight\n",
      "name :  model.encoder.layers.4.fc2.bias\n",
      "name :  model.encoder.layers.4.final_layer_norm.weight\n",
      "name :  model.encoder.layers.4.final_layer_norm.bias\n",
      "name :  model.encoder.layers.5.self_attn.k_proj.weight\n",
      "name :  model.encoder.layers.5.self_attn.k_proj.bias\n",
      "name :  model.encoder.layers.5.self_attn.v_proj.weight\n",
      "name :  model.encoder.layers.5.self_attn.v_proj.bias\n",
      "name :  model.encoder.layers.5.self_attn.q_proj.weight\n",
      "name :  model.encoder.layers.5.self_attn.q_proj.bias\n",
      "name :  model.encoder.layers.5.self_attn.out_proj.weight\n",
      "name :  model.encoder.layers.5.self_attn.out_proj.bias\n",
      "name :  model.encoder.layers.5.self_attn_layer_norm.weight\n",
      "name :  model.encoder.layers.5.self_attn_layer_norm.bias\n",
      "name :  model.encoder.layers.5.fc1.weight\n",
      "name :  model.encoder.layers.5.fc1.bias\n",
      "name :  model.encoder.layers.5.fc2.weight\n",
      "name :  model.encoder.layers.5.fc2.bias\n",
      "name :  model.encoder.layers.5.final_layer_norm.weight\n",
      "name :  model.encoder.layers.5.final_layer_norm.bias\n",
      "name :  model.decoder.embed_positions.weight\n",
      "name :  model.decoder.layers.0.self_attn.k_proj.weight\n",
      "name :  model.decoder.layers.0.self_attn.k_proj.bias\n",
      "name :  model.decoder.layers.0.self_attn.v_proj.weight\n",
      "name :  model.decoder.layers.0.self_attn.v_proj.bias\n",
      "name :  model.decoder.layers.0.self_attn.q_proj.weight\n",
      "name :  model.decoder.layers.0.self_attn.q_proj.bias\n",
      "name :  model.decoder.layers.0.self_attn.out_proj.weight\n",
      "name :  model.decoder.layers.0.self_attn.out_proj.bias\n",
      "name :  model.decoder.layers.0.self_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.0.self_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.0.encoder_attn.k_proj.weight\n",
      "name :  model.decoder.layers.0.encoder_attn.k_proj.bias\n",
      "name :  model.decoder.layers.0.encoder_attn.v_proj.weight\n",
      "name :  model.decoder.layers.0.encoder_attn.v_proj.bias\n",
      "name :  model.decoder.layers.0.encoder_attn.q_proj.weight\n",
      "name :  model.decoder.layers.0.encoder_attn.q_proj.bias\n",
      "name :  model.decoder.layers.0.encoder_attn.out_proj.weight\n",
      "name :  model.decoder.layers.0.encoder_attn.out_proj.bias\n",
      "name :  model.decoder.layers.0.encoder_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.0.encoder_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.0.fc1.weight\n",
      "name :  model.decoder.layers.0.fc1.bias\n",
      "name :  model.decoder.layers.0.fc2.weight\n",
      "name :  model.decoder.layers.0.fc2.bias\n",
      "name :  model.decoder.layers.0.final_layer_norm.weight\n",
      "name :  model.decoder.layers.0.final_layer_norm.bias\n",
      "name :  model.decoder.layers.1.self_attn.k_proj.weight\n",
      "name :  model.decoder.layers.1.self_attn.k_proj.bias\n",
      "name :  model.decoder.layers.1.self_attn.v_proj.weight\n",
      "name :  model.decoder.layers.1.self_attn.v_proj.bias\n",
      "name :  model.decoder.layers.1.self_attn.q_proj.weight\n",
      "name :  model.decoder.layers.1.self_attn.q_proj.bias\n",
      "name :  model.decoder.layers.1.self_attn.out_proj.weight\n",
      "name :  model.decoder.layers.1.self_attn.out_proj.bias\n",
      "name :  model.decoder.layers.1.self_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.1.self_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.1.encoder_attn.k_proj.weight\n",
      "name :  model.decoder.layers.1.encoder_attn.k_proj.bias\n",
      "name :  model.decoder.layers.1.encoder_attn.v_proj.weight\n",
      "name :  model.decoder.layers.1.encoder_attn.v_proj.bias\n",
      "name :  model.decoder.layers.1.encoder_attn.q_proj.weight\n",
      "name :  model.decoder.layers.1.encoder_attn.q_proj.bias\n",
      "name :  model.decoder.layers.1.encoder_attn.out_proj.weight\n",
      "name :  model.decoder.layers.1.encoder_attn.out_proj.bias\n",
      "name :  model.decoder.layers.1.encoder_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.1.encoder_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.1.fc1.weight\n",
      "name :  model.decoder.layers.1.fc1.bias\n",
      "name :  model.decoder.layers.1.fc2.weight\n",
      "name :  model.decoder.layers.1.fc2.bias\n",
      "name :  model.decoder.layers.1.final_layer_norm.weight\n",
      "name :  model.decoder.layers.1.final_layer_norm.bias\n",
      "name :  model.decoder.layers.2.self_attn.k_proj.weight\n",
      "name :  model.decoder.layers.2.self_attn.k_proj.bias\n",
      "name :  model.decoder.layers.2.self_attn.v_proj.weight\n",
      "name :  model.decoder.layers.2.self_attn.v_proj.bias\n",
      "name :  model.decoder.layers.2.self_attn.q_proj.weight\n",
      "name :  model.decoder.layers.2.self_attn.q_proj.bias\n",
      "name :  model.decoder.layers.2.self_attn.out_proj.weight\n",
      "name :  model.decoder.layers.2.self_attn.out_proj.bias\n",
      "name :  model.decoder.layers.2.self_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.2.self_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.2.encoder_attn.k_proj.weight\n",
      "name :  model.decoder.layers.2.encoder_attn.k_proj.bias\n",
      "name :  model.decoder.layers.2.encoder_attn.v_proj.weight\n",
      "name :  model.decoder.layers.2.encoder_attn.v_proj.bias\n",
      "name :  model.decoder.layers.2.encoder_attn.q_proj.weight\n",
      "name :  model.decoder.layers.2.encoder_attn.q_proj.bias\n",
      "name :  model.decoder.layers.2.encoder_attn.out_proj.weight\n",
      "name :  model.decoder.layers.2.encoder_attn.out_proj.bias\n",
      "name :  model.decoder.layers.2.encoder_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.2.encoder_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.2.fc1.weight\n",
      "name :  model.decoder.layers.2.fc1.bias\n",
      "name :  model.decoder.layers.2.fc2.weight\n",
      "name :  model.decoder.layers.2.fc2.bias\n",
      "name :  model.decoder.layers.2.final_layer_norm.weight\n",
      "name :  model.decoder.layers.2.final_layer_norm.bias\n",
      "name :  model.decoder.layers.3.self_attn.k_proj.weight\n",
      "name :  model.decoder.layers.3.self_attn.k_proj.bias\n",
      "name :  model.decoder.layers.3.self_attn.v_proj.weight\n",
      "name :  model.decoder.layers.3.self_attn.v_proj.bias\n",
      "name :  model.decoder.layers.3.self_attn.q_proj.weight\n",
      "name :  model.decoder.layers.3.self_attn.q_proj.bias\n",
      "name :  model.decoder.layers.3.self_attn.out_proj.weight\n",
      "name :  model.decoder.layers.3.self_attn.out_proj.bias\n",
      "name :  model.decoder.layers.3.self_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.3.self_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.3.encoder_attn.k_proj.weight\n",
      "name :  model.decoder.layers.3.encoder_attn.k_proj.bias\n",
      "name :  model.decoder.layers.3.encoder_attn.v_proj.weight\n",
      "name :  model.decoder.layers.3.encoder_attn.v_proj.bias\n",
      "name :  model.decoder.layers.3.encoder_attn.q_proj.weight\n",
      "name :  model.decoder.layers.3.encoder_attn.q_proj.bias\n",
      "name :  model.decoder.layers.3.encoder_attn.out_proj.weight\n",
      "name :  model.decoder.layers.3.encoder_attn.out_proj.bias\n",
      "name :  model.decoder.layers.3.encoder_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.3.encoder_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.3.fc1.weight\n",
      "name :  model.decoder.layers.3.fc1.bias\n",
      "name :  model.decoder.layers.3.fc2.weight\n",
      "name :  model.decoder.layers.3.fc2.bias\n",
      "name :  model.decoder.layers.3.final_layer_norm.weight\n",
      "name :  model.decoder.layers.3.final_layer_norm.bias\n",
      "name :  model.decoder.layers.4.self_attn.k_proj.weight\n",
      "name :  model.decoder.layers.4.self_attn.k_proj.bias\n",
      "name :  model.decoder.layers.4.self_attn.v_proj.weight\n",
      "name :  model.decoder.layers.4.self_attn.v_proj.bias\n",
      "name :  model.decoder.layers.4.self_attn.q_proj.weight\n",
      "name :  model.decoder.layers.4.self_attn.q_proj.bias\n",
      "name :  model.decoder.layers.4.self_attn.out_proj.weight\n",
      "name :  model.decoder.layers.4.self_attn.out_proj.bias\n",
      "name :  model.decoder.layers.4.self_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.4.self_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.4.encoder_attn.k_proj.weight\n",
      "name :  model.decoder.layers.4.encoder_attn.k_proj.bias\n",
      "name :  model.decoder.layers.4.encoder_attn.v_proj.weight\n",
      "name :  model.decoder.layers.4.encoder_attn.v_proj.bias\n",
      "name :  model.decoder.layers.4.encoder_attn.q_proj.weight\n",
      "name :  model.decoder.layers.4.encoder_attn.q_proj.bias\n",
      "name :  model.decoder.layers.4.encoder_attn.out_proj.weight\n",
      "name :  model.decoder.layers.4.encoder_attn.out_proj.bias\n",
      "name :  model.decoder.layers.4.encoder_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.4.encoder_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.4.fc1.weight\n",
      "name :  model.decoder.layers.4.fc1.bias\n",
      "name :  model.decoder.layers.4.fc2.weight\n",
      "name :  model.decoder.layers.4.fc2.bias\n",
      "name :  model.decoder.layers.4.final_layer_norm.weight\n",
      "name :  model.decoder.layers.4.final_layer_norm.bias\n",
      "name :  model.decoder.layers.5.self_attn.k_proj.weight\n",
      "name :  model.decoder.layers.5.self_attn.k_proj.bias\n",
      "name :  model.decoder.layers.5.self_attn.v_proj.weight\n",
      "name :  model.decoder.layers.5.self_attn.v_proj.bias\n",
      "name :  model.decoder.layers.5.self_attn.q_proj.weight\n",
      "name :  model.decoder.layers.5.self_attn.q_proj.bias\n",
      "name :  model.decoder.layers.5.self_attn.out_proj.weight\n",
      "name :  model.decoder.layers.5.self_attn.out_proj.bias\n",
      "name :  model.decoder.layers.5.self_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.5.self_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.5.encoder_attn.k_proj.weight\n",
      "name :  model.decoder.layers.5.encoder_attn.k_proj.bias\n",
      "name :  model.decoder.layers.5.encoder_attn.v_proj.weight\n",
      "name :  model.decoder.layers.5.encoder_attn.v_proj.bias\n",
      "name :  model.decoder.layers.5.encoder_attn.q_proj.weight\n",
      "name :  model.decoder.layers.5.encoder_attn.q_proj.bias\n",
      "name :  model.decoder.layers.5.encoder_attn.out_proj.weight\n",
      "name :  model.decoder.layers.5.encoder_attn.out_proj.bias\n",
      "name :  model.decoder.layers.5.encoder_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.5.encoder_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.5.fc1.weight\n",
      "name :  model.decoder.layers.5.fc1.bias\n",
      "name :  model.decoder.layers.5.fc2.weight\n",
      "name :  model.decoder.layers.5.fc2.bias\n",
      "name :  model.decoder.layers.5.final_layer_norm.weight\n",
      "name :  model.decoder.layers.5.final_layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in generator_student.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    print(\"name : \", name)\n",
    "    # print(\"param : \", param)\n",
    "\n",
    "# # Enable requires_grad only for bias terms\n",
    "#     if 'bias' in name:\n",
    "#         param.requires_grad = config['gradient_update']['BIAS'] #True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param :  Parameter containing:\n",
      "tensor([[ 0.0011,  0.0069, -0.0142,  ...,  0.0068, -0.0423, -0.0321],\n",
      "        [ 0.0535,  0.0125,  0.0061,  ..., -0.0350, -0.0401, -0.0082],\n",
      "        [ 0.0129,  0.0213,  0.0095,  ..., -0.0304, -0.0643, -0.0291],\n",
      "        ...,\n",
      "        [ 0.0773,  0.0246,  0.0176,  ..., -0.0395, -0.0363, -0.0033],\n",
      "        [ 0.0738,  0.0227,  0.0156,  ..., -0.0419, -0.0389, -0.0047],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "for param in generator_student.lm_head.parameters():\n",
    "    print(\"param : \", param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config =[    { \"combination\" : \"G_0_0_0_0_1_cos_kl_pg_rkldlgts_0_0_0_D_x_to_1_mil_Bias_F_LM_T_PGloss_1_2_upd_bs_40_0PG_100rkld_lgts_00001lr_ep_5\",\n",
    "    \"total_g_loss\" : {\"g_loss\":0.00, \"g_cosine_loss\":00.00,\"g_kl_loss\":0.00, \"g_pg_loss\":0, \"g_rkld_logits\":100}, \n",
    "    \"d_loss\" : {\"real_loss\":0.0, \"fake_loss\":0.0, \"fake_loss_pretrain\":0.0},\n",
    "    \"gradient_update\": {\"BIAS\": False, \"LM\": True}, \n",
    "    \"Dataset\":{\"train_size\":1000},\n",
    "    \"Mscll\":{\"Comments\": \"C : This is the combination with ALL layers updating and LM layer updating\"}\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "getpwd = os.getcwd()\n",
    "checkpoint_path_generator = os.path.join(getpwd, \"checkpoints\", \"bert_dualG\", \"wmt14_en_fr_1mil_pg_kd_loss_MarianMT_unfreezeonlylmlayer_debug_Normalkd_comb_\" + config[0]['combination'] +'_save_open_direct_pretrained'+'/train_checkpoint_generator_save_pretrained_at_3')\n",
    "checkpoint_path_tokenizer = os.path.join(getpwd, \"checkpoints\", \"bert_dualG\", \"wmt14_en_fr_1mil_pg_kd_loss_MarianMT_unfreezeonlylmlayer_debug_Normalkd_comb_\" + config[0]['combination'] +'_save_open_direct_pretrained'+'/train_checkpoint_tokenizer_save_pretrained_at_3')\n",
    "\n",
    "# Load the entire model directly\n",
    "# generator2_checkpoint = torch.load(open(checkpoint_path_generator, \"rb\"), pickle_module=dill)\n",
    "generator2_checkpoint = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path_generator)\n",
    "\n",
    "# generator2_train # Extract the underlying model from the DataParallel wrapper\n",
    "generator2_checkpoint = generator2_checkpoint.module if isinstance(generator2_checkpoint, torch.nn.DataParallel) else generator2_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name :  model.shared.weight\n",
      "name :  model.encoder.embed_positions.weight\n",
      "name :  model.encoder.layers.0.self_attn.k_proj.weight\n",
      "name :  model.encoder.layers.0.self_attn.k_proj.bias\n",
      "name :  model.encoder.layers.0.self_attn.v_proj.weight\n",
      "name :  model.encoder.layers.0.self_attn.v_proj.bias\n",
      "name :  model.encoder.layers.0.self_attn.q_proj.weight\n",
      "name :  model.encoder.layers.0.self_attn.q_proj.bias\n",
      "name :  model.encoder.layers.0.self_attn.out_proj.weight\n",
      "name :  model.encoder.layers.0.self_attn.out_proj.bias\n",
      "name :  model.encoder.layers.0.self_attn_layer_norm.weight\n",
      "name :  model.encoder.layers.0.self_attn_layer_norm.bias\n",
      "name :  model.encoder.layers.0.fc1.weight\n",
      "name :  model.encoder.layers.0.fc1.bias\n",
      "name :  model.encoder.layers.0.fc2.weight\n",
      "name :  model.encoder.layers.0.fc2.bias\n",
      "name :  model.encoder.layers.0.final_layer_norm.weight\n",
      "name :  model.encoder.layers.0.final_layer_norm.bias\n",
      "name :  model.encoder.layers.1.self_attn.k_proj.weight\n",
      "name :  model.encoder.layers.1.self_attn.k_proj.bias\n",
      "name :  model.encoder.layers.1.self_attn.v_proj.weight\n",
      "name :  model.encoder.layers.1.self_attn.v_proj.bias\n",
      "name :  model.encoder.layers.1.self_attn.q_proj.weight\n",
      "name :  model.encoder.layers.1.self_attn.q_proj.bias\n",
      "name :  model.encoder.layers.1.self_attn.out_proj.weight\n",
      "name :  model.encoder.layers.1.self_attn.out_proj.bias\n",
      "name :  model.encoder.layers.1.self_attn_layer_norm.weight\n",
      "name :  model.encoder.layers.1.self_attn_layer_norm.bias\n",
      "name :  model.encoder.layers.1.fc1.weight\n",
      "name :  model.encoder.layers.1.fc1.bias\n",
      "name :  model.encoder.layers.1.fc2.weight\n",
      "name :  model.encoder.layers.1.fc2.bias\n",
      "name :  model.encoder.layers.1.final_layer_norm.weight\n",
      "name :  model.encoder.layers.1.final_layer_norm.bias\n",
      "name :  model.encoder.layers.2.self_attn.k_proj.weight\n",
      "name :  model.encoder.layers.2.self_attn.k_proj.bias\n",
      "name :  model.encoder.layers.2.self_attn.v_proj.weight\n",
      "name :  model.encoder.layers.2.self_attn.v_proj.bias\n",
      "name :  model.encoder.layers.2.self_attn.q_proj.weight\n",
      "name :  model.encoder.layers.2.self_attn.q_proj.bias\n",
      "name :  model.encoder.layers.2.self_attn.out_proj.weight\n",
      "name :  model.encoder.layers.2.self_attn.out_proj.bias\n",
      "name :  model.encoder.layers.2.self_attn_layer_norm.weight\n",
      "name :  model.encoder.layers.2.self_attn_layer_norm.bias\n",
      "name :  model.encoder.layers.2.fc1.weight\n",
      "name :  model.encoder.layers.2.fc1.bias\n",
      "name :  model.encoder.layers.2.fc2.weight\n",
      "name :  model.encoder.layers.2.fc2.bias\n",
      "name :  model.encoder.layers.2.final_layer_norm.weight\n",
      "name :  model.encoder.layers.2.final_layer_norm.bias\n",
      "name :  model.encoder.layers.3.self_attn.k_proj.weight\n",
      "name :  model.encoder.layers.3.self_attn.k_proj.bias\n",
      "name :  model.encoder.layers.3.self_attn.v_proj.weight\n",
      "name :  model.encoder.layers.3.self_attn.v_proj.bias\n",
      "name :  model.encoder.layers.3.self_attn.q_proj.weight\n",
      "name :  model.encoder.layers.3.self_attn.q_proj.bias\n",
      "name :  model.encoder.layers.3.self_attn.out_proj.weight\n",
      "name :  model.encoder.layers.3.self_attn.out_proj.bias\n",
      "name :  model.encoder.layers.3.self_attn_layer_norm.weight\n",
      "name :  model.encoder.layers.3.self_attn_layer_norm.bias\n",
      "name :  model.encoder.layers.3.fc1.weight\n",
      "name :  model.encoder.layers.3.fc1.bias\n",
      "name :  model.encoder.layers.3.fc2.weight\n",
      "name :  model.encoder.layers.3.fc2.bias\n",
      "name :  model.encoder.layers.3.final_layer_norm.weight\n",
      "name :  model.encoder.layers.3.final_layer_norm.bias\n",
      "name :  model.encoder.layers.4.self_attn.k_proj.weight\n",
      "name :  model.encoder.layers.4.self_attn.k_proj.bias\n",
      "name :  model.encoder.layers.4.self_attn.v_proj.weight\n",
      "name :  model.encoder.layers.4.self_attn.v_proj.bias\n",
      "name :  model.encoder.layers.4.self_attn.q_proj.weight\n",
      "name :  model.encoder.layers.4.self_attn.q_proj.bias\n",
      "name :  model.encoder.layers.4.self_attn.out_proj.weight\n",
      "name :  model.encoder.layers.4.self_attn.out_proj.bias\n",
      "name :  model.encoder.layers.4.self_attn_layer_norm.weight\n",
      "name :  model.encoder.layers.4.self_attn_layer_norm.bias\n",
      "name :  model.encoder.layers.4.fc1.weight\n",
      "name :  model.encoder.layers.4.fc1.bias\n",
      "name :  model.encoder.layers.4.fc2.weight\n",
      "name :  model.encoder.layers.4.fc2.bias\n",
      "name :  model.encoder.layers.4.final_layer_norm.weight\n",
      "name :  model.encoder.layers.4.final_layer_norm.bias\n",
      "name :  model.encoder.layers.5.self_attn.k_proj.weight\n",
      "name :  model.encoder.layers.5.self_attn.k_proj.bias\n",
      "name :  model.encoder.layers.5.self_attn.v_proj.weight\n",
      "name :  model.encoder.layers.5.self_attn.v_proj.bias\n",
      "name :  model.encoder.layers.5.self_attn.q_proj.weight\n",
      "name :  model.encoder.layers.5.self_attn.q_proj.bias\n",
      "name :  model.encoder.layers.5.self_attn.out_proj.weight\n",
      "name :  model.encoder.layers.5.self_attn.out_proj.bias\n",
      "name :  model.encoder.layers.5.self_attn_layer_norm.weight\n",
      "name :  model.encoder.layers.5.self_attn_layer_norm.bias\n",
      "name :  model.encoder.layers.5.fc1.weight\n",
      "name :  model.encoder.layers.5.fc1.bias\n",
      "name :  model.encoder.layers.5.fc2.weight\n",
      "name :  model.encoder.layers.5.fc2.bias\n",
      "name :  model.encoder.layers.5.final_layer_norm.weight\n",
      "name :  model.encoder.layers.5.final_layer_norm.bias\n",
      "name :  model.decoder.embed_positions.weight\n",
      "name :  model.decoder.layers.0.self_attn.k_proj.weight\n",
      "name :  model.decoder.layers.0.self_attn.k_proj.bias\n",
      "name :  model.decoder.layers.0.self_attn.v_proj.weight\n",
      "name :  model.decoder.layers.0.self_attn.v_proj.bias\n",
      "name :  model.decoder.layers.0.self_attn.q_proj.weight\n",
      "name :  model.decoder.layers.0.self_attn.q_proj.bias\n",
      "name :  model.decoder.layers.0.self_attn.out_proj.weight\n",
      "name :  model.decoder.layers.0.self_attn.out_proj.bias\n",
      "name :  model.decoder.layers.0.self_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.0.self_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.0.encoder_attn.k_proj.weight\n",
      "name :  model.decoder.layers.0.encoder_attn.k_proj.bias\n",
      "name :  model.decoder.layers.0.encoder_attn.v_proj.weight\n",
      "name :  model.decoder.layers.0.encoder_attn.v_proj.bias\n",
      "name :  model.decoder.layers.0.encoder_attn.q_proj.weight\n",
      "name :  model.decoder.layers.0.encoder_attn.q_proj.bias\n",
      "name :  model.decoder.layers.0.encoder_attn.out_proj.weight\n",
      "name :  model.decoder.layers.0.encoder_attn.out_proj.bias\n",
      "name :  model.decoder.layers.0.encoder_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.0.encoder_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.0.fc1.weight\n",
      "name :  model.decoder.layers.0.fc1.bias\n",
      "name :  model.decoder.layers.0.fc2.weight\n",
      "name :  model.decoder.layers.0.fc2.bias\n",
      "name :  model.decoder.layers.0.final_layer_norm.weight\n",
      "name :  model.decoder.layers.0.final_layer_norm.bias\n",
      "name :  model.decoder.layers.1.self_attn.k_proj.weight\n",
      "name :  model.decoder.layers.1.self_attn.k_proj.bias\n",
      "name :  model.decoder.layers.1.self_attn.v_proj.weight\n",
      "name :  model.decoder.layers.1.self_attn.v_proj.bias\n",
      "name :  model.decoder.layers.1.self_attn.q_proj.weight\n",
      "name :  model.decoder.layers.1.self_attn.q_proj.bias\n",
      "name :  model.decoder.layers.1.self_attn.out_proj.weight\n",
      "name :  model.decoder.layers.1.self_attn.out_proj.bias\n",
      "name :  model.decoder.layers.1.self_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.1.self_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.1.encoder_attn.k_proj.weight\n",
      "name :  model.decoder.layers.1.encoder_attn.k_proj.bias\n",
      "name :  model.decoder.layers.1.encoder_attn.v_proj.weight\n",
      "name :  model.decoder.layers.1.encoder_attn.v_proj.bias\n",
      "name :  model.decoder.layers.1.encoder_attn.q_proj.weight\n",
      "name :  model.decoder.layers.1.encoder_attn.q_proj.bias\n",
      "name :  model.decoder.layers.1.encoder_attn.out_proj.weight\n",
      "name :  model.decoder.layers.1.encoder_attn.out_proj.bias\n",
      "name :  model.decoder.layers.1.encoder_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.1.encoder_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.1.fc1.weight\n",
      "name :  model.decoder.layers.1.fc1.bias\n",
      "name :  model.decoder.layers.1.fc2.weight\n",
      "name :  model.decoder.layers.1.fc2.bias\n",
      "name :  model.decoder.layers.1.final_layer_norm.weight\n",
      "name :  model.decoder.layers.1.final_layer_norm.bias\n",
      "name :  model.decoder.layers.2.self_attn.k_proj.weight\n",
      "name :  model.decoder.layers.2.self_attn.k_proj.bias\n",
      "name :  model.decoder.layers.2.self_attn.v_proj.weight\n",
      "name :  model.decoder.layers.2.self_attn.v_proj.bias\n",
      "name :  model.decoder.layers.2.self_attn.q_proj.weight\n",
      "name :  model.decoder.layers.2.self_attn.q_proj.bias\n",
      "name :  model.decoder.layers.2.self_attn.out_proj.weight\n",
      "name :  model.decoder.layers.2.self_attn.out_proj.bias\n",
      "name :  model.decoder.layers.2.self_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.2.self_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.2.encoder_attn.k_proj.weight\n",
      "name :  model.decoder.layers.2.encoder_attn.k_proj.bias\n",
      "name :  model.decoder.layers.2.encoder_attn.v_proj.weight\n",
      "name :  model.decoder.layers.2.encoder_attn.v_proj.bias\n",
      "name :  model.decoder.layers.2.encoder_attn.q_proj.weight\n",
      "name :  model.decoder.layers.2.encoder_attn.q_proj.bias\n",
      "name :  model.decoder.layers.2.encoder_attn.out_proj.weight\n",
      "name :  model.decoder.layers.2.encoder_attn.out_proj.bias\n",
      "name :  model.decoder.layers.2.encoder_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.2.encoder_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.2.fc1.weight\n",
      "name :  model.decoder.layers.2.fc1.bias\n",
      "name :  model.decoder.layers.2.fc2.weight\n",
      "name :  model.decoder.layers.2.fc2.bias\n",
      "name :  model.decoder.layers.2.final_layer_norm.weight\n",
      "name :  model.decoder.layers.2.final_layer_norm.bias\n",
      "name :  model.decoder.layers.3.self_attn.k_proj.weight\n",
      "name :  model.decoder.layers.3.self_attn.k_proj.bias\n",
      "name :  model.decoder.layers.3.self_attn.v_proj.weight\n",
      "name :  model.decoder.layers.3.self_attn.v_proj.bias\n",
      "name :  model.decoder.layers.3.self_attn.q_proj.weight\n",
      "name :  model.decoder.layers.3.self_attn.q_proj.bias\n",
      "name :  model.decoder.layers.3.self_attn.out_proj.weight\n",
      "name :  model.decoder.layers.3.self_attn.out_proj.bias\n",
      "name :  model.decoder.layers.3.self_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.3.self_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.3.encoder_attn.k_proj.weight\n",
      "name :  model.decoder.layers.3.encoder_attn.k_proj.bias\n",
      "name :  model.decoder.layers.3.encoder_attn.v_proj.weight\n",
      "name :  model.decoder.layers.3.encoder_attn.v_proj.bias\n",
      "name :  model.decoder.layers.3.encoder_attn.q_proj.weight\n",
      "name :  model.decoder.layers.3.encoder_attn.q_proj.bias\n",
      "name :  model.decoder.layers.3.encoder_attn.out_proj.weight\n",
      "name :  model.decoder.layers.3.encoder_attn.out_proj.bias\n",
      "name :  model.decoder.layers.3.encoder_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.3.encoder_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.3.fc1.weight\n",
      "name :  model.decoder.layers.3.fc1.bias\n",
      "name :  model.decoder.layers.3.fc2.weight\n",
      "name :  model.decoder.layers.3.fc2.bias\n",
      "name :  model.decoder.layers.3.final_layer_norm.weight\n",
      "name :  model.decoder.layers.3.final_layer_norm.bias\n",
      "name :  model.decoder.layers.4.self_attn.k_proj.weight\n",
      "name :  model.decoder.layers.4.self_attn.k_proj.bias\n",
      "name :  model.decoder.layers.4.self_attn.v_proj.weight\n",
      "name :  model.decoder.layers.4.self_attn.v_proj.bias\n",
      "name :  model.decoder.layers.4.self_attn.q_proj.weight\n",
      "name :  model.decoder.layers.4.self_attn.q_proj.bias\n",
      "name :  model.decoder.layers.4.self_attn.out_proj.weight\n",
      "name :  model.decoder.layers.4.self_attn.out_proj.bias\n",
      "name :  model.decoder.layers.4.self_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.4.self_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.4.encoder_attn.k_proj.weight\n",
      "name :  model.decoder.layers.4.encoder_attn.k_proj.bias\n",
      "name :  model.decoder.layers.4.encoder_attn.v_proj.weight\n",
      "name :  model.decoder.layers.4.encoder_attn.v_proj.bias\n",
      "name :  model.decoder.layers.4.encoder_attn.q_proj.weight\n",
      "name :  model.decoder.layers.4.encoder_attn.q_proj.bias\n",
      "name :  model.decoder.layers.4.encoder_attn.out_proj.weight\n",
      "name :  model.decoder.layers.4.encoder_attn.out_proj.bias\n",
      "name :  model.decoder.layers.4.encoder_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.4.encoder_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.4.fc1.weight\n",
      "name :  model.decoder.layers.4.fc1.bias\n",
      "name :  model.decoder.layers.4.fc2.weight\n",
      "name :  model.decoder.layers.4.fc2.bias\n",
      "name :  model.decoder.layers.4.final_layer_norm.weight\n",
      "name :  model.decoder.layers.4.final_layer_norm.bias\n",
      "name :  model.decoder.layers.5.self_attn.k_proj.weight\n",
      "name :  model.decoder.layers.5.self_attn.k_proj.bias\n",
      "name :  model.decoder.layers.5.self_attn.v_proj.weight\n",
      "name :  model.decoder.layers.5.self_attn.v_proj.bias\n",
      "name :  model.decoder.layers.5.self_attn.q_proj.weight\n",
      "name :  model.decoder.layers.5.self_attn.q_proj.bias\n",
      "name :  model.decoder.layers.5.self_attn.out_proj.weight\n",
      "name :  model.decoder.layers.5.self_attn.out_proj.bias\n",
      "name :  model.decoder.layers.5.self_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.5.self_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.5.encoder_attn.k_proj.weight\n",
      "name :  model.decoder.layers.5.encoder_attn.k_proj.bias\n",
      "name :  model.decoder.layers.5.encoder_attn.v_proj.weight\n",
      "name :  model.decoder.layers.5.encoder_attn.v_proj.bias\n",
      "name :  model.decoder.layers.5.encoder_attn.q_proj.weight\n",
      "name :  model.decoder.layers.5.encoder_attn.q_proj.bias\n",
      "name :  model.decoder.layers.5.encoder_attn.out_proj.weight\n",
      "name :  model.decoder.layers.5.encoder_attn.out_proj.bias\n",
      "name :  model.decoder.layers.5.encoder_attn_layer_norm.weight\n",
      "name :  model.decoder.layers.5.encoder_attn_layer_norm.bias\n",
      "name :  model.decoder.layers.5.fc1.weight\n",
      "name :  model.decoder.layers.5.fc1.bias\n",
      "name :  model.decoder.layers.5.fc2.weight\n",
      "name :  model.decoder.layers.5.fc2.bias\n",
      "name :  model.decoder.layers.5.final_layer_norm.weight\n",
      "name :  model.decoder.layers.5.final_layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in generator2_checkpoint.named_parameters():\n",
    "    # param.requires_grad = False\n",
    "    print(\"name : \", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param :  Parameter containing:\n",
      "tensor([[ 0.0016,  0.0071, -0.0144,  ...,  0.0075, -0.0426, -0.0326],\n",
      "        [ 0.0535,  0.0126,  0.0063,  ..., -0.0350, -0.0400, -0.0081],\n",
      "        [ 0.0124,  0.0211,  0.0091,  ..., -0.0311, -0.0642, -0.0289],\n",
      "        ...,\n",
      "        [ 0.0766,  0.0253,  0.0169,  ..., -0.0387, -0.0359, -0.0040],\n",
      "        [ 0.0730,  0.0233,  0.0149,  ..., -0.0412, -0.0385, -0.0054],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in generator2_checkpoint.lm_head.parameters():\n",
    "    print(\"param : \", param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
