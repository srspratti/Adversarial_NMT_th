{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "# from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import evaluate\n",
    "# raw_datasets = load_dataset(\"wmt16\", \"ro-en\")\n",
    "# metric = load_metric(\"sacrebleu\")\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache at: /root/.cache/huggingface/datasets/wmt14/fr-en\n",
      "Dataset already downloaded, loading from cache.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff89c0b97034dac8628f82392e1934a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0a6a424ebb40bda4f2fca6ac75bbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get user's home directory\n",
    "import os\n",
    "home = os.path.expanduser(\"~\")\n",
    "\n",
    "# Define the path of the cache directory\n",
    "cache_dir = os.path.join(home, \".cache\", \"huggingface\", \"datasets\")\n",
    "\n",
    "# Define the name and configuration of the dataset\n",
    "dataset_name = \"wmt14\"\n",
    "config_name = \"fr-en\"\n",
    "\n",
    "# Build the path for the specific dataset configuration\n",
    "dataset_config_path = os.path.join(cache_dir, dataset_name, config_name)\n",
    "\n",
    "print(f\"Checking cache at: {dataset_config_path}\")\n",
    "\n",
    "# Check if the dataset configuration is already cached\n",
    "if os.path.exists(dataset_config_path) and len(os.listdir(dataset_config_path)) > 0:\n",
    "    print(\"Dataset already downloaded, loading from cache.\")\n",
    "    # If the dataset is already downloaded, load it from the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "else:\n",
    "    print(\"Downloading the dataset.\")\n",
    "    # Download the dataset and specify the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "\n",
    "# Here, you should adjust the loading of subsets to avoid redundant downloads or loading.\n",
    "# Load 50k rows of the train dataset\n",
    "train_dataset = dataset[\"train\"].select(range(1000000))\n",
    "# train_dataset = dataset[\"train\"].select(range(600))\n",
    "\n",
    "# Keep the full valid and test datasets\n",
    "valid_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'en': 'Pursuant to the Minister of Transport's direction, Cargolux Airlines International, S.A. (hereinafter Cargolux) was originally granted the authorization set out above on August 9, 1984, and renewals of the authorization were granted each year thereafter.', 'fr': 'Conformément à la directive du ministre des Transports, Cargolux Airlines International, S.A. (ci-après Cargolux) s'est vu initialement accorder l'autorisation susmentionnée le 9 août 1984. Cette autorisation a par la suite été renouvelée d'année en année.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'en': 'See chap.', 'fr': 'Voir chap.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'en': 'Subject to rule 33, decisions of the Assembly on all matters of substance shall be taken by a two-thirds majority of the representatives present and voting.', 'fr': 'Sous réserve de l'article 33, les décisions de l'Assemblée sur toutes les questions de fond sont prises à la majorité des deux tiers des représentants présents et votants.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'en': 'The Board reported in paragraph 42 of its previous report (A/57/5, vol. II, chap.', 'fr': 'Au paragraphe 42 de son rapport précédent (A/57/5, vol. II, chap. II), le Comité indiquait qu'un montant de 6,4 millions de dollars des contributions volontaires à recevoir au titre de la Force des Nations Unies chargée du maintien de la paix à Chypre (UNFICYP) était impayé depuis plus de huit ans.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'en': 'Moreover, dogs, cats, Raccoons, Coyotes, and vehicular traffic also cause Five-lined Skink deaths.', 'fr': 'Enfin, les chiens, les chats, les ratons laveurs, les coyotes et la circulation routiÃ¨re causent Ã©galement la mortalitÃ© des scinques pentalignes.'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, MarianMTModel, AutoModelForSeq2SeqLM\n",
    "\n",
    "# checkpoint = \"google-t5/t5-small\"\n",
    "src = \"en\"  # source language\n",
    "trg = \"fr\"  # target language\n",
    "\n",
    "checkpoint_name = f\"Helsinki-NLP/opus-mt-{src}-{trg}\"\n",
    "# checkpoint = MarianMTModel.from_pretrained(checkpoint_name)\n",
    "checkpoint = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee24332e559448f8e4e8f3ba4cdaa8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e74453bce545d08f9e40b6b9b18083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenized_datasets = train_dataset.map(preprocess_function, batched=True)\n",
    "valid_tokenized_datasets = valid_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Helsinki-NLP/opus-mt-en-fr'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint_name, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b75914db16749df91627d098fc1ff5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_tokenized_datasets,\n",
    "    eval_dataset=valid_tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15626' max='15626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15626/15626 1:23:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.937900</td>\n",
       "      <td>1.265381</td>\n",
       "      <td>32.323100</td>\n",
       "      <td>29.461700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.910500</td>\n",
       "      <td>1.265185</td>\n",
       "      <td>32.133500</td>\n",
       "      <td>29.421000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/preprocess_bert_udem_train/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15626, training_loss=0.9355023573413177, metrics={'train_runtime': 5005.5576, 'train_samples_per_second': 399.556, 'train_steps_per_second': 3.122, 'total_flos': 5.245765192935014e+16, 'train_loss': 0.9355023573413177, 'epoch': 2.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64850ce381694a4c8915695e4e99bcdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/299M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainer.save_model(output_dir)\n",
    "trainer.save_model( f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the PreTrained model on TEST DATA of WMT14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache at: /root/.cache/huggingface/datasets/wmt14/fr-en\n",
      "Downloading the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 10.5k/10.5k [00:00<00:00, 34.1kB/s]\n",
      "Downloading data: 100%|██████████| 30/30 [04:46<00:00,  9.54s/files]\n",
      "Downloading data: 100%|██████████| 475k/475k [00:00<00:00, 734kB/s]\n",
      "Downloading data: 100%|██████████| 536k/536k [00:00<00:00, 1.38MB/s]\n",
      "Generating train split: 100%|██████████| 40836715/40836715 [00:53<00:00, 763931.61 examples/s]\n",
      "Generating validation split: 100%|██████████| 3000/3000 [00:00<00:00, 703978.52 examples/s]\n",
      "Generating test split: 100%|██████████| 3003/3003 [00:00<00:00, 778027.98 examples/s]\n",
      "Downloading builder script: 100%|██████████| 8.15k/8.15k [00:00<00:00, 10.2MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.93k/6.93k [00:00<00:00, 7.87MB/s]\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 7.09MB/s]\n",
      "Downloading builder script: 100%|██████████| 9.99k/9.99k [00:00<00:00, 10.3MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.97k/6.97k [00:00<00:00, 6.58MB/s]\n",
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Fetching 5 files: 100%|██████████| 5/5 [01:13<00:00, 14.79s/it]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint root/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Encoder model frozen.\n",
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/transformers/modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating batches: 100%|██████████| 76/76 [00:46<00:00,  1.63it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX 6000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import os\n",
    "import torch\n",
    "import dill\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# seed = 1234\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Get user's home directory\n",
    "import os\n",
    "home = os.path.expanduser(\"~\")\n",
    "\n",
    "# Define the path of the cache directory\n",
    "cache_dir = os.path.join(home, \".cache\", \"huggingface\", \"datasets\")\n",
    "\n",
    "# Define the name and configuration of the dataset\n",
    "dataset_name = \"wmt14\"\n",
    "config_name = \"fr-en\"\n",
    "\n",
    "# Build the path for the specific dataset configuration\n",
    "dataset_config_path = os.path.join(cache_dir, dataset_name, config_name)\n",
    "\n",
    "print(f\"Checking cache at: {dataset_config_path}\")\n",
    "\n",
    "# Check if the dataset configuration is already cached\n",
    "if os.path.exists(dataset_config_path) and len(os.listdir(dataset_config_path)) > 0:\n",
    "    print(\"Dataset already downloaded, loading from cache.\")\n",
    "    # If the dataset is already downloaded, load it from the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "else:\n",
    "    print(\"Downloading the dataset.\")\n",
    "    # Download the dataset and specify the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "\n",
    "# Keep the full valid and test datasets\n",
    "valid_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "texts =[]\n",
    "labels = []\n",
    "for element in test_dataset[\"translation\"]:\n",
    "        # print(\"element: \", element)\n",
    "        texts.append(element[\"en\"])\n",
    "        labels.append(element[\"fr\"])\n",
    "\n",
    "# metric = evaluate.load(\"sacrebleu\")\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "ter_metric = evaluate.load(\"ter\")\n",
    "# chrf_metric = evaluate.load(\"chrf\")\n",
    "# bleurt_metric = evaluate.load(\"bleurt\")\n",
    "comet_metric = evaluate.load(\"comet\")\n",
    "getpwd = os.getcwd()\n",
    "\n",
    "checkpoint_path_tokenizer = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "generator2_checkpoint = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "# generator2_train # Extract the underlying model from the DataParallel wrapper\n",
    "generator2_checkpoint = generator2_checkpoint.module if isinstance(generator2_checkpoint, torch.nn.DataParallel) else generator2_checkpoint\n",
    "\n",
    "# Check if CUDA is available and then set the default device to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path_tokenizer)\n",
    "\n",
    "batch_size = 40  # Adjust this based on your GPU's memory capacity\n",
    "\n",
    "translations_batch_PreTrained = []\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    generator2_checkpoint = torch.nn.DataParallel(generator2_checkpoint).cuda()\n",
    "else:\n",
    "    generator2_checkpoint.cuda()\n",
    "\n",
    "generator2_checkpoint = generator2_checkpoint.module if hasattr(generator2_checkpoint, 'module') else generator2_checkpoint\n",
    "\n",
    "generator2_checkpoint.eval()\n",
    "# generator2_checkpoint.to(device)\n",
    "\n",
    "# Process texts in batches\n",
    "for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating batches\"):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    inputs = tokenizer(batch, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # print(\"inputs shape: \", inputs.shape)\n",
    "    # Generate outputs for the entire batch\n",
    "    outputs = generator2_checkpoint.generate(inputs, max_length=60, num_beams=5, early_stopping=True)\n",
    "    # print(\"outputs shape\", outputs.shape)\n",
    "    \n",
    "    # Decode all outputs in the batch\n",
    "    # batch_translations = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    batch_translation = tokenizer.batch_decode(outputs , skip_special_tokens=True)\n",
    "    translations_batch_PreTrained.extend(batch_translation)\n",
    "\n",
    "# result_batch = bleu_metric.compute(predictions=translations_batch_PreTrained, references=labels)\n",
    "# result_batch = {\"bleu\": result_batch[\"score\"]}\n",
    "result_batch = []\n",
    "\n",
    "result_batch = {\n",
    "\"bleu\": bleu_metric.compute(predictions=translations_batch_PreTrained, references=labels)[\"score\"],\n",
    "\"meteor\": meteor_metric.compute(predictions=translations_batch_PreTrained, references=labels)[\"meteor\"],\n",
    "\"rouge\": rouge_metric.compute(predictions=translations_batch_PreTrained, references=labels),\n",
    "\"ter\": ter_metric.compute(predictions=translations_batch_PreTrained, references=labels)[\"score\"],\n",
    "\"comet\": comet_metric.compute(predictions=translations_batch_PreTrained, references=labels, sources=texts)[\"mean_score\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 39.1440990614117,\n",
       " 'meteor': 0.6451421562380347,\n",
       " 'rouge': {'rouge1': 0.6803935614543364,\n",
       "  'rouge2': 0.4901839476087089,\n",
       "  'rougeL': 0.6471082639159225,\n",
       "  'rougeLsum': 0.6467350807635843},\n",
       " 'ter': 48.41090893423906,\n",
       " 'comet': 0.8455673381165191}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results_translations_batch_PreTrained\", \"w\") as f:\n",
    "        f.write(\"BLEU Score: \" + str(result_batch[\"bleu\"]) + \"\\n\")\n",
    "        f.write(\"METEOR Score: \" + str(result_batch[\"meteor\"]) + \"\\n\")\n",
    "        f.write(\"ROUGE Scores: \" + str(result_batch[\"rouge\"]) + \"\\n\")\n",
    "        f.write(\"TER Score: \" + str(result_batch[\"ter\"]) + \"\\n\")\n",
    "        f.write(\"COMET Score: \" + str(result_batch[\"comet\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the translations to a text file - translations\n",
    "import os\n",
    "file_path = os.path.join(os.getcwd(), \"translations_batch_PreTrained\")\n",
    "with open(file_path, \"w\") as file:\n",
    "    for translation in translations_batch_PreTrained:\n",
    "        file.write(translation + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_en = os.path.join(getpwd, \"original_english_translations_batch_PreTrained.txt\")\n",
    "# file_path = \"/path/to/translations.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_en, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for text in texts:\n",
    "        file.write(text + \"\\n\")\n",
    "\n",
    "\n",
    "file_path_fr = os.path.join(getpwd, \"original_french_translations_batch_PreTrained.txt\")\n",
    "# file_path = \"/path/to/translations.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_fr, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for label in labels:\n",
    "        file.write(label + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the FineTuned model on TEST DATA of WMT14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache at: /root/.cache/huggingface/datasets/wmt14/fr-en\n",
      "Dataset already downloaded, loading from cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 45990.18it/s]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint root/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Encoder model frozen.\n",
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/transformers/modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating batches: 100%|██████████| 76/76 [00:46<00:00,  1.63it/s]\n",
      "Using default tokenizer.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import os\n",
    "import torch\n",
    "import dill\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# seed = 1234\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Get user's home directory\n",
    "import os\n",
    "home = os.path.expanduser(\"~\")\n",
    "\n",
    "# Define the path of the cache directory\n",
    "cache_dir = os.path.join(home, \".cache\", \"huggingface\", \"datasets\")\n",
    "\n",
    "# Define the name and configuration of the dataset\n",
    "dataset_name = \"wmt14\"\n",
    "config_name = \"fr-en\"\n",
    "\n",
    "# Build the path for the specific dataset configuration\n",
    "dataset_config_path = os.path.join(cache_dir, dataset_name, config_name)\n",
    "\n",
    "print(f\"Checking cache at: {dataset_config_path}\")\n",
    "\n",
    "# Check if the dataset configuration is already cached\n",
    "if os.path.exists(dataset_config_path) and len(os.listdir(dataset_config_path)) > 0:\n",
    "    print(\"Dataset already downloaded, loading from cache.\")\n",
    "    # If the dataset is already downloaded, load it from the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "else:\n",
    "    print(\"Downloading the dataset.\")\n",
    "    # Download the dataset and specify the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "\n",
    "# Keep the full valid and test datasets\n",
    "valid_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "texts =[]\n",
    "labels = []\n",
    "for element in test_dataset[\"translation\"]:\n",
    "        # print(\"element: \", element)\n",
    "        texts.append(element[\"en\"])\n",
    "        labels.append(element[\"fr\"])\n",
    "\n",
    "# metric = evaluate.load(\"sacrebleu\")\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "ter_metric = evaluate.load(\"ter\")\n",
    "# chrf_metric = evaluate.load(\"chrf\")\n",
    "# bleurt_metric = evaluate.load(\"bleurt\")\n",
    "comet_metric = evaluate.load(\"comet\")\n",
    "getpwd = os.getcwd()\n",
    "\n",
    "checkpoint_path_tokenizer = 'sriram-sanjeev9s/opus-mt-en-fr-finetuned-en-to-fr'\n",
    "generator2_checkpoint = AutoModelForSeq2SeqLM.from_pretrained(\"sriram-sanjeev9s/opus-mt-en-fr-finetuned-en-to-fr\")\n",
    "\n",
    "# generator2_train # Extract the underlying model from the DataParallel wrapper\n",
    "generator2_checkpoint = generator2_checkpoint.module if isinstance(generator2_checkpoint, torch.nn.DataParallel) else generator2_checkpoint\n",
    "\n",
    "# Check if CUDA is available and then set the default device to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path_tokenizer)\n",
    "\n",
    "batch_size = 40  # Adjust this based on your GPU's memory capacity\n",
    "\n",
    "translations_batch_fineTuned = []\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    generator2_checkpoint = torch.nn.DataParallel(generator2_checkpoint).cuda()\n",
    "else:\n",
    "    generator2_checkpoint.cuda()\n",
    "\n",
    "generator2_checkpoint = generator2_checkpoint.module if hasattr(generator2_checkpoint, 'module') else generator2_checkpoint\n",
    "\n",
    "generator2_checkpoint.eval()\n",
    "# generator2_checkpoint.to(device)\n",
    "\n",
    "# Process texts in batches\n",
    "for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating batches\"):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    inputs = tokenizer(batch, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # print(\"inputs shape: \", inputs.shape)\n",
    "    # Generate outputs for the entire batch\n",
    "    outputs = generator2_checkpoint.generate(inputs, max_length=60, num_beams=5, early_stopping=True)\n",
    "    # print(\"outputs shape\", outputs.shape)\n",
    "    \n",
    "    # Decode all outputs in the batch\n",
    "    # batch_translations = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    batch_translation = tokenizer.batch_decode(outputs , skip_special_tokens=True)\n",
    "    translations_batch_fineTuned.extend(batch_translation)\n",
    "\n",
    "# result_batch = bleu_metric.compute(predictions=translations_batch_PreTrained, references=labels)\n",
    "# result_batch = {\"bleu\": result_batch[\"score\"]}\n",
    "result_batch_ft = []\n",
    "\n",
    "result_batch_ft = {\n",
    "\"bleu\": bleu_metric.compute(predictions=translations_batch_fineTuned, references=labels)[\"score\"],\n",
    "\"meteor\": meteor_metric.compute(predictions=translations_batch_fineTuned, references=labels)[\"meteor\"],\n",
    "\"rouge\": rouge_metric.compute(predictions=translations_batch_fineTuned, references=labels),\n",
    "\"ter\": ter_metric.compute(predictions=translations_batch_fineTuned, references=labels)[\"score\"],\n",
    "\"comet\": comet_metric.compute(predictions=translations_batch_fineTuned, references=labels, sources=texts)[\"mean_score\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 36.82378117644398,\n",
       " 'meteor': 0.627017352608584,\n",
       " 'rouge': {'rouge1': 0.656765588537936,\n",
       "  'rouge2': 0.4629045462998402,\n",
       "  'rougeL': 0.6236932313220118,\n",
       "  'rougeLsum': 0.6234142948120543},\n",
       " 'ter': 50.583791701971826,\n",
       " 'comet': 0.8286577677710867}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_batch_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results_translations_batch_fineTuned\", \"w\") as f:\n",
    "        f.write(\"BLEU Score: \" + str(result_batch_ft[\"bleu\"]) + \"\\n\")\n",
    "        f.write(\"METEOR Score: \" + str(result_batch_ft[\"meteor\"]) + \"\\n\")\n",
    "        f.write(\"ROUGE Scores: \" + str(result_batch_ft[\"rouge\"]) + \"\\n\")\n",
    "        f.write(\"TER Score: \" + str(result_batch_ft[\"ter\"]) + \"\\n\")\n",
    "        f.write(\"COMET Score: \" + str(result_batch_ft[\"comet\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the translations to a text file - translations\n",
    "import os\n",
    "file_path = os.path.join(os.getcwd(), \"translations_batch_fineTuned.txt\")\n",
    "with open(file_path, \"w\") as file:\n",
    "    for translation in translations_batch_fineTuned:\n",
    "        file.write(translation + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_en = os.path.join(getpwd, \"original_english_translations_batch_fineTuned.txt\")\n",
    "# file_path = \"/path/to/translations.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_en, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for text in texts:\n",
    "        file.write(text + \"\\n\")\n",
    "\n",
    "\n",
    "file_path_fr = os.path.join(getpwd, \"original_french_translations_batch_fineTuned.txt\")\n",
    "# file_path = \"/path/to/translations.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path_fr, \"w\") as file:\n",
    "    # Write each translation to the file\n",
    "    for label in labels:\n",
    "        file.write(label + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on Different Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreTrained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tedtalks = load_dataset(\"ted_talks_iwslt\", \"en-fr\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FineTuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN-KD Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProtoTyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_292236/3035705735.py:4: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  datasets_list = list_datasets()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amirveyseh/acronym_identification', 'ade-benchmark-corpus/ade_corpus_v2', 'UCLNLP/adversarial_qa', 'Yale-LILY/aeslc', 'nwu-ctext/afrikaans_ner_corpus', 'fancyzhx/ag_news', 'allenai/ai2_arc', 'google/air_dialogue', 'komari6/ajgt_twitter_ar', 'legacy-datasets/allegro_reviews']\n"
     ]
    }
   ],
   "source": [
    "from datasets import list_datasets\n",
    "\n",
    "# List all available datasets\n",
    "datasets_list = list_datasets()\n",
    "\n",
    "# Print the first 10 datasets\n",
    "print(datasets_list[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ignatius/igbo_english_machine_translation', 'microsoft/msr_zhen_translation_parity', 'abidlabs/test-translation-dataset', 'persiannlp/parsinlu_translation_en_fa', 'persiannlp/parsinlu_translation_fa_en', 'shivam/test-translation-2', 'shivam/test-translation', 'svalabs/all-nli-german-translation-wmt19', 'botisan-ai/cantonese-mandarin-translations', 'atenglens/taiwanese_english_translation', 'DigitalUmuganda/kinyarwanda-english-machine-translation-dataset', 'VanessaSchenkel/translation-en-pt', 'Kamrani/en-fa-translation', 'open-source-metrics/translation-checkpoint-downloads', 'KETI-AIR/aihub_spoken_language_translation', 'woctordho/autotrain-data-lojban-translation', 'KETI-AIR/aihub_scitech_translation', 'KETI-AIR/aihub_koenzh_food_translation', 'KETI-AIR/aihub_scitech20_translation', 'KETI-AIR/aihub_socialtech20_translation', 'Sotaro0124/Ainu-Japan_translation_model', 'HuggingFace-CN-community/translation', 'swaption2009/20k-en-zh-translation-pinyin-hsk', 'DigitalUmuganda/monolingual_machine_translation_data', 'lansinuote/nlp.7.translation', 'grosenthal/latin_english_translation', 'theblackcat102/instruction_translations', 'jalalnb/back_translation_fr_on_small_persian_QA', 'jalalnb/back_translation_en_on_small_persian_QA', 'jalalnb/back_translation_hy_on_small_persian_QA', 'RocioUrquijo/translation', 'chenyouyou/translation-en-de', 'semeru/code-code-translation-java-csharp', 'spdenisov/word_aligned_translation', 'iamwille/igbo-translation', 'M-AI-C/quran_en_translations', 'mesolitica/chatgpt-noisy-translation-banjarese', 'kunishou/databricks-dolly-69k-ja-en-translation', 'skrishna/salient_translation_error_detection_preprocessed', 'g0beze/am_bilbe_translation_dataset', 'bigpang/muti-lang-translation-with-twitter', 'zetavg/coct-en-zh-tw-translations-twp-300k', 'OdiaGenAI/OdiEnCorp_translation_instructions_25k', 'Koyd111/llama-eng-jp-translation-captions', 'PaulineSanchez/Multi_restaurants_menus_translation', 'PaulineSanchez/dataset_food_translation_v2', 'PaulineSanchez/Dataset_food_translation_fr_en', 'bri25yu/flores200_devtest_translation_pairs', 'hlillemark/flores200_devtest_translation_pairs_mt5', 'shrusti333/konkani_translation', 'talmp/en-vi-translation-test', 'talmp/en-vi-translation', 'GeorgeSpark/translationmodels', 'PaulineSanchez/recipes_translation_4_helsinki_3.0', 'PaulineSanchez/recipes_translation_2', 'Zaid/tmp-translation', 'PaulineSanchez/recipes_translation_4_helsinki_4.0', 'PaulineSanchez/recipes_translation_400', 'mekaneeky/Processed-Luganda-SpeechT5-with-SALT-translation-11-7-23', 'mekaneeky/Processed-Luganda-SpeechT5-with-SALT-translation-11-7-23-val-only', 'squarelike/sharegpt_deepl_ko_translation', 'Amani27/massive_translation_dataset', 'thesistranslation/wmt14', 'xzuyn/manythings-translations', 'xzuyn/manythings-translations-alpaca', 'kaenakiakona/machinetranslationspanish', 'thesistranslation/distilled-ccmatrix-de-en', 'alayaran/bodo-english-prompt-translation', 'thesistranslation/distilled-ccmatrix-en-de', 'Sylvana/qa_en_translation', 'rajuptvs/English-to-hindi-podcast-translation', 'harouzie/vi_en-translation', 'thesistranslation/distilled-ccmatrix-fr-en', 'thesistranslation/distilled-ccmatrix-en-fr', 'neil-code/autotrain-data-translation-en-zh', 'thesistranslation/distilled-ccmatrix-en-es', 'norashameri97/tmp-translation', 'thesistranslation/distilled-ccmatrix-es-en', 'indiejoseph/yue-zh-translation', 'KE-AI/translation', 'indonlp/nusatranslation_emot', 'indonlp/nusatranslation_senti', 'indonlp/nusatranslation_mt', 'shaowenchen/translation_zh', 'Unspoiled-Egg/indigenous-eng-translation', 'tiagoblima/translation-pt-indigenouns', 'SEACrowd/nusatranslation_mt', 'SEACrowd/nusatranslation_emot', 'ArmelR/test_instruction_backtranslation', 'indiejoseph/ted-translation-zhhk-zhcn', 'vsarathy/nl-robotics-translation-simple_english-30k-context', 'vsarathy/nl-robotics-translation-simple_english-30k-no-context', 'ninja/arabic-english-translation', 'vsarathy/nl-robotics-translation-simple_english-12k-no-context-TEST', 'vsarathy/nl-robotics-translation-simple_english-12k-context-TEST', 'RikoteMaster/llama2_4_translation', 'vsarathy/nl-robotics-translation-simple_english-2k-no-context-TEST', 'vsarathy/nl-robotics-translation-simple_english-2k-context-TEST', 'RikoteMaster/translation_4_llama2_with_end_token', 'zuko2/conditional-translation-me-en-me', 'vsarathy/nl-robotics-translation-simple_english-2k-novelty-no-context-TEST', 'vsarathy/nl-robotics-translation-simple_english-2k-novelty-context-TEST', 'kjappelbaum/chemnlp-qm9-file-translation', 'p1atdev/kogo-bonjin-translation', 'nlplabtdtu/translation-text', 'Weni/Dataset_semantic_alignment_translation_en-es-direction_en-pt_br-direction', 'bot-yaya/undl_en2zh_translation', 'ranWang/undl_en2zh_translation', 'mhmtcrkglu/autotrain-data-testtranslation', 'mhmtcrkglu/autotrain-data-test-translation-t5-small', 'shashwat16/autotrain-data-dialect-translation', 'Svngoku/kikongo-french-translation', 'reckitt-anugrahakbarp/SNS_audio_translation', 'chirunder/text_message_translations_1k', 'NghiemAbe/translation-vietnamese-english', 'jhflow/share_gpt_translation_with_instruction', 'Sampuran01/english-hindi-translation', 'barghavani/translation_machine_en_to_fa', 'vsak/Enron_translation_greek', 'vsak/YouTube_spam_translation', 'vsak/sms_translation_greek', 'yairschiff/translation', 'bot-yaya/undl_fr2en_translation', 'bot-yaya/undl_de2en_translation', 'bot-yaya/undl_ru2en_translation', 'bot-yaya/undl_es2en_translation', 'bot-yaya/undl_ar2en_translation', 'bot-yaya/undl_zh2en_translation', 'qgyd2021/few_shot_translation_sft', 'Broomva/translation_guc_spa', 'Broomva/translation_pbb_spa', 'seedboxai/german_to_english_translations_v1', 'galsenai/french-wolof-translation', 'youdiniplays/tagalog-cebuano_translation', 'harpreetsahota/modern-to-shakesperean-translation', 'Hazzzardous/synthetic-translations-6k-unvalidated', 'mesolitica/standard-malay-translation-instructions', 'mesolitica/noisy-standard-malay-translation-instructions', 'waveiilllii/eng-nav-translation', 'mesolitica/chatgpt4-noisy-translation-twitter-dialect', 'hon9kon9ize/38k-zh-yue-translation-llm-generated', 'RaiBP/openwebtext2-first-30-chunks-translation-examples', 'atmallen/quirky_translation', 'Rishabh02/translation_hindi', 'Mike0307/zh-tw-machine-translation', 'RaiBP/openwebtext2-first-30-chunks-ablation-translation', 'jhflow/sharegpt_deepl_ko_translation_dedup', 'tfft/translation_test', 'mideind/icelandic-english-translation', 'mideind/english-icelandic-translation', 'traintogpb/aihub-koen-translation-integrated-large-10m', 'traintogpb/aihub-koen-translation-integrated-base-1m', 'traintogpb/aihub-koen-translation-integrated-small-100k', 'mehr32/Persian_English_translation', 'khwrali011/En_Ger_translation', 'satpalsr/translation-filter', 'satpalsr/translation', 'huseinzol05/mosaic-standard-translation', 'liaad/translation_sample', 'liaad/translation_sample_lid', 'ashokpoudel/nepali-english-translation-dataset', 'matekadlicsko/hungarian-news-translations', 'satpalsr/chatml-translation-filter', 'klima7/en-pl-translation', 'prosa-text/nusa-translation', 'innodatalabs/rt-realtoxicity-translation', 'BhabhaAI/translation-classify', 'DerErikP/translations', 'kuresakikuriko/report-translation-feedback', 'kuotient/reddit_enko_translation_preference', 'YashRawal225/translation', 'riggj/carrot-engine-normalization-translation', 'sellersew/carrot-engine-normalization-translation-v2', 'mrSoul7766/eng_to_odia_translation_20k', 'mrSoul7766/hindi_to_odia_translation_10k', 'mfmezger/sandboxai_german_to_english_translations_seperated', 'Ehtisham1328/urdu-idioms-with-english-translation', 'plutokokoa/translation-for-yu-gi-oh-ja-traditional-zh', 'zohann/pam-masc-arabic-translation', 'syeda-raisa/idiom_translation_final', 'IPSAN/tatar_translation_dataset', 'zouharvi/optimal-reference-translations', 'PAD6/drop-d-w_back_translation-modified', 'PAD6/dev-v2-w_back_translation-original', 'PAD6/dev-v1-w_back_translation-modified', 'PAD6/dev-v2-w_back_translation-modified', 'PAD6/drop-d-w_back_translation-original', 'PAD6/dev-v1-w_back_translation-original', 'Saxo/en_ko_translation_social_science_linkbricks_single_dataset_with_prompt_text_huggingface_sampled', 'Saxo/en_ko_translation_tech_science_linkbricks_single_dataset_with_prompt_text_huggingface_sampled', 'Saxo/cn_ko_translation_tech_social_science_linkbricks_single_dataset_with_prompt_text_huggingface', 'Saxo/jp_ko_translation_tech_social_science_linkbricks_single_dataset_with_prompt_text_huggingface', 'Saxo/en_ko_translation_social_science_linkbricks_single_dataset_with_prompt_text_huggingface', 'Saxo/en_ko_translation_tech_science_linkbricks_single_dataset_with_prompt_text_huggingface', 'atlasia/darija-translation', 'trajesh/eng_french_translation_sample', 'trajesh/english_tamil_translation_examples', 'trajesh/eng_french_translation_samples', 'trajesh/language_translation_samples', 'Tippawan/medical_translation_v.2', 'bayandashnan/tmp-translation', 'AndrewLIANG03/en-zh-translation', 'trajesh/rajnikanth_diagloes_tamil_english_translation', 'misclassified/meps_speeches_with_translation.csv', 'projectbaraat/hindi-translation-data-v0.1', 'projectbaraat/kannada-translation-data-v0.1', 'lemon-mint/en_ko_translation_purified_v0.1', 'dichmau/ja_vi_translation', 'somosnlp/prompt-translation-for-es', 'somosnlp/dib-translation-for-es-bench', 'argilla/prompt-translation-for-es-bench', 'innodatalabs/rt-realtoxicity-translation-multiturn', 'janetyu900/translation2019zh_train', 'MohamedRashad/rasaif-translations', 'alvarobartt/self-alignment-with-instruction-backtranslation', 'DIBT/dibt-prompt-translation-for-arabic', 'distilabel-internal-testing/instruction-backtranslation-mini', 'alvations/wikisummary-translations', 'sawradip/bn-translation-mega-raw-noisy', 'davanstrien/report-translation-feedback', 'DanteAl97/hindi-english-translation', 'ziozzang/osx_dictionary_translation_pairs', 'ziozzang/multi-lang-translation-set', 'AbhiKrov/basic_translation_model_1', 'tiagoblima/bible-ptbr-gun-gub-translation', 'inorrr/BiLD_translation_discrim_eval', 'liaad/machine_translation_dataset', 'liaad/machine_translation_dataset_detokenized', 'AlexanderBenady/english_spanish_translations', 'DewiBrynJones/banc-trawsgrifiadau-bangor-translations', 'AlexanderBenady/lecture_summary_translations_english_spanish', 'lemon-mint/ko_dataset_translations_v1.0', 'ashisjk/zh_en_translation_dataset', 'sltAI/crowdsourced-text-to-sign-language-rule-based-translation-corpus', 'nayohan/025_daily_translation', 'nayohan/026_tech_translation', 'nayohan/032_broadcast_translation', 'AbderrahmanSkiredj1/data_translation_quran_yusufali_10k', 'AbderrahmanSkiredj1/ahadith_translation_34k', 'shidowake/240413-040612-mixtral-conv-ja-translation', 'shidowake/240413-082653-mixtral-conv-ja-translation', 'shidowake/240413-115148-mixtral-conv-ja-translation-3300-3800', 'shidowake/240413-115735-mixtral-conv-ja-translation-3800-4300', 'shidowake/240413-120633-mixtral-conv-ja-translation-4300-4800', 'shidowake/240413-125742-mixtral-conv-ja-translation-4800-5391', 'AY2324S2-CS4248-Team-47/backtranslations-5', 'SKNahin/ai4bharat-en2bn-translation-data', 'AY2324S2-CS4248-Team-47/gec-ranked-backtranslation', 'lemon-mint/ko_dataset_translations2_test', 'djsamseng/khmer-speech-large-english-google-translations', 'nayohan/155_industrial_patent_translation', 'open-llm-leaderboard-old/details_lemon-mint__gemma-2b-translation-v0.103', 'lemon-mint/advanced_vocabulary_korean_translation_v0.1', 'AkashDevelops/english_to_bengali_translation', 'nayohan/156_food_expertise_translation', 'nayohan/308_app_translation', 'nayohan/expertise_translation', 'nayohan/tech_science_translation', 'nayohan/social_science_translation', 'nayohan/parallel_translation', 'nayohan/aihub-en-ko-translation-12m', 'AkashDevelops/english-bengali-translation', 'AkashDevelops/EnglishBengali_translation', 'wangrongsheng/zh-en-translation', 'lemon-mint/korean_translation_mixture_v0.2', 'nayohan/aihub-en-ko-translation-1.2m', 'nayohan/aihub-en-ko-translation-120k', 'lemon-mint/korean_translation_mixture_v0.3', 'distilabel-internal-testing/instruction-backtranslation-mini-sft', 'lemon-mint/advanced_korean_vocabulary_translation_v0.3', 'lemon-mint/korean_translation_mixture_v0.6_raw', 'distilabel-internal-testing/instruction-backtranslation-mini-groq', 'scb10x/translation_val', 'ai-maker-space/gen-z-translation', 'laurentiubp/translations', 'dtthanh/gen_z_translation', 'Becks9002/eng-to-amh-translation', 'gplsi/ES-CA_translation_test', 'dmatekenya/chichewa-machine-translation', 'mabidan/econ_paper_abstracts_fa_translation', 'ikeno-ada/Japanese-English_translation_of_contents_HScodes', 'nayohan/raw_instruction_en_ko_translation', 'nayohan/instruction_en_ko_translation_1.4m', 'nayohan/instruction_en_ko_translation_1.4m_chat', 'amrosama/ar_en_dataset_for_lang_translations_taks', 'pt4c/yat_translation_dataset', 'hajili/squad-azerbaijani-reindex-translation', 'NMashalov/olympiad_task_translation', 'jdpressman/retro-weave-eval-analogical-translations-v0.1', 'mesolitica/synthetic-word-switching-translation', 'sohaibmanah/translation', 'pollitoconpapass/cuzco-quechua-translation-spanish', 'Ouail02/translation', 'itsmeismaill/translation_stories', 'qducnguyen/orca_dpo_pair_translation', 'qducnguyen/truthy_dpo_v0.1_translation', 'qducnguyen/truthy_dpo_v0.1_translation_refined', 'qducnguyen/orca_dpo_pair_translation_refined', 'qducnguyen/ultrafeedback_translation_refined', 'iot/eng-to-french-translation', 'SEACrowd/sea_translationese_resampled', 'Dodero1305/machine_translation', 'pollitoconpapass/spa-quz-translation-dataset', 'pollitoconpapass/eng-quz-translation-dataset', 'ytzi/racket-macro-instr-backtranslation', 'IPSAN/tatar_translation_dataset_1m', 'iscreammediaai/multilingual-translation-simple-gpt4o', 'traintogpb/aihub-koja-translation-integrated-large-4.3m', 'traintogpb/aihub-kozh-translation-integrated-large-5.9m', 'traintogpb/aihub-koja-translation-integrated-base-1m', 'traintogpb/aihub-koja-translation-integrated-small-100k', 'traintogpb/aihub-kozh-translation-integrated-base-1m', 'traintogpb/aihub-kozh-translation-integrated-small-100k', 'gplsi/ES-VA_translation_test', 'KorMaverick/translation_data', 'ohsuz/translation_temp', 'prudhvirajdowluri/english_to_telugu_translation', 'malteos/translation_test', 'malteos/translation_test2', 'dadu/patternized-backtranslation', 'gentaiscool/bitext_nusatranslation_miners', 'gentaiscool/nusatranslation_senti', 'cmxuebuhui/translation-e2z', 'TrinhDacPhu/translation-en-vi-dataset', 'AperiJessyca/ml_translation', 'arabic-translation-prompt-engineering/TpDwD', 'timpearce/translation-test-dataset', 'RyotaKadoya1993/translation', 'kotoba-speech/full-translation-overfit-en-ja-6-23', 'SEACrowd/nusatranslation_senti', 'SEACrowd/indonesian_madurese_bible_translation', 'huiwonLee/translation_enzh2ko_3000000_aihub', 'davidberenstein1957/report-translation-feedback', 'thesherrycode/gen-z-slangs-translation', 'bezir/tr_translation_instr_setimes', 'wolfgangmeyers/karuk-translations', 'circulus/ko-translation', 'mahi448/amen_translation', 'misdelivery/translation_examples', 'NorHsangPha/oasst1_shan_translation', 'sahaniaditya/translation_dataset', 'SM0rc/mathleaks_translation2', 'vidula123/translation', 'vidula123/translation_2', 'sed01/english-wolof-translation', 'RyotaKadoya1993/translation-wiki-nemotron', 'haohaaorg/oasst1_shan_translation', 'ahmedsamirio/oasst2-9k-translation', 'ohsuz/translation_corpus', 'RyotaKadoya1993/translation-law', 'orion-research/aura-translations-en-pt-br-v0.1', 'hassaan-qaisar/instruction-backtranslation-instruction-dataset', 'hassaan-qaisar/instruction-backtranslation-instruction-dataset-2', 'ParsBench/parsinlu-machine-translation-en-fa-alpaca-style', 'ParsBench/parsinlu-machine-translation-fa-en-alpaca-style', 'AndreasThinks/welsh-translation-instruction', 'lightontech/tech-viet-translation', 'chethan-mahindrakar/high_quality_instruction_backtranslation', 'mesolitica/crowdsourced-malaysian-translation-v2', 'Zaherrr/translation_log', 'c123ian/dpo_irish_eng_translations', 'minh1110/translation_vi', 'minh1110/translation_vi_2k', 'Saxo/ko_en_translation_tech_science_linkbricks_single_dataset', 'Saxo/ko_en_translation_social_science_linkbricks_single_dataset', 'Saxo/ko_jp_translation_tech_social_science_linkbricks_single_dataset', 'Saxo/ko_cn_translation_tech_social_science_linkbricks_single_dataset', 'morpheus73/hindi_translation', 'Charif-Ayfarah/English-to-Afar-language-translation-dataset', 'LiveReader/translation', 'Slim205/aya_no_translation', 'igoranoni/It-to-En-translations-alpaca', 'igoranoni/En-to-It-translations-alpaca', 'minh1110/translation_vi_20-22k', 'SKNahin/fr-2-bm-translation', 'SKNahin/fr-2-dyu-translation', 'ai-amplified/medical-translation-test-set', 'aimped/medical-translation-test-set', 'LeFluffyPunk/gen-z-translations', 'pythainlp/thai-local-language-translation-dataset', 'sionic/ko-dpo-mix-7k-translation-exclude', 'Noxus09/code-translation', 'sam2ai/hindi_translation_wat2024', 'OdiaGenAIdata/wat24_text_to_text_translation', 'OdiaGenAIdata/wat_text_to_text_translation', 'abideen/instruction-backtranslation-instruction-dataset', 'abideen/instruction-backtranslation-instruction-dataset2', 'abideen/instruction-backtranslation-instruction-dataset-2', 'HANTIFARAH/combined-translations2', 'hiimbach/envi_translation', 'HANTIFARAH/combined-translations-2', 'werent4/lithuanian-translations']\n"
     ]
    }
   ],
   "source": [
    "translation_datasets = [ds for ds in datasets_list if \"translation\" in ds]\n",
    "\n",
    "# Print the translation-related datasets\n",
    "print(translation_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/modules/datasets_modules/datasets/mc4/78f7a2b7e2524fa44ee464ef429d011c365f5fe129283869e7fd76856aacb83a/mc4.py:284: FutureWarning: Dataset 'mc4' is deprecated and will be deleted. Use 'allenai/c4' instead.\n",
      "  warnings.warn(\n",
      "Downloading data:  68%|██████▊   | 1400/2048 [4:47:07<2:12:53, 12.31s/files] \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 2\u001b[0m dataset_ccrawl \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmc4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/datasets/load.py:2628\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2625\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2627\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2628\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2634\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2636\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2637\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2638\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2639\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/datasets/builder.py:1029\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1028\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1029\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/datasets/builder.py:1791\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1791\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASIC_CHECKS\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/datasets/builder.py:1102\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[1;32m   1101\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m-> 1102\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msplit_generators_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/mc4/78f7a2b7e2524fa44ee464ef429d011c365f5fe129283869e7fd76856aacb83a/mc4.py:315\u001b[0m, in \u001b[0;36mMc4._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    305\u001b[0m     data_urls[split] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    306\u001b[0m         _DATA_URL\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    307\u001b[0m             language\u001b[38;5;241m=\u001b[39mlang,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(_N_SHARDS_PER_SPLIT[lang][split])\n\u001b[1;32m    314\u001b[0m     ]\n\u001b[0;32m--> 315\u001b[0m train_downloaded_files \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_urls\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m validation_downloaded_files \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownload(data_urls[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    318\u001b[0m     datasets\u001b[38;5;241m.\u001b[39mSplitGenerator(name\u001b[38;5;241m=\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mSplit\u001b[38;5;241m.\u001b[39mTRAIN, gen_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepaths\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_downloaded_files}),\n\u001b[1;32m    319\u001b[0m     datasets\u001b[38;5;241m.\u001b[39mSplitGenerator(\n\u001b[1;32m    320\u001b[0m         name\u001b[38;5;241m=\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mSplit\u001b[38;5;241m.\u001b[39mVALIDATION, gen_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepaths\u001b[39m\u001b[38;5;124m\"\u001b[39m: validation_downloaded_files}\n\u001b[1;32m    321\u001b[0m     ),\n\u001b[1;32m    322\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/datasets/download/download_manager.py:257\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    255\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[0;32m--> 257\u001b[0m     downloaded_path_or_paths \u001b[38;5;241m=\u001b[39m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloading data files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    267\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;241m.\u001b[39mtotal_seconds() \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/datasets/utils/py_utils.py:511\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    509\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    510\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[0;32m--> 511\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    512\u001b[0m     _single_map_nested((function, obj, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    514\u001b[0m ]\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    516\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/datasets/utils/py_utils.py:512\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    509\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    510\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[1;32m    511\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 512\u001b[0m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    514\u001b[0m ]\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    516\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/datasets/utils/py_utils.py:380\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    375\u001b[0m     batched\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[1;32m    379\u001b[0m ):\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m function(batch)]\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/datasets/utils/py_utils.py:380\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    375\u001b[0m     batched\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[1;32m    379\u001b[0m ):\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/datasets/download/download_manager.py:300\u001b[0m, in \u001b[0;36mDownloadManager._download_batched\u001b[0;34m(self, url_or_filenames, download_config)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     max_workers \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    297\u001b[0m         config\u001b[38;5;241m.\u001b[39mHF_DATASETS_MULTITHREADING_MAX_WORKERS \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m20\u001b[39m \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m20\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    298\u001b[0m     )  \u001b[38;5;66;03m# enable multithreading if files are small\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_desc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloading\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfiles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_identity\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# contains the ranks of subprocesses\u001b[39;49;00m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHF_DATASETS_STACK_MULTIPROCESSING_DOWNLOAD_PROGRESS_BARS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmultiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_identity\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_single(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[1;32m    316\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/tqdm/contrib/concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                       initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/datasets/download/download_manager.py:323\u001b[0m, in \u001b[0;36mDownloadManager._download_single\u001b[0;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;66;03m# append the relative path to the base_path\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     url_or_filename \u001b[38;5;241m=\u001b[39m url_or_path_join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_path, url_or_filename)\n\u001b[0;32m--> 323\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m out \u001b[38;5;241m=\u001b[39m tracked_str(out)\n\u001b[1;32m    325\u001b[0m out\u001b[38;5;241m.\u001b[39mset_origin(url_or_filename)\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/datasets/utils/file_utils.py:211\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    206\u001b[0m         storage_options\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m storage_options\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m<\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (download_config\u001b[38;5;241m.\u001b[39mstorage_options \u001b[38;5;129;01mand\u001b[39;00m download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m<\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m    209\u001b[0m     ):\n\u001b[1;32m    210\u001b[0m         storage_options \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 211\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_etag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_etag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_url_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_url_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_desc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/datasets/utils/file_utils.py:689\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, token, use_auth_token, ignore_url_params, storage_options, download_desc, disable_tqdm)\u001b[0m\n\u001b[1;32m    687\u001b[0m     ftp_get(url, temp_file)\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m scheme \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;129;01mor\u001b[39;00m storage_options\u001b[38;5;241m.\u001b[39mget(scheme):\n\u001b[0;32m--> 689\u001b[0m     \u001b[43mfsspec_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_tqdm\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m     http_get(\n\u001b[1;32m    694\u001b[0m         url,\n\u001b[1;32m    695\u001b[0m         temp_file\u001b[38;5;241m=\u001b[39mtemp_file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    702\u001b[0m         disable_tqdm\u001b[38;5;241m=\u001b[39mdisable_tqdm,\n\u001b[1;32m    703\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/datasets/utils/file_utils.py:395\u001b[0m, in \u001b[0;36mfsspec_get\u001b[0;34m(url, temp_file, storage_options, desc, disable_tqdm)\u001b[0m\n\u001b[1;32m    382\u001b[0m fs, path \u001b[38;5;241m=\u001b[39m url_to_fs(url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[1;32m    383\u001b[0m callback \u001b[38;5;241m=\u001b[39m TqdmCallback(\n\u001b[1;32m    384\u001b[0m     tqdm_kwargs\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    385\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m\"\u001b[39m: desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m     }\n\u001b[1;32m    394\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py:648\u001b[0m, in \u001b[0;36mHfFileSystem.get_file\u001b[0;34m(self, rpath, lpath, callback, outfile, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m callback\u001b[38;5;241m.\u001b[39mset_size(expected_size)\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 648\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_hub_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolve_remote_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolve_remote_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolve_remote_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolve_remote_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m            \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_hf_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_tqdm_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTqdmCallback\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m     outfile\u001b[38;5;241m.\u001b[39mseek(initial_pos)\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;66;03m# Close file only if we opened it ourselves\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/preprocess_bert_udem_eval/lib/python3.10/site-packages/huggingface_hub/file_download.py:552\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n\u001b[0;32m--> 552\u001b[0m     \u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m     new_resume_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# Some data has been downloaded from the server so we reset the number of retries.\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_ccrawl = load_dataset(\"mc4\", \"fr\", trust_remote_code=True)\n",
    "# print(dataset['train'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocess_bert_udem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
