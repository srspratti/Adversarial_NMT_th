{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/preprocess_bert_udem/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache at: /home/paperspace/.cache/huggingface/datasets/wmt14/fr-en\n",
      "Dataset already downloaded, loading from cache.\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating batches: 100%|██████████| 376/376 [04:45<00:00,  1.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0011027631663308342}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import os\n",
    "import torch\n",
    "import dill\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "# Get user's home directory\n",
    "import os\n",
    "home = os.path.expanduser(\"~\")\n",
    "\n",
    "# Define the path of the cache directory\n",
    "cache_dir = os.path.join(home, \".cache\", \"huggingface\", \"datasets\")\n",
    "\n",
    "# Define the name and configuration of the dataset\n",
    "dataset_name = \"wmt14\"\n",
    "config_name = \"fr-en\"\n",
    "\n",
    "# Build the path for the specific dataset configuration\n",
    "dataset_config_path = os.path.join(cache_dir, dataset_name, config_name)\n",
    "\n",
    "print(f\"Checking cache at: {dataset_config_path}\")\n",
    "\n",
    "# Check if the dataset configuration is already cached\n",
    "if os.path.exists(dataset_config_path) and len(os.listdir(dataset_config_path)) > 0:\n",
    "    print(\"Dataset already downloaded, loading from cache.\")\n",
    "    # If the dataset is already downloaded, load it from the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "else:\n",
    "    print(\"Downloading the dataset.\")\n",
    "    # Download the dataset and specify the cache directory\n",
    "    dataset = load_dataset(dataset_name, config_name, cache_dir=cache_dir)\n",
    "\n",
    "# Keep the full valid and test datasets\n",
    "valid_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "texts =[]\n",
    "labels = []\n",
    "for element in test_dataset[\"translation\"]:\n",
    "        # print(\"element: \", element)\n",
    "        texts.append(element[\"en\"])\n",
    "        labels.append(element[\"fr\"])\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "getpwd = os.getcwd()\n",
    "\n",
    "\n",
    "# file_path_en = os.path.join(getpwd, \"original_english_mm_v2.txt\")\n",
    "# # file_path = \"/path/to/translations.txt\"\n",
    "\n",
    "# # Open the file in write mode\n",
    "# with open(file_path_en, \"w\") as file:\n",
    "#     # Write each translation to the file\n",
    "#     for text in texts:\n",
    "#         file.write(text + \"\\n\")\n",
    "\n",
    "\n",
    "# file_path_fr = os.path.join(getpwd, \"original_french_mm_v2.txt\")\n",
    "# # file_path = \"/path/to/translations.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "# with open(file_path_fr, \"w\") as file:\n",
    "#     # Write each translation to the file\n",
    "#     for label in labels:\n",
    "#         file.write(label + \"\\n\")\n",
    "\n",
    "# checkpoint_path_generator = '/home/paperspace/google_drive_v1/Research_Thesis/2024/git_repo/checkpoints/bert_dualG/wmt14_en_fr_1mil_pg_kd_loss_MarianMT_unfreeze_lmlayer_dcd_tp_2_1000sents_debug_Normalkd_2_save_open_direct_pretrained/best_generator_dill_open_at_3.pt'\n",
    "# checkpoint_path_tokenizer = \"/home/paperspace/google_drive_v1/Research_Thesis/2024/git_repo/checkpoints/bert_dualG/wmt14_en_fr_1mil_pg_kd_loss_MarianMT_unfreeze_lmlayer_dcd_tp_2_1000sents_debug_Normalkd_2_save_open_direct_pretrained/best_generator_tokenizer_save_pretrained_at_3\"\n",
    "# translations_generated_filename_batch = \"translated_french_by_MarianMT_FT_1000sents_kd_3_dcd_2.txt\"\n",
    "\n",
    "checkpoint_path_generator = '/home/paperspace/google_drive_v3/Research_Thesis/2024/git_repo/checkpoints/bert_dualG/1mil_checkpoints/best_generator_dill_open_format_at_2.pt'\n",
    "checkpoint_path_tokenizer = '/home/paperspace/google_drive_v3/Research_Thesis/2024/git_repo/checkpoints/bert_dualG/1mil_checkpoints/best_generator_tokenizer_save_pretrained_at_2'\n",
    "\n",
    "# Load the entire model directly\n",
    "generator2_checkpoint = torch.load(open(checkpoint_path_generator, \"rb\"), pickle_module=dill)\n",
    "# generator2_checkpoint= generator2_checkpoint.from_pretrained(checkpoint_path_generator)\n",
    "\n",
    "# generator2_train # Extract the underlying model from the DataParallel wrapper\n",
    "generator2_checkpoint = generator2_checkpoint.module if isinstance(generator2_checkpoint, torch.nn.DataParallel) else generator2_checkpoint\n",
    "\n",
    "# Check if CUDA is available and then set the default device to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path_tokenizer)\n",
    "\n",
    "\n",
    "batch_size = 8  # Adjust this based on your GPU's memory capacity\n",
    "\n",
    "translations_batch = []\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    generator2_checkpoint = torch.nn.DataParallel(generator2_checkpoint).cuda()\n",
    "else:\n",
    "    generator2_checkpoint.cuda()\n",
    "\n",
    "generator2_checkpoint = generator2_checkpoint.module if hasattr(generator2_checkpoint, 'module') else generator2_checkpoint\n",
    "\n",
    "generator2_checkpoint.eval()\n",
    "# generator2_checkpoint.to(device)\n",
    "\n",
    "# Process texts in batches\n",
    "for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating batches\"):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    inputs = tokenizer(batch, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # print(\"inputs shape: \", inputs.shape)\n",
    "    # Generate outputs for the entire batch\n",
    "    outputs = generator2_checkpoint.generate(inputs, max_length=60, num_beams=5, early_stopping=True)\n",
    "    # print(\"outputs shape\", outputs.shape)\n",
    "    \n",
    "    # Decode all outputs in the batch\n",
    "    # batch_translations = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    batch_translation = tokenizer.batch_decode(outputs , skip_special_tokens=True)\n",
    "    translations_batch.extend(batch_translation)\n",
    "\n",
    "# Save the translations to a text file\n",
    "# import os\n",
    "# file_path = os.path.join(os.getcwd(), translations_generated_filename_batch)\n",
    "# with open(file_path, \"w\") as file:\n",
    "#     for translation in translations_batch:\n",
    "#         file.write(translation + \"\\n\")\n",
    "\n",
    "\n",
    "result_batch = metric.compute(predictions=translations_batch, references=labels)\n",
    "result_batch = {\"bleu\": result_batch[\"score\"]}\n",
    "result_batch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocess_bert_udem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
